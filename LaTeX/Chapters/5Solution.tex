%*****************************************
\chapter{Ausführung der Lösung}\label{ch:solution}
%*****************************************

\section{Erstellung des Benchmarks}
Zur Erstellung eines Benchmarks braucht es zwei Dinge:
Natürlichsprachige Fragen und die Antworten als \ac{sparql}-Abfrage.
Der Ansatz, um die Fragen zu erhalten, ist, die bereits durch \citet{arneba} klassifizierten Fragen aus \citet{bb} zu verwenden\footnote{Die Fragen sind auch in \cref{tab:fragenklassifikation} zu finden.}.
Diese wurden jedoch für ein Quiz auf Basis von \ac{snik} klassifiziert, weshalb wir nun neue Kriterien benötigen.

Die Fragen müssen durch eine SPARQL-Query abfragbar sein, dass heißt die möglichen Antworttypen sind Mengen an Ressourcen mit mindestens null Elementen, Literale und einen Wahrheitswert,
der sich bei affirmativen Fragen auf die Existenz der Menge oder ihrer Elemente bezieht.
Nicht abfragbar sind jedoch Größe die Menge, dies ist pädagogisch nicht sinnvoll, da die Ontologie unvollständig sein könnte.
Nach der \emph{open world assumption} ist nicht sämtliches existierendes Wissen in SNIK vorhanden, es ist also nur ein Teil des Existenten in der Ontologie.
Wenn beispielsweise nach der Anzahl unterschiedlicher Arten von Architekturen von Informationssystemen im Gesundheitswesen gefragt würde, sollte das System \enquote{zwei} antworten.
Es könnte jedoch noch mehr geben, weshalb solche Fragen nicht unterstützt werden sollen.
Des Weiteren sollen keine Aggregate, also Operationen wie Summe, Durchschnitt, o.ä. abfragbar sein, da \ac{snik} keine statistischen Daten enthält.
So ergeben solche Fragen aber keinen Sinn.
Zuletzt soll \ac{snik} auch nicht Sachverhalte erklären können, auch, weil nach dem aktuellen Forschungsstand solches nicht möglich ist.
Dafür bräuchte man eine künstliche Intelligenz, welche Sachverhalte verstehen und in eigenen Worten wiedergeben kann.
Außerdem ist das nicht das Ziel dieser Arbeit.
Darauf basierend ist feststellbar, dass nur faktische Fragen beantwortbar sein sollen, also Fragen, die mit Fakten beantwortet werden.
Andere Fragetypen sind solche wie temporale Fragen, welche sich mit Zeitwerten befassen, solche gibt es in \ac{snik} jedoch nicht.
Prozedurale Fragen, meist, aber nicht immer, erkennbar am Schlüsselwort \enquote{wie}.
Sie erhalten Prozesse oder Erklärungen von Schrittabfolgen als Antworten.
Sie sind, genau wie kausale Fragen, welche mit einem Grund o.ä. beantwortet werden, aus den Tripeln schlecht ableitbar.
Kausale Fragen haben oft das Fragewort \enquote{Warum}.
Geographische Fragen sind, wie temporale Fragen, sinnlos, da solche Daten hier nicht gespeichert werden.
Häufige Fragen sind auch Vergleiche, oder auch evaluierende Fragen.
Hier sollen Gemeinsamkeiten und Unterschiede festzustellen.
Man kann sie also als zusammengesetzte Frage aus drei Teilfragen verstehen.
Angenommen man soll zwei fiktive Ressourcen \texttt{?x} und \texttt{?y} vergleichen.
\todo{Ich glaube an der SPARQL Abfrage stimmt etwas nicht. Die zwei gegebenen fiktiven Ressourcen dürfen kein Fragezeichen haben, sonst wären sie nämlich
Variablen.
URIs haben in SPARQL die Form <x> oder mit Präfix meinpraefix:x.
Selbst wenn wir ?x und ?y durch Beispielsweise <x> und <y> ersetzen, erhalten wir am Ende alle Paare, die beiden gemeinsam sind aber keine Unterschiede.
Beispiel:
<x> rdfs:label "Konrad"; rdf:type :Human.
<y> rdfs:label "Hannes"; rdf:type :Human.

Das Ergebnis der Abfrage wäre die einzige Zeile ?p=rdf:type ?o=:Human.
}
\begin{lstlisting}[language=SPARQL]
SELECT *
WHERE
  { ?x ?p ?o
    ?y ?p ?o . }
\end{lstlisting}
Sei die Menge $E$ wie folgt definiert:
$E(s) = \{(p,o) \in KB | (s,p,o) \in KB\}$.
Sie enthält also, abhängig vom Subjekt $s$, die geordnete Menge $(p,o)$ aus Objekt und Prädikat.
Diese sind Element der Wissensbasis, sowohl als Paar als auch als Tripel.
Folgende Mengenoperationen müssten ausgeführt werden, um die Gemeinsamkeiten und Unterschiede einzeln zu betrachten:
\begin{align*}
E(x) &\cap E(y) \\
E(x) &\setminus E(y) \\
E(y) &\setminus E(x)
\end{align*}
Dies ist, weil es eine zusammengesetzte Frage ist und der Komplexität einer solchen Operation im Allgemeinen, noch nicht möglich.
Das Metamodell \ac{snik}s stellt die Aufgabe der Ontologie gut dar: \emph{Wer} macht \emph{was} und \emph{womit}, nicht \emph{warum}, \emph{wann} oder \emph{wie}.

Das System soll nach \cref{def:efrage} und \cref{def:kfrage} simple und komplexe Fragen beantworten können, oder zumindest mit ihnen trainiert werden.
Es soll jedoch keine zusammengesetzten Fragen nach \cref{def:zfrage} beantworten können, da die einerseits oft in mehrere Unterfragen aufgespalten werden können
und andererseits nur schwer beantwortbar sind.
Letztlich ist auch anzumerken, dass nur Fragen aus \ac{afb} 1 \citep{afb}, und auch bei diesen nur ein Teil, beantwortet werden können.
Unter diesen fallen Operatoren wie \enquote{angeben} oder \enquote{aufzählen}.

\subsection{Klassifizierung der Textbuchfragen}\label{sub:fragenklassifikation}

Nun sollen die Fragen aus \citet{bb} einzeln nach Fragetyp und -art, welche oben erklärt wurden, klassifiziert werden.
Sie wurden von \citet{arneba} bereits für das Quiz eingeordnet, in der Tabelle als \enquote{Original} erkennbar.
In \cref{tab:fragenklassifikation} in \cref{ch:klassifizierungtextbuchfragen} werden sie erneut eingeordnet, Unterschiede sind \textbf{fett} gedruckt.
Hier wird eine Frage als geeignet eingestuft, wenn sie faktisch und simpel oder komplex, nicht aber zusammengesetzt sind.
Außerdem wird den Fragen eine ID gegeben, welche aus Kapitelnummer und einer fortlaufenden Zahl gebildet wird.

Hier wurden insgesamt 47 der 79 Fragen als geeignet befunden, verglichen mit 30 vorher.
Dies beruht größtenteils auf zwei Gründen:
\begin{enumerate}
  \item Den Unterschieden zwischen dem Stellen von Fragen in Form eines Quizzes und der Fragenbeantwortung und
  \item fälschlicher Einordnung einzelner Fragen mit dem Fragewort \enquote{how} als prozedural.
\end{enumerate}
Ersteres ist beispielsweise bei den Fragen \texttt{6.6/1}, \texttt{8.6/1} oder \texttt{9/2} der Fall.
Häufig wurden nach \cref{def:kfrage} komplexe Fragen aussortiert, besonders bei Einschränkungen, wie etwa Frage \texttt{7/3}.
Die vermutlich fälschliche Einordnung liegt bei \texttt{6.5/1}, \texttt{8/6} oder \texttt{10/2} vor.
Jedoch können von den initial als geeignet befundenen Fragen nicht alle verwendet werden, da einige nicht in der Ontologie modelliert sind.
Diese sind \texttt{4/3}, \texttt{6/1}, \texttt{6/3}, \texttt{8/4}, \texttt{8.4/1} und \texttt{9/2} sowie \texttt{9.3.4.2/3} bis \texttt{9.3.4.2/5}, \texttt{9.4/1} und \texttt{10/3}.
Somit verbleiben letztendlich 36 Fragen, welche im Folgenden in \ac{sparql}-Abfragen umgewandelt werden.

Aus den in \cref{sub:fragenklassifikation} ausgewählten Fragen werden nun \ac{sparql}-Abfragen gebildet
und die Antwortendurch Dr. Franziska Jahn korrigiert.
Diese sind auch in \cref{ch:klassifizierungtextbuchfragen} in \cref{sub:sparqltextbuchfragen} vorhanden.

Nutzer, die die jeweiligen Fragen fragen, sollen, basierend auf den obigen Abfragen, die Antworten aufgeführt in \cref{sub:antwortentextbuch} erhalten.
\footnote{Momentan stehen hier noch einige falsche Antworten, diese sind \textbf{fett} gedruckt.
In der finalen Version dieser Arbeit werden diese allerdings so gut wie möglich korrigiert sein.}
%\todo{nach frage 6/2 kommt 6/4. in der tabelle steht 6/3 aber noch drin. soll das so sein?} Ja, wurde aussortiert weil keine Antworten in SNIK vorhanden. Steht in einer Auflistung nach der Tabelle, die hab ich aber alle in der Tabelle selbst nicht als nicht-beantwortbar gewertet.

\subsection{Automatische Erstellung einfacher Fragen}

Für die Erstellung der einfachen Fragen muss die großen Datenmenge aus Tripeln \ac{snik}s genutzt werden.
Fragen können sowohl nach dem Subjekt als auch nach dem Objekt oder Prädikat des Tripels gestellt werden.
Für die ersteren beiden Intentionen ist das Fragewort über das Prädikat herausfindbar und ist immer \enquote{Who} oder \enquote{What}.
Für die Frage nach dem Prädikat ist die Zeichenkette immer \enquote{How are ?sl and ?ol related?}, wobei \texttt{?sl} für das Label des Subjekts und \texttt{?ol} für das Label des Objekts steht.
Danach folgt das Label des Prädikats des Tripels und das Label der anderen gegebenen Ressource.

Die Erstellung der Abfragen ist aufgrund der vielen Funktionen, die \ac{sparql} insbesondere auch für die Verarbeitung von Zeichenketten bereitstellt, pro Typ über eine einzige \ac{sparql}-Abfrage möglich,
welche, vom Betreuer Dr. Konrad Höffner bereitgestellt, auf der des \ac{snik}-Quizzes basiert.

\begin{lstlisting}[language=SPARQL]
# SPARQL-Abfrage für Fragen nach dem Subjekt
SELECT DISTINCT REPLACE(REPLACE(REPLACE(REPLACE(
        CONCAT("What ",?pl, " ", ?ol, "?"),
        "What is responsible", "Who is responsible"),
        "What approves", "Who approves"),
        "What is involved", "Who is involved"),
        "What .* component", "What has the component") as ?question,
CONCAT("SELECT DISTINCT ?s WHERE { ?s <", STR(?p), "> <", STR(?o), ">. }") as ?sparql
FROM sniko:meta
FROM sniko:bb
{
 ?s ?p ?o.
 ?p rdfs:domain [rdfs:subClassOf meta:Top].
 ?p rdfs:range [rdfs:subClassOf meta:Top].
 ?s a [rdfs:subClassOf meta:Top].
 ?o a [rdfs:subClassOf meta:Top].
 ?p rdfs:label ?pl. FILTER(langmatches(lang(?pl),"en")).
 ?o rdfs:label ?ol. FILTER(langmatches(lang(?ol),"en")).
}
ORDER BY RAND()
\end{lstlisting}

Zuerst wird die Frage mithilfe der String-Funktionen generiert.
Anfangs wird eine Zeichenkette aus dem Fragewort \enquote{What}, welches das von Prädikaten am häufigsten verwendete ist, dem Label des Prädikats und dem Label des Objekts gebildet,
um eine bearbeitbare Grundlage zu haben, welche oft schon so verwendet werden kann.
Es werden die einzelnen Prädikate, welche das Fragewort \enquote{Who} haben oder für die Nutzung als Frage leicht verändert werden müssen,
durchgegangen und gegebenenfalls mithilfe der Funktion \texttt{REPLACE} ersetzt.

Über die Auswahl von den Teilontologien \texttt{meta} und {bb} über das Schlüsselwort \texttt{FROM} wird gewährleistet, dass nur für diese Fragen generiert werden.
Es wird außerdem garantiert, dass nur Prädikate, die Beziehungen zwischen Ressourcen darstellen, genutzt werden, wie in \cref{fig:snik-metamodel} zu sehen ist.

In der eigentlichen Abfrage wird festgelegt, dass das Prädikat den Definitions- und Wertebereich \aurl{rdfs}{subClassOf} \aurl{meta}{top} haben muss, also nur Beziehungen zwischen Ressourcen der \ac{snik}-Ontologie darstellen soll.
Außerdem wird als erstes das Tripel \texttt{?s ?p ?o.} hingeschrieben, um die Beziehung dieser drei Variablen zu sichern.
Dann werden das Objekt sowie die Subjekte und deren englischsprachige Labels ausgewählt.
Da eine Ressource mit dem gleichen Prädikat als Objekt zu mehreren anderen Ressourcen in Verbindung stehen kann, wird eine Zeichenkette erstellt, die die Labels der Subjekte mit Semikolons getrennt ausgibt.
Die \acp{uri} werden auch ausgegeben und es wird alles zufällig sortiert.

Die Frage zur Generierung der Fragen nach dem Objekt sieht der nach dem Subjekt ziemlich ähnlich, hier wird immer das Fragewort \enquote{What} verwendet, etwa bei \enquote{What is the Chief Information Officer responsible for?}.
Allerdings sieht man an dieser Abfrage auch die Schwierigkeit dieser Art von Fragen:
Es gibt Prädikate, welche aus zwei oder mehr Wörtern bestehen, bei denen das Subjekt zwischen diesen stehen muss, wie hier zum Beispiel der Chief Information Officer zwischen \enquote{is} und \enquote{responsible for}.
Dies muss jedoch nicht pro Prädikat einzeln gemacht werden, hierzu kann ein regulärer Ausdruck genutzt werden.
Außerdem müssen manche Prädikate, wie zum Beispiel \enquote{approves}, konjugiert werden.
Dies lässt sich auch mithilfe eines regulären Ausdrucks für alle Prädikatlabel mit nur einem \texttt{REPLACE}-Statement erreichen.
Die Abfrage lautet wie folgt:
\begin{lstlisting}[language=SPARQL]
# SPARQL-Abfrage für Fragen nach dem Objekt
SELECT DISTINCT CONCAT(
    "What ", REPLACE(REPLACE(REPLACE(
    STR(?pl), ".* component", CONCAT("are components of ", STR(?sl))),
    "^is ([a-z]* [a-z]*)", CONCAT("is ", STR(?sl), " $1")),
    "^([a-z]*e)s", CONCAT("is $1d by ", STR(?sl))),
    "?") as ?question,
CONCAT ("SELECT DISTINCT ?o WHERE { <", STR(?s), "> <", STR(?p), "> ?o. }") as ?sparql
FROM sniko:meta
FROM sniko:bb
{
 ?s ?p ?o.
 ?p rdfs:domain [rdfs:subClassOf meta:Top].
 ?p rdfs:range [rdfs:subClassOf meta:Top].
 ?s a [rdfs:subClassOf meta:Top].
 ?o a [rdfs:subClassOf meta:Top].
 ?s rdfs:label ?sl. FILTER(langmatches(lang(?sl),"en")).
 ?p rdfs:label ?pl. FILTER(langmatches(lang(?pl),"en")).
}
ORDER BY RAND()
\end{lstlisting}

Dadurch konnten 621 Fragen nach dem Subjekt und 376 Fragen nach dem Objekt generiert werden.
Dies entspricht nicht ganz der Anzahl von Tripeln der Teilontologie \texttt{bb}, was zuerst erwartet war.
Die Anzahl wird allerdings einerseits dadurch verringert, dass manche Ressourcen kein Label haben und dies in der Abfrage gefordert wird.
und dass bei der Frage nach den Subjekten viele Fragen durch das Schlüsselwort \texttt{DISTINCT} gekürzt wurden.
Die Fragen nach einem Objekt mit dem gleichen Prädikat von verschiedenen Subjekten waren natürlich immer gleich,
äquivalent zu den Fragen nach dem Subjekt mit verschiedenen Objekten und gleichen Prädikaten.

\section{Auswahl eines Kandidaten}

Die Auswahl eines oder mehrerer Kandidaten gestaltete sich aufgrund von nicht instand gehaltener Programme
und der Spezialisierung von vielen Systemen auf DBpedia oder ähnliche Wissensbasen sehr schwierig.
Bei gAnswer2 war es beispielsweise nicht möglich, überhaupt ein Programm, was man hätte ausprobieren können, zu finden,
bei gAnswer war die Vorbereitung der Daten sehr aufwändig und die gegebenen Werkzeuge nicht mehr funktional.
DeepPavlov war am Anfang sehr vielversprechend, besonders aufgrund des modularen Aufbaus.
Jedoch gibt es keine Möglichkeit, andere Daten als Wikidata zu verwenden, weshalb es für den Einsatz mit \ac{snik} nicht nutzbar ist.
Ähnlich sieht es mit AskNow aus, wofür aufgrund des Entity Linkings DBpedia verwendet werden muss.
In der Dokumentation\footnote{\url{http://docs.deeppavlov.ai/en/master/features/models/kbqa.html}, abgerufen am 9. Mai 2022} wird allerdings geschrieben,
dass in der Zukunft auch die Verwendung anderer Quellen möglich sein sollen.
All diese und andere Probleme beschrieb bereits \citet{diefenbachkbqa}.
Er nannte speziell die Probleme der
\begin{itemize}
  \item Mehrsprachigkeit,
  \item Portabilität,
  \item Skalierbarkeit,
  \item Robustheit,
  \item Fähigkeit, über mehrere Dateien zu suchen und
  \item Präsentation der Antwort.
\end{itemize}
Hier sind vor allem die Portabilität und Robustheit, aber auch teilweise die Präsentation der Antwort problematisch.

\subsection{TeBaQA}

TeBaQA war anfangs sehr vielversprechend, mehr noch als DeepPavlov.
Es ermöglicht die Verwendung von eigenen \ac{rdf}-Tripeln ermöglichen, indem die \texttt{indexing.properties}-Datei verändert wird.
Es gibt zwei Ordner, einen namens \texttt{ontology} und einen namens \texttt{data}.
Da \ac{snik}, wie in \cref{sec:snik} schon erwähnt, eine Ontologien aus Ontologien ist, wird in den \texttt{ontology}-Ordner nur die \texttt{meta.ttl}-Datei gelegt,
welche die Teilontologie enthält, in welcher die Properties gespeichert sind.
Die restlichen Tripel kommen alle in den \texttt{data}-Ordner, da es bei TeBaQA auch möglich ist, mehrere Dateien als Quelle zu verwenden.
Die auf \texttt{.flag} endenden Variablen wurden auf \texttt{true} gesetzt.
Andere Dateien wurden nicht verändert.

Leider warf das System immer wieder Fehler bezüglich der ElasticSearch-Konfiguration, weshalb wir ein Docker-Image,
also eine auf Anwendungsebene virtuelle Umgebung, mit ElasticSearch 6.6.1, TeBaQA und der \ac{snik}-Ontologie erstellt haben.
Die Ontologie ist zusätzlich noch ein weiteres Mal in dem Docker-Image, um Probleme mit der Reihenfolge des Ladens der Daten zu beheben.
Letztendlich können wir das System aber nicht nutzen, da immer wieder zu Laufzeitfehlern auftreten,
sodass weder \ac{sparql}-Abfragen noch Ergebnisse angezeigt werden.

\subsection{QAnswer KG}

QAnswer KG präsentierte sich als einfache Methode, mithilfe von eigenen Daten Question Answering zu betreiben.
Es funktioniert, indem man auf der Website\footnote{\url{https://qanswer-frontend.univ-st-etienne.fr/}} einen Account erstellt und in diesem seine Daten hochlädt.
Auf der Startseite ist bereits ein Beispiel mit Wikidata vorhanden.

\subsubsection{Anfängliche Konfiguration und Probleme}

Beim Einrichten der Umgebung wurde die Sprache auf Englisch gestellt, da in dieser auch die Fragen beantwortet werden sollen.
QAnswer KG versucht, sich auf eine Antwort zu konzentrieren und stellt nur die als am besten bewertete \ac{sparql}-Abfrage dar.
Hier wird das Problem der Ambiguität deutlich, da bei der Frage \enquote{What is the chief information officer responsible for?} sowohl
\aurl{meta}{isResponsibleForEntityType}, \aurl{meta}{isResponsibleForFunction} und \aurl{meta}{isResponsibleForRole} gemeint sein könnten.
Der Ansatz für die Lösung dises Problems war es, im Training in den als Lösung markierten \ac{sparql}-Abfragen \emph{property paths} zu verwenden,
mit denen man zum Beispiel das Prädikat unterschiedliche Ressourcen sein lassen kann.
Es werden letztendlich drei Abfragen ausgeführt, eine für jede mögliche Kombination der Attribute, also hier einmal pro Prädikat.
Statt etwa \texttt{\aurl{meta}{isResponsibleForEntityType}} stünde nun \texttt{\aurl{meta}{isResponsibleForRole} | \aurl{meta}{isResponsibleForFunction} | \aurl{meta}{isResponsibleForRole}} dort.
Praktisch für die Lokalisierung der Fehler war die Funktion, sich alle generierten Anfragen anzeigen zu lassen.
Somit konnte erahnt werden, warum es das macht, was es macht.

Jedoch kann QAnswer keine solchen property paths bilden.
Deshalb mussten vor dem Training die \aurl{rdfs}{subClassOf}+-Beziehung materialisiert werden,
%\todo{müsste das nicht subPropertyOf sein? wie genau hatten wir das gemacht?
%sind die materialisierten Tripel überhaupt noch da?
%Bei \url{https://www.snik.eu/ontology/bb/ChiefInformationOfficer} sehe ich z.B. nichts materialisiertes.
%Vielleicht hatten wir das irgendwann mal in den SPARQL Endpunkt gestellt was dann beim neu Aufsetzen verloren gegangen ist?
%}

% Das hatten wir mit einer SPARQL-Query mit CONSTRUCT gemacht, glaube ich, aber nur für den Datensatz, den wir dann auf QAnswer hochgeladen haben.
% Sollte es noch in den Dateien in QAnswer geben.
% Muss subClassOf sein, da in viieelen Fragen Beziehungen zwischen den Ressourcen gewollt sind.
das heißt es wurden alle transitiven Subklassenbeziehungen zu Trainingszwecken mittels dem \ac{sparql}-Befehl \texttt{CONSTRUCT} zu direkten Subklassenbeziehungen umgeformt. % Ach, hier stehts ja auch

Wenn keine Lösung für die Frage gefunden wurde, gab es als Ausgabe meist die Ressource selbst, also zum Beispiel \aurl{bb}{ChiefInformationOfficer}.
Dies kann zwar nützlich, aber auch verwirrend sein und sollte mithilfe von Training verhindert werden.
Die Präzision der Ergebnisse ohne jegliches fine-tuning ist aber trotzdem erstaunlich.

Mit der Konfiguration an sich konnten auch schon viele Fehler behoben werden.
So gibt es eine Liste von \enquote{stop words}, welche nicht betrachtet werden.
Unter diesen sind häufig verwendete Präpositionen, Konjunktionen, Verben oder Füllwörter wie \enquote{and} oder \enquote{many}.
Diese sollen verhindern, dass falsche Ressourcen gefunden werden.
Hier musste diese Liste allerdings so modifiziert werden, dass das Wort \texttt{for} nicht mehr darin vorkommt, denn ohne es können Prädikate wie \enquote{responsible for} schwer erkannt werden,
besonders, da die beiden Wörter oft getrennt im Satz vorkommen.
Entfernt wurden auch \texttt{define} und \texttt{describe}.
Somit verblieben dann folgende Wörter in der Liste der Stopp-Wörter:
\texttt{a}, \texttt{about}, \texttt{all}, \texttt{an}, \texttt{and}, \texttt{are}, \texttt{as}, \texttt{at}, \texttt{be}, \texttt{by}, \texttt{can}, \texttt{did}, \texttt{do}, \texttt{does}, \texttt{from}, \texttt{give}, \texttt{goes}, \texttt{had}, \texttt{has}, \texttt{have}, \texttt{here}, \texttt{how}, \texttt{in}, \texttt{into}, \texttt{is}, \texttt{its}, \texttt{list}, \texttt{many}, \texttt{most}, \texttt{my}, \texttt{no}, \texttt{of}, \texttt{on}, \texttt{or}, \texttt{s}, \texttt{show}, \texttt{some}, \texttt{something}, \texttt{such}, \texttt{tell}, \texttt{the}, \texttt{their}, \texttt{these}, \texttt{they}, \texttt{this}, \texttt{to}, \texttt{using}, \texttt{was}, \texttt{were}, \texttt{what}, \texttt{which}, \texttt{will}, \texttt{with}, \texttt{yes}.

Aus den \emph{Hidden Properties} wurde \aurl{rdfs}{subClassOf} hinzugefügt.
Hidden Properties sind solche, die nicht explizit in der Frage benutzt werden, aber trotzdem gemeint werden können.
Es wurde \aurl{skos}{altLabel} als weiteres Label für Ressourcen hinzugefügt, sodass QAnswer auch diese beachtet.
Die Mappings mussten auch dahingehend verändert werden, als dass \texttt{skos}{definition}
und \texttt{rdfs}{comment} als Beschreibung hinzugefügt wurden.
Die Definition wurde auch bei den Ergebnissen angezeigt, sodass es bei Ergebnissen von Fragen nach der Definition als richtig erachtet wurde, wenn die zu definierende Ressource richtig erkannt wurde.

Es gibt auch die Möglichkeit, direkt Wörter als Aliase für \acp{uri} zu nutzen.
Bei den \emph{Property Mapping} können \acp{uri} und dafür stehende Lexikalisierungen in Abhängigkeit gebracht werden, wie bei einem Wörterbuch.
Dies wurde hier für \enquote{phases} und \enquote{methods} bei \aurl{meta}{updates} sowie \enquote{tasks} und \aurl{meta}{functionComponent} gemacht.

\subsubsection{Training}

Das Fragenset wurde in zwei Hälften geteilt, eine zum Training und eine zum Testen.
Vor dem Training wurde der Testdatensatz verwendet, um die Leistung des Systems vor und nach dem Training vergleichen zu können.
Siehe hierzu \cref{tab:qanswervortraining}.
Die Fragen wurden randomisiert in die Gruppen eingeteilt, sodass 19 im Trainingsdatensatz und 18 im Testdatensatz sind.

\begin{longtable}{r c c c c c c c}
  \caption[Testdatensatz QAnswer vor Training]{Testdatensatz auf QAnswer KG vor dem Training.
  Conf.: QAnswer \emph{Confidence}-Wert}
  \label{tab:qanswervortraining}
  \\
  \toprule
  Kapitel/ID    & Conf. & |O|   & |C|   & $|O \cap C|$  & P     & R     & F-Maß   \\
  \midrule
  \endfirsthead
  \toprule
  Kapitel/ID    & Conf. & |O|   & |C|   & $|O \cap C|$  & P     & R     & F-Maß   \\
  \midrule
  \endhead
	4/1			& 0.95 & 1 & 1  & 0 & 0.00 & 0.00 & 0.00 \\
	4/2			& 0.93 & 1 & 1  & 0 & 0.00 & 0.00 & 0.00 \\
	4/4			& 0.89 & 1 & 1  & 1 & 1.00 & 1.00 & 1.00 \\
	5/3			& 0.79 & 1 & 1  & 0 & 0.00 & 0.00 & 0.00 \\
	5/4			& 0.35 & 0 & 7  & 0 & 0.00 & 0.00 & 0.00 \\
	6/2			& 0.33 & 4 & 4  & 4 & 1.00 & 1.00 & 1.00 \\
	6.6/2		& 0.22 & 1 & 1  & 0 & 0.00 & 0.00 & 0.00 \\
	6.7/2		& 0.49 & 1 & 1  & 0 & 0.00 & 0.00 & 0.00 \\
	8/3			& 0.27 & 1 & 5  & 0 & 0.00 & 0.00 & 0.00 \\
	8/6			& 0.22 & 1 & 24 & 0 & 0.00 & 0.00 & 0.00 \\
	8.6/2		& 0.27 & 1 & 5  & 0 & 0.00 & 0.00 & 0.00 \\
	9/4			& 0.42 & 1 & 4  & 1 & 1.00 & 0.25 & 0.40 \\
	9.2/2		& 0.24 & 1 & 5  & 0 & 0.00 & 0.00 & 0.00 \\
	9.3.4.2/1	& 0.24 & 5 & 1  & 0 & 0.00 & 0.00 & 0.00 \\
	9.4/1		& 0.54 & 0 & 5  & 0 & 0.00 & 0.00 & 0.00 \\
	9.5/2		& 0.66 & 4 & 4  & 4 & 1.00 & 1.00 & 1.00 \\
	9.6/1		& 0.67 & 6 & 6  & 6 & 1.00 & 1.00 & 1.00 \\
	10/2		& 0.47 & 3 & 1  & 0 & 0.00 & 0.00 & 0.00 \\
  \midrule
  Durchschnitt  & 0.50 &  &    &   & 0.28 & 0.24 & 0.24 \\
  \bottomrule
\end{longtable}

Auffällig ist, dass nur Fragen nach der Definition richtig beantwortet wurden, diese aber konsistent.
Das liegt daran, dass QAnswer so konfiguriert ist, dass es immer die Definition der gefundenen Ressource mit anzeigt.
Bei Definitionsfragen wird die Ressource meist genannt und kann so gut vom System gefunden werden.
Die einzige Frage, wo dies nicht der Fall ist, ist \texttt{10/2}, wo das Kapitel \enquote{Health Care Networks} gefunden wird.

\begin{longtable}{r c c c c c c c}
  \caption[Testdatensatz QAnswer nach Training]{Testdatensatz auf QAnswer KG nach dem Training.
  Conf.: QAnswer \emph{Confidence}-Wert}
  \label{tab:qanswernachtraining}
  \\
  \toprule
  Kapitel/ID    & Conf. & |O|   & |C|   & $|O \cap C|$  & P     & R     & F-Maß   \\
  \midrule
  \endfirsthead
  \toprule
  Kapitel/ID    & Conf. & |O|   & |C|   & $|O \cap C|$  & P     & R     & F-Maß   \\
  \midrule
  \endhead
	4/1			& 0.39 & 1 & 1 & 0 & 0.00 & 0.00 & 0.00 \\
	4/2			& 0.31 & 0 & 1 & 0 & 0.00 & 0.00 & 0.00 \\
	4/4			& 0.31 & 1 & 1 & 0 & 0.00 & 0.00 & 0.00 \\
	5/3			& 0.34 & 4 & 1 & 0 & 0.00 & 0.00 & 0.00 \\
	5/4			& 0.73 & 0 & 7 & 0 & 0.00 & 0.00 & 0.00 \\
	6/2			& 0.64 & 0 & 4 & 0 & 0.00 & 0.00 & 0.00 \\
	6.6/2		& 0.05 & 528 & 1 & 0 & 0.00 & 0.00 & 0.00 \\
	6.7/2		& 0.29 & 2 & 1 & 0 & 0.00 & 0.00 & 0.00 \\
	8/3			& 0.52 & 0 & 5 & 0 & 0.00 & 0.00 & 0.00 \\
	8/6			& 0.46 & 1 & 24 & 0 & 0.00 & 0.00 & 0.00 \\
	8.6/2		& 0.41 & 0 & 5 & 0 & 0.00 & 0.00 & 0.00 \\
	9/4			& 0.40 & 3 & 4 & 0 & 0.00 & 0.00 & 0.00 \\
	9.2/2		& 0.31 & 1 & 5 & 0 & 0.00 & 0.00 & 0.00 \\
	9.3.4.2/1	& 0.30 & 1 & 1 & 0 & 0.00 & 0.00 & 0.00 \\
	9.4/1		& 0.38 & 0 & 5 & 0 & 0.00 & 0.00 & 0.00 \\
	9.5/2		& 0.42 & 1 & 4 & 0 & 0.00 & 0.00 & 0.00 \\
	9.6/1		& 0.67 & 0 & 6 & 0 & 0.00 & 0.00 & 0.00 \\
	10/2		& 0.29 & 1 & 1 & 0 & 0.00 & 0.00 & 0.00 \\
  \midrule
  Durchschnitt  & 0.40 &  &  &   & 0.00 & 0.00 & 0.00 \\
  \bottomrule
\end{longtable}

Nach dem Training haben sich die Ergebnisse sehr stark verschlechtert.
Keine einzige Frage wurde auch nur teilweise richtig beantwortet.
Sogar der Confidence-Wert ist gesunken.
Dies deutet auf eine Überspezialisierung durch die Trainingsfragen hin, vorher funktionierte es deutlich besser.
Zur Bestätigung dieser These müsste aber erst Training und Evaluierung durch die automatisch generierten Fragen stattfinden.
Eine weitaus wahrscheinlichere Möglichkeit ist aber ein großer Fehler bei der Konfiguration der Trainingsfragen oder des Systems in QAnswer.
Dieser muss noch behoben werden.
\todo{Was ist denn jetzt dabei herausgekommen? Konntest du den Fehler beheben? Das ist ja der absolut wichtigste Teil. Wenn noch nicht, dann bitte hierauf
konzentrieren. Und den Trainingsaspekt bei der Vorstellung von QAnswer KG bitte mit einbeziehen, wie das genau funktioniert mit dem Training, das kann ja auch
die Fehlersuche vereinfachen. Kann ich dir dabei was helfen?}
