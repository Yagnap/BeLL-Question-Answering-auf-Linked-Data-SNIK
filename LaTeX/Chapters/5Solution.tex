%*****************************************
\chapter{Ausführung der Lösung}\label{ch:solution}
%*****************************************

\section{Erstellung des Benchmarks}
Zur Erstellung eines Benchmarks braucht es zwei Dinge:
Natürlichsprachige Fragen und die Antworten als \ac{sparql}-Abfrage.
Der Ansatz, um die Fragen zu erhalten, ist, die bereits durch \citet{arneba} klassifizierten Fragen aus \citet{bb} zu verwenden\footnote{Die Fragen sind auch in \cref{tab:fragenklassifikation} zu finden.}.
Diese wurden jedoch für ein Quiz auf Basis von \ac{snik} klassifiziert, weshalb wir nun neue Kriterien benötigen.

Die Fragen müssen durch eine SPARQL-Query abfragbar sein, dass heißt die möglichen Antworttypen sind Mengen an Ressourcen mit mindestens null Elementen, Literale und einen Wahrheitswert,
der sich bei affirmativen Fragen auf die Existenz der Menge oder ihrer Elemente bezieht.
Nicht abfragbar sind jedoch Größe die Menge, dies ist pädagogisch nicht sinnvoll, da die Ontologie unvollständig sein könnte.
Nach der \emph{open world assumption} ist nicht sämtliches existierendes Wissen in SNIK vorhanden, es ist also nur ein Teil des Existenten in der Ontologie.
Wenn beispielsweise nach der Anzahl unterschiedlicher Arten von Architekturen von Informationssystemen im Gesundheitswesen gefragt würde, sollte das System \enquote{zwei} antworten.
Es könnte jedoch noch mehr geben, weshalb solche Fragen nicht unterstützt werden sollen.
Des Weiteren sollen keine Aggregate, also Operationen wie Summe, Durchschnitt, o.ä. abfragbar sein, da \ac{snik} keine statistischen Daten enthält.
So ergeben solche Fragen aber keinen Sinn.
Zuletzt soll \ac{snik} auch nicht Sachverhalte erklären können, auch, weil nach dem aktuellen Forschungsstand solches nicht möglich ist.
Dafür bräuchte man eine künstliche Intelligenz, welche Sachverhalte verstehen und in eigenen Worten wiedergeben kann.
Außerdem ist das nicht das Ziel dieser Arbeit.
Darauf basierend ist feststellbar, dass nur faktische Fragen beantwortbar sein sollen, also Fragen, die mit Fakten beantwortet werden.
Andere Fragetypen sind solche wie temporale Fragen, welche sich mit Zeitwerten befassen, solche gibt es in \ac{snik} jedoch nicht.
Prozedurale Fragen, meist, aber nicht immer, erkennbar am Schlüsselwort \enquote{wie}.
Sie erhalten Prozesse oder Erklärungen von Schrittabfolgen als Antworten.
Sie sind, genau wie kausale Fragen, welche mit einem Grund o.ä. beantwortet werden, aus den Tripeln schlecht ableitbar.
Kausale Fragen haben oft das Fragewort \enquote{Warum}.
Geographische Fragen sind, wie temporale Fragen, sinnlos, da solche Daten hier nicht gespeichert werden.
Häufige Fragen sind auch Vergleiche, oder auch evaluierende Fragen.
Hier sollen Gemeinsamkeiten und Unterschiede festzustellen.
Man kann sie also als zusammengesetzte Frage aus drei Teilfragen verstehen.
Angenommen man soll zwei fiktive Ressourcen \texttt{?x} und \texttt{?y} vergleichen.

\begin{lstlisting}[language=SPARQL]
SELECT *
WHERE
  { <x> ?p ?ox .
    <y> ?p ?oy . }
\end{lstlisting}
Sei die Menge $E$ wie folgt definiert:
$E(s) = \{(p,o) \in KB | (s,p,o) \in KB\}$.
Sie enthält also, abhängig vom Subjekt $s$, die geordnete Menge $(p,o)$ aus Objekt und Prädikat.
Diese sind Element der Wissensbasis, sowohl als Paar als auch als Tripel.
Folgende Mengenoperationen müssten ausgeführt werden, um die Gemeinsamkeiten und Unterschiede einzeln zu betrachten:
\begin{align*}
E(x) &\cap E(y) \\
E(x) &\setminus E(y) \\
E(y) &\setminus E(x)
\end{align*}
Dies ist, weil es eine zusammengesetzte Frage ist und der Komplexität einer solchen Operation im Allgemeinen, noch nicht möglich.
Das Metamodell \ac{snik}s stellt die Aufgabe der Ontologie gut dar: \emph{Wer} macht \emph{was} und \emph{womit}, nicht \emph{warum}, \emph{wann} oder \emph{wie}.

Das System soll nach \cref{def:efrage} und \cref{def:kfrage} simple und komplexe Fragen beantworten können, oder zumindest mit ihnen trainiert werden.
Es soll jedoch keine zusammengesetzten Fragen nach \cref{def:zfrage} beantworten können, da die einerseits oft in mehrere Unterfragen aufgespalten werden können
und andererseits nur schwer beantwortbar sind.
Letztlich ist auch anzumerken, dass nur Fragen aus \ac{afb} 1 \citep{afb}, und auch bei diesen nur ein Teil, beantwortet werden können.
Unter diesen fallen Operatoren wie \enquote{angeben} oder \enquote{aufzählen}.

\subsection{Klassifizierung der Textbuchfragen}\label{sub:fragenklassifikation}

Nun sollen die Fragen aus \citet{bb} einzeln nach Fragetyp und -art, welche oben erklärt wurden, klassifiziert werden.
Sie wurden von \citet{arneba} bereits für das Quiz eingeordnet, in der Tabelle als \enquote{Original} erkennbar.
In \cref{tab:fragenklassifikation} in \cref{ch:klassifizierungtextbuchfragen} werden sie erneut eingeordnet, Unterschiede sind \textbf{fett} gedruckt.
Hier wird eine Frage als geeignet eingestuft, wenn sie faktisch und simpel oder komplex, nicht aber zusammengesetzt sind.
Außerdem wird den Fragen eine ID gegeben, welche aus Kapitelnummer und einer fortlaufenden Zahl gebildet wird.

Hier wurden insgesamt 47 der 79 Fragen als geeignet befunden, verglichen mit 30 vorher.
Dies beruht größtenteils auf zwei Gründen:
\begin{enumerate}
  \item Den Unterschieden zwischen dem Stellen von Fragen in Form eines Quizzes und der Fragenbeantwortung und
  \item fälschlicher Einordnung einzelner Fragen mit dem Fragewort \enquote{how} als prozedural.
\end{enumerate}
Ersteres ist beispielsweise bei den Fragen \texttt{6.6/1}, \texttt{8.6/1} oder \texttt{9/2} der Fall.
Häufig wurden nach \cref{def:kfrage} komplexe Fragen aussortiert, besonders bei Einschränkungen, wie etwa Frage \texttt{7/3}.
Die vermutlich fälschliche Einordnung liegt bei \texttt{6.5/1}, \texttt{8/6} oder \texttt{10/2} vor.
Jedoch können von den initial als geeignet befundenen Fragen nicht alle verwendet werden, da einige nicht in der Ontologie modelliert sind.
Diese sind \texttt{4/3}, \texttt{6/1}, \texttt{6/3}, \texttt{8/4}, \texttt{8.4/1} und \texttt{9/2} sowie \texttt{9.3.4.2/3} bis \texttt{9.3.4.2/5}, \texttt{9.4/1} und \texttt{10/3}.
Somit verbleiben letztendlich 36 Fragen, welche im Folgenden in \ac{sparql}-Abfragen umgewandelt werden.

Aus den in \cref{sub:fragenklassifikation} ausgewählten Fragen werden nun \ac{sparql}-Abfragen gebildet
und die Antworten durch Dr. Franziska Jahn korrigiert.
Diese sind auch in \cref{ch:klassifizierungtextbuchfragen} in \cref{sub:sparqltextbuchfragen} vorhanden.

Nutzer, die die jeweiligen Fragen fragen, sollen, basierend auf den obigen Abfragen, die Antworten aufgeführt in \cref{sub:antwortentextbuch} erhalten.
\footnote{Momentan stehen hier noch einige falsche Antworten, diese sind \textbf{fett} gedruckt.
In der finalen Version dieser Arbeit werden diese allerdings so gut wie möglich korrigiert sein.}

\subsection{Automatische Erstellung einfacher Fragen}

Für die Erstellung der einfachen Fragen muss die großen Datenmenge aus Tripeln \ac{snik}s genutzt werden.
Fragen können sowohl nach dem Subjekt als auch nach dem Objekt oder Prädikat des Tripels gestellt werden.
Für die ersteren beiden Intentionen ist das Fragewort über das Prädikat herausfindbar und ist immer \enquote{Who} oder \enquote{What}.
Für die Frage nach dem Prädikat ist die Zeichenkette immer \enquote{How are ?sl and ?ol related?}, wobei \texttt{?sl} für das Label des Subjekts und \texttt{?ol} für das Label des Objekts steht.
Danach folgt das Label des Prädikats des Tripels und das Label der anderen gegebenen Ressource.

Die Erstellung der Abfragen ist aufgrund der vielen Funktionen, die \ac{sparql} insbesondere auch für die Verarbeitung von Zeichenketten bereitstellt, pro Typ über eine einzige \ac{sparql}-Abfrage möglich,
welche, vom Betreuer Dr. Konrad Höffner bereitgestellt, auf der des \ac{snik}-Quizzes basiert.

\begin{lstlisting}[language=SPARQL]
# SPARQL-Abfrage für Fragen nach dem Subjekt
SELECT DISTINCT REPLACE(REPLACE(REPLACE(REPLACE(
        CONCAT("What ",?pl, " ", ?ol, "?"),
        "What is responsible", "Who is responsible"),
        "What approves", "Who approves"),
        "What is involved", "Who is involved"),
        "What .* component", "What has the component") as ?question,
CONCAT("SELECT DISTINCT ?s WHERE { ?s <", STR(?p), "> <", STR(?o), ">. }") as ?sparql
FROM sniko:meta
FROM sniko:bb
{
 ?s ?p ?o.
 ?p rdfs:domain [rdfs:subClassOf meta:Top].
 ?p rdfs:range [rdfs:subClassOf meta:Top].
 ?s a [rdfs:subClassOf meta:Top].
 ?o a [rdfs:subClassOf meta:Top].
 ?p rdfs:label ?pl. FILTER(langmatches(lang(?pl),"en")).
 ?o rdfs:label ?ol. FILTER(langmatches(lang(?ol),"en")).
}
ORDER BY RAND()
\end{lstlisting}

Zuerst wird die Frage mithilfe der String-Funktionen generiert.
Anfangs wird eine Zeichenkette aus dem Fragewort \enquote{What}, welches das von Prädikaten am häufigsten verwendete ist, dem Label des Prädikats und dem Label des Objekts gebildet,
um eine bearbeitbare Grundlage zu haben, welche oft schon so verwendet werden kann.
Es werden die einzelnen Prädikate, welche das Fragewort \enquote{Who} haben oder für die Nutzung als Frage leicht verändert werden müssen,
durchgegangen und gegebenenfalls mithilfe der Funktion \texttt{REPLACE} ersetzt.

Über die Auswahl von den Teilontologien \texttt{meta} und {bb} über das Schlüsselwort \texttt{FROM} wird gewährleistet, dass nur für diese Fragen generiert werden.
Es wird außerdem garantiert, dass nur Prädikate, die Beziehungen zwischen Ressourcen darstellen, genutzt werden, wie in \cref{fig:snik-metamodel} zu sehen ist.

In der eigentlichen Abfrage wird festgelegt, dass das Prädikat den Definitions- und Wertebereich \aurl{rdfs}{subClassOf} \aurl{meta}{top} haben muss, also nur Beziehungen zwischen Ressourcen der \ac{snik}-Ontologie darstellen soll.
Außerdem wird als erstes das Tripel \texttt{?s ?p ?o.} hingeschrieben, um die Beziehung dieser drei Variablen zu sichern.
Dann werden das Objekt sowie die Subjekte und deren englischsprachige Labels ausgewählt.
Da eine Ressource mit dem gleichen Prädikat als Objekt zu mehreren anderen Ressourcen in Verbindung stehen kann, wird eine Zeichenkette erstellt, die die Labels der Subjekte mit Semikolons getrennt ausgibt.
Die \acp{uri} werden auch ausgegeben und es wird alles zufällig sortiert.

Die Frage zur Generierung der Fragen nach dem Objekt sieht der nach dem Subjekt ziemlich ähnlich, hier wird immer das Fragewort \enquote{What} verwendet, etwa bei \enquote{What is the Chief Information Officer responsible for?}.
Allerdings sieht man an dieser Abfrage auch die Schwierigkeit dieser Art von Fragen:
Es gibt Prädikate, welche aus zwei oder mehr Wörtern bestehen, bei denen das Subjekt zwischen diesen stehen muss, wie hier zum Beispiel der Chief Information Officer zwischen \enquote{is} und \enquote{responsible for}.
Dies muss jedoch nicht pro Prädikat einzeln gemacht werden, hierzu kann ein regulärer Ausdruck genutzt werden.
Außerdem müssen manche Prädikate, wie zum Beispiel \enquote{approves}, konjugiert werden.
Dies lässt sich auch mithilfe eines regulären Ausdrucks für alle Prädikatlabel mit nur einem \texttt{REPLACE}-Statement erreichen.
Die Abfrage lautet wie folgt:
\begin{lstlisting}[language=SPARQL]
# SPARQL-Abfrage für Fragen nach dem Objekt
SELECT DISTINCT CONCAT(
    "What ", REPLACE(REPLACE(REPLACE(
    STR(?pl), ".* component", CONCAT("are components of ", STR(?sl))),
    "^is ([a-z]* [a-z]*)", CONCAT("is ", STR(?sl), " $1")),
    "^([a-z]*e)s", CONCAT("is $1d by ", STR(?sl))),
    "?") as ?question,
CONCAT ("SELECT DISTINCT ?o WHERE { <", STR(?s), "> <", STR(?p), "> ?o. }") as ?sparql
FROM sniko:meta
FROM sniko:bb
{
 ?s ?p ?o.
 ?p rdfs:domain [rdfs:subClassOf meta:Top].
 ?p rdfs:range [rdfs:subClassOf meta:Top].
 ?s a [rdfs:subClassOf meta:Top].
 ?o a [rdfs:subClassOf meta:Top].
 ?s rdfs:label ?sl. FILTER(langmatches(lang(?sl),"en")).
 ?p rdfs:label ?pl. FILTER(langmatches(lang(?pl),"en")).
}
ORDER BY RAND()
\end{lstlisting}

Dadurch konnten 621 Fragen nach dem Subjekt und 376 Fragen nach dem Objekt generiert werden.\todo{Zahlen updaten}
Dies entspricht nicht ganz der Anzahl von Tripeln der Teilontologie \texttt{bb}, was zuerst erwartet war.
Die Anzahl wird allerdings einerseits dadurch verringert, dass manche Ressourcen kein Label haben und dies in der Abfrage gefordert wird.
und dass bei der Frage nach den Subjekten viele Fragen durch das Schlüsselwort \texttt{DISTINCT} gekürzt wurden.
Die Fragen nach einem Objekt mit dem gleichen Prädikat von verschiedenen Subjekten waren natürlich immer gleich,
äquivalent zu den Fragen nach dem Subjekt mit verschiedenen Objekten und gleichen Prädikaten.\todo{Inhalt verwirrend}

\section{Auswahl eines Kandidaten}

Die Auswahl eines oder mehrerer Kandidaten gestaltete sich aufgrund von nicht instand gehaltener Programme
und der Spezialisierung von vielen Systemen auf DBpedia oder ähnliche Wissensbasen sehr schwierig.
Bei gAnswer2 war es beispielsweise nicht möglich, überhaupt ein Programm, was man hätte ausprobieren können, zu finden,
bei gAnswer war die Vorbereitung der Daten sehr aufwändig und die gegebenen Werkzeuge nicht mehr funktional.
DeepPavlov war am Anfang sehr vielversprechend, besonders aufgrund des modularen Aufbaus.
Jedoch gibt es keine Möglichkeit, andere Daten als Wikidata zu verwenden, weshalb es für den Einsatz mit \ac{snik} nicht nutzbar ist.
Ähnlich sieht es mit AskNow aus, wofür aufgrund des Entity Linkings DBpedia verwendet werden muss.
In der Dokumentation\footnote{\url{http://docs.deeppavlov.ai/en/master/features/models/kbqa.html}, abgerufen am 9. Mai 2022} wird allerdings geschrieben,
dass in der Zukunft auch die Verwendung anderer Quellen möglich sein sollen.
All diese und andere Probleme beschrieb bereits \citet{diefenbachkbqa}.
Er nannte speziell die Probleme der
\begin{itemize}
  \item Mehrsprachigkeit,
  \item Portabilität,
  \item Skalierbarkeit,
  \item Robustheit,
  \item Fähigkeit, über mehrere Dateien zu suchen und
  \item Präsentation der Antwort.
\end{itemize}
Hier sind vor allem die Portabilität und Robustheit, aber auch teilweise die Präsentation der Antwort problematisch.

\subsection{TeBaQA}

TeBaQA war anfangs sehr vielversprechend, mehr noch als DeepPavlov.
Es ermöglicht die Verwendung von eigenen \ac{rdf}-Tripeln ermöglichen, indem die \texttt{indexing.properties}-Datei verändert wird.
Es gibt zwei Ordner, einen namens \texttt{ontology} und einen namens \texttt{data}.
Da \ac{snik}, wie in \cref{sec:snik} schon erwähnt, eine Ontologien aus Ontologien ist, wird in den \texttt{ontology}-Ordner nur die \texttt{meta.ttl}-Datei gelegt,
welche die Teilontologie enthält, in welcher die Properties gespeichert sind.
Die restlichen Tripel kommen alle in den \texttt{data}-Ordner, da es bei TeBaQA auch möglich ist, mehrere Dateien als Quelle zu verwenden.
Die auf \texttt{.flag} endenden Variablen wurden auf \texttt{true} gesetzt.
Andere Dateien wurden nicht verändert.

Leider warf das System immer wieder Fehler bezüglich der ElasticSearch-Konfiguration, weshalb wir ein Docker-Image,
also eine auf Anwendungsebene virtuelle Umgebung, mit ElasticSearch 6.6.1, TeBaQA und der \ac{snik}-Ontologie erstellt haben.
Die Ontologie ist zusätzlich noch ein weiteres Mal in dem Docker-Image, um Probleme mit der Reihenfolge des Ladens der Daten zu beheben.
Letztendlich können wir das System aber nicht nutzen, da immer wieder zu Laufzeitfehlern auftreten,
sodass weder \ac{sparql}-Abfragen noch Ergebnisse angezeigt werden.

\subsection{QAnswer KG}

QAnswer KG präsentierte sich als einfache Methode, mithilfe von eigenen Daten \acl{qa} zu betreiben.
Es funktioniert, indem man auf der Website\footnote{\url{https://app.qanswer.ai/}} einen Account erstellt und in diesem seine \ac{rdf}-Daten in Form von n-Tripeln hochlädt,
einem Serialisierungsformat wie \ac{turtle} für \ac{rdf}.

\subsubsection{Anfängliche Konfiguration und Probleme}

Beim Einrichten der Umgebung wurde die Sprache auf Englisch gestellt, da in dieser auch die Fragen beantwortet werden sollen.

QAnswer KG versucht, sich auf eine Antwort zu konzentrieren und stellt nur die als am besten bewertete \ac{sparql}-Abfrage dar.
Hier wird das Problem der Ambiguität deutlich, da bei der Frage \enquote{What is the chief information officer responsible for?} sowohl
\aurl{meta}{isResponsibleForEntityType}, \aurl{meta}{isResponsibleForFunction} und \aurl{meta}{isResponsibleForRole} gemeint sein könnten.
Der Ansatz für die Lösung dieses Problems ist es, das System alle drei ausführen zu lassen, oder ihm das zumindest zu ermöglichen.
Dies kann zum Beispiel über Property Paths realisiert werden.
In diesem Fall erlauben Property Paths dem Prädikat, in der Abfrage verschiedene Ressourcen darstellen zu können.
Es werden letztendlich drei Abfragen ausgeführt, eine für jede mögliche Kombination der Attribute, also hier einmal pro Prädikat.
Statt allein \texttt{\aurl{meta}{isResponsibleForEntityType}} steht durch die Nutzung von Property Paths nun \texttt{\aurl{meta}{isResponsibleForRole} | \aurl{meta}{isResponsibleForFunction} | \aurl{meta}{isResponsibleForEntityType}} dort.
Die ganze Abfrage sieht unter Verwendung von Property Paths wie folgt aus:
\begin{lstlisting}[language=SPARQL]
  SELECT ?o
  WHERE {
    bb:ChiefInformationOfficer (meta:isResponsibleForRole | meta:isResponsibleForFunction | meta:isResponsibleForEntityType) ?o .
  }
\end{lstlisting}

Jedoch kann QAnswer keine Property Paths austellen, weshalb diese Option zur Lösung des Ambiguitätsproblems wegfällt.

Eine andere Möglichkeit, die obige Frage mittels einer \ac{sparql}-Abfrage zu beantworten, ist mittels der \aurl{rdfs}{subPropertyOf}-Beziehung:
\begin{lstlisting}[language=SPARQL]
  SELECT ?o
  WHERE {
    bb:ChiefInformationOfficer ?p ?o .
    ?p rdfs:subPropertyOf* meta:isResponsibleFor .
  }
\end{lstlisting}

Diese regelt die Hierarchie von Properties.
So sind die drei, welche hier alle in einer Abfrage zusammengefasst werden sollen, alle ein Subproperty von \aurl{meta}{subPropertyOf}.

Dabei wichtig ist, dass hier \texttt{*} statt \texttt{+} oder gar nichts nach \aurl{rdfs}{subPropertyOf} steht, falls \aurl{meta}{isResponsibleFor} selbst genutzt wird,
und nicht nur die untergeordneten Properties.
Diese Zeichen geben wieder einen Property Path an, der von QAnswer wieder nicht gelöst werden kann.

Deshalb mussten vor dem Training die \aurl{rdfs}{subPropertyOf}+-Beziehung materialisiert werden%
\footnote{Es werden nur \aurl{rdfs}{subPropertyOf}+-Beziehungen materialisiert, da das Tripel mit der übergeordneten Property schon in der \ac{kb} ist.},
das heißt es wurden alle transitiven Subpropertybeziehungen zu Trainingszwecken mittels dem \ac{sparql}-Befehl \texttt{CONSTRUCT} zu direkten Subklassenbeziehungen umgeformt.

Das ist mit dieser \ac{sparql}-Abfrage möglich:
\begin{lstlisting}[language=SPARQL]
  CONSTRUCT {?s ?p ?o}
  FROM sniko:meta
  FROM sniko:bb
  WHERE {
    ?s ?pp ?o .
    ?pp rdfs:subPropertyOf+ ?p . 
  }
\end{lstlisting}

Es werden erst alle Tripel gefunden, die ausschließlich aus den Teilontologien \texttt{meta} und \texttt{bb} bestehen, da dies den Umfang dieser Arbeit beschreibt.
Somit werden auch nur Prädikate aus \texttt{meta} betrachtet, nicht aber \texttt{rdfs} oder anderen.
Danach werden alle Subproperties des Prädikats aus dem ersten Tripel ausgewählt.
Mittels \texttt{CONSTRUCT} werden die ausgewählten Prädikate anstelle dem anderen Property eingesetzt.
Diese Tripel werden dann manuell den \ac{rdf}-Daten hinzugefügt\footnote{Verwendete n-Tripel-Datei verfügbar unter:\\\url{https://github.com/Yagnap/BeLL-Question-Answering-auf-Linked-Data-SNIK/blob/main/Data/qanswerset.nt}}

Zusätzlich werden auch die \aurl{rdfs}{subClassOf}+-Beziehungen materialisiert.
Dies muss geschehen, da die Antworten für manche Textbuchfragen alle transitiven Subklassen sind.

Das kann mittels der folgenden Abfrage bewerkstelligt werden:
\begin{lstlisting}[language=SPARQL]
  CONSTRUCT {?s ?p ?oo}
  FROM sniko:meta
  FROM sniko:bb
  WHERE {
    ?s ?p ?o . 
    FILTER(?oo!=owl:Thing && ?oo!=meta:Top) .
    FILTER(STRSTARTS(STR(?oo),"http://www.snik.eu/ontology/bb")) .
    ?o rdfs:subClassOf+ ?oo .
  }
\end{lstlisting}

Beide dieser \ac{sparql}-Abfragen wurden vom Betreuer bereitgestellt.
Durch die Materialisierung kann beispielsweise für die Frage \texttt{8/6}, \enquote{How can quality of HIS be evaluated?},
die Klasse \aurl{bb}{CaseStudy} gefunden werden (siehe \cref{sub:antwortentextbuch}), obwohl diese eine direkte Subklasse von \aurl{bb}{QualitativeEvaluationMethod} ist,
welche wiederum eine direkte Subklasse von \aurl{bb}{EvaluationMethod} ist, welche in der in \cref{sub:sparqltextbuchfragen} sichtbaren Abfrage gesucht wird.
Für die \ac{kb} mit den unmaterialisierten transitiven Subklassenbeziehungen sieht die Abfrage so aus:
\begin{lstlisting}[language=SPARQL]
  SELECT DISTINCT ?s
  WHERE
    { ?s rdfs:subClassOf+ bb:EvaluationMethod . }
\end{lstlisting}

Nach der Materialisierung kann das \texttt{+} entfernt werden, wodurch folgende Abfrage ausreicht:
\begin{lstlisting}[language=SPARQL]
  SELECT DISTINCT ?s
  WHERE
    { ?s rdfs:subClassOf bb:EvaluationMethod . }
\end{lstlisting}

Die erste Abfrage kann nicht durch QAnswer generiert werden, die zweite jedoch schon.

Wenn keine Lösung für die Frage gefunden wurde, gab es als Ausgabe meist die Ressource selbst, also zum Beispiel \aurl{bb}{ChiefInformationOfficer}.
Dies kann zwar nützlich, aber auch verwirrend sein und sollte mithilfe von Training verhindert werden.
Die Präzision der Ergebnisse ohne jegliches fine-tuning ist aber trotzdem erstaunlich.

Praktisch für die Lokalisierung der Fehler war die Funktion, sich alle generierten Anfragen anzeigen zu lassen.
Somit konnte erahnt werden, warum es das macht, was es macht.

Mit der Konfiguration an sich konnten schon viele Fehler behoben werden.
So gibt es eine Liste von \enquote{stop words}, welche nicht betrachtet werden.
Unter diesen sind häufig verwendete Präpositionen, Konjunktionen, Verben oder Füllwörter wie \enquote{and} oder \enquote{many}.
Diese sollen verhindern, dass falsche Ressourcen gefunden werden.
Hier musste diese Liste allerdings so modifiziert werden, dass das Wort \texttt{for} nicht mehr darin vorkommt, denn ohne es können Prädikate wie \enquote{responsible for} schwer erkannt werden,
besonders, da die beiden Wörter oft getrennt im Satz vorkommen.
Entfernt wurden auch \texttt{define} und \texttt{describe}.
Somit verblieben dann folgende Wörter in der Liste der Stopp-Wörter:
\texttt{a}, \texttt{about}, \texttt{all}, \texttt{an}, \texttt{and}, \texttt{are}, \texttt{as}, \texttt{at}, \texttt{be}, \texttt{by}, \texttt{can}, \texttt{did}, \texttt{do}, \texttt{does}, \texttt{from}, \texttt{give}, \texttt{goes}, \texttt{had}, \texttt{has}, \texttt{have}, \texttt{here}, \texttt{how}, \texttt{in}, \texttt{into}, \texttt{is}, \texttt{its}, \texttt{list}, \texttt{many}, \texttt{most}, \texttt{my}, \texttt{no}, \texttt{of}, \texttt{on}, \texttt{or}, \texttt{s}, \texttt{show}, \texttt{some}, \texttt{something}, \texttt{such}, \texttt{tell}, \texttt{the}, \texttt{their}, \texttt{these}, \texttt{they}, \texttt{this}, \texttt{to}, \texttt{using}, \texttt{was}, \texttt{were}, \texttt{what}, \texttt{which}, \texttt{will}, \texttt{with}, \texttt{yes}.

Aus den \emph{Hidden Properties} wurde \aurl{rdfs}{subClassOf} hinzugefügt.
Hidden Properties sind solche, die nicht explizit in der Frage benutzt werden, aber trotzdem gemeint werden können.
Es wurde \aurl{skos}{altLabel} als weiteres Label für Ressourcen hinzugefügt, sodass QAnswer auch diese beachtet.
Die Mappings mussten auch dahingehend verändert werden, als dass \aurl{skos}{definition}
und \aurl{rdfs}{comment} als Beschreibung hinzugefügt wurden.
Die Definition wurde auch bei den Ergebnissen angezeigt, sodass es bei Ergebnissen von Fragen nach der Definition als richtig erachtet wurde, wenn die zu definierende Ressource richtig erkannt wurde.

Es gibt auch die Möglichkeit, direkt Wörter als Aliase für \acp{uri} zu nutzen.
Bei den \emph{Property Mapping} können \acp{uri} und dafür stehende Lexikalisierungen in Abhängigkeit gebracht werden, wie bei einem Wörterbuch.
Dies wurde hier für \enquote{phases} und \enquote{methods} bei \aurl{meta}{updates} sowie \enquote{tasks} und \aurl{meta}{functionComponent} gemacht.
Für \aurl{meta}{entityTypeComponent} wurde \enquote{facets} hinzugefügt.
Notwendig war es aufgrund der Funktionsweise QAnswers; ohne Lexikalisierungen wüsste das Programm nicht, wofür die Ressourcen stehen.
Da die Properties in \ac{snik} sehr allgemein gehalten sind, fehlen oft Labels, mithilfe deren QAnswer auf die Ressource als Teil der Antwort schließen kann.
Für diese stehen die Lexikalisierungen zur Verfügung.

\subsubsection{Training}

Das Frage-Antwort-Set aus dem Lehrbuch wurde in zwei Hälften geteilt, eine zum Training und eine zum Testen.
Die Fragen wurden randomisiert in die Gruppen eingeteilt, sodass 18 im Trainingsdatensatz und 18 im Testdatensatz sind.

Von den automatisch generierten Paaren wurden 100 für das Testen und 895 für das Training genutzt.

Das Training verläuft so ab, das dem System automatisiert pro Schritt zehn weitere automatisch generierte Paare aus dem Trainingsdatensatz gegeben werden,
sodass jede Runde mit zehn zusätzlichen Fragen trainiert wird.
Der Trainingssatz der Lehrbuchfragen wird in der ersten Runde nicht mit verwendet,
in der zweiten Runde werden von Anfang an alle Textbuchfragen verwendet.

Dann wird QAnswer mithilfe der \ac{api}-Methode trainiert.
Auf das trainierte System wird der Testdatensatz zur Evaluierung angewandt.
Über eine \ac{api}-Abfrage wird die Antwort mit dem höchsten Confidence-Wert, eine \ac{sparql}-Abfrage, geholt und auf dem \ac{sparql}-Endpunkt von \ac{snik} ausgeführt.
Die Antworten der richtigen Lösung wurden schon vorher gespeichert und werden nun mit denen der Abfrage von QAnswer verglichen.

Dann wird das trainierte Modell zurückgesetzt, die gegebenen Frage-Antwort-Paare bleiben jedoch erhalten.

In \cref{plot:genfscore} sieht man, wie groß das F-Maß abhängig von der Fragenanzahl ist.
Es sind zwei Datensätze dargestellt.
Der eine wurde auch mit den Lehrbuchfragen trainiert, der andere ausschließlich über die \ac{sparql}-generierten Frage-Antwort-Paare.
Am Anfang, wo noch gar keine oder nur sehr wenige Fragen zum Training verwendet wurden, ist das F-Maß vergleichsweise eher gering.
Besonders beim Training mit den Lehrbuchfragen liegt es bei 20 generierten Trainingsfragen deutlich unter 30\%.
Ab 30 Fragen nähert es sich den Werten des Trainings ohne Lehrbuchfragen an und übersteigt diese ab 50 Fragen erstmals.
Bei 150 Fragen erreicht das F-Maß des Trainings mit den Lehrbuchfragen bei 91\% ein Maximum und bleibt recht konstant über den Werten des Trainings ohne sie.

Bei 590 Fragen sinken die Werte für beide Graphen aber sehr abrupt sehr stark ab, die sowieso schon große Varianz wird noch viel größer,
mit Werten zwischen fast 10\% und fast 90\%.
Das Training ohne Lehrbuchfragen erreicht bei 850 Trainingsfragen sein Maximum von 89\%, bei 890 Fragen jedoch auch sein Minimum von 18\%.

Auch die Graphen von Precision (\cref{plot:genprecision}) und Recall (\cref{plot:genprecision}) bieten ein ähnliches Bild.
Sie haben auch am Anfang kurz geringere Werte, eine recht starke Mitte und danach eine extreme Varianz.

Der Confidence-Wert sieht im Allgemeinen deutlich konstanter aus, an ihm spiegeln sich aber auch einige der beim F-Maß betrachteten Phänomene.
Anfangs ist der Wert durchschnittlich etwas geringer als später, aber immer noch fast überall über 50\%.
Ab etwa 100 Fragen erreicht es ein Niveau zwischen 80\% und 90\%, auf dem es bleibt.
Ab 510 bzw. 560 Fragen kommt es jedoch zu zu größeren Ausreißern, die teilweise bis auf 11\% heruntergehen.

Die Auswertung der automatisch generierten Fragen deutet also darauf hin, dass die Anzahl der generierten Trainingsfragen zwischen 150 und 550 liegen sollte.
Dass das Training mit Lehrbuchfragen erst schlechter und dann teilweise deutlich bessere Ergebnisse liefert, ist ein Indiz dafür,
dass QAnswer von der unterschiedlichen Art der Fragen profitieren könnte, sie aber nicht zu schwer sein sollten.

% F-Score
\begin{figure}%[h!]
  \begin{center}
    \begin{tikzpicture}
      \begin{axis}[
          width=\linewidth, % Scale the plot to \linewidth
          grid=major, % Display a grid
          grid style={dashed,gray!30}, % Set the style
          xlabel=Anzahl der generierten Trainingsfragen, % Set the labels
          ylabel=F-Maß,
          legend style={at={(0.5,-0.10)},anchor=north}, % Put the legend below the plot
          x tick label style={rotate=45,anchor=east} % Display labels sideways
        ]
        \addplot table[x=Anzahl Trainingsfragen,y=F-Score,col sep=comma] {../Data/Tabellen/Training-Auswertung/generated.csv};
        \addlegendentry{Training ohne Lehrbuchfragen}
        \addplot table[x=Anzahl Trainingsfragen,y=F-Score,col sep=comma] {../Data/Tabellen/Training-Auswertung/generated-withtb.csv}; 
        \addlegendentry{Training mit Lehrbuchfragen}
      \end{axis}
    \end{tikzpicture}
    \caption{Durchschnittliches F-Maß bei den automatisch generierten Testfragen in Abhängigkeit zur Anzahl der generierten Trainingsfragen.}\label{plot:genfscore}
  \end{center}
\end{figure}

% Precision
\begin{figure}%[h!]
  \begin{center}
    \begin{tikzpicture}
      \begin{axis}[
          width=\linewidth, % Scale the plot to \linewidth
          grid=major, % Display a grid
          grid style={dashed,gray!30}, % Set the style
          xlabel=Anzahl der generierten Trainingsfragen, % Set the labels
          ylabel=Präzision,
          legend style={at={(0.5,-0.10)},anchor=north}, % Put the legend below the plot
          x tick label style={rotate=45,anchor=east} % Display labels sideways
        ]
        \addplot table[x=Anzahl Trainingsfragen,y=Precision,col sep=comma] {../Data/Tabellen/Training-Auswertung/generated.csv};
        \addlegendentry{Training ohne Lehrbuchfragen}
        \addplot table[x=Anzahl Trainingsfragen,y=Precision,col sep=comma] {../Data/Tabellen/Training-Auswertung/generated-withtb.csv}; 
        \addlegendentry{Training mit Lehrbuchfragen}
      \end{axis}
    \end{tikzpicture}
    \caption{Durchschnittliche Präzision bei den automatisch generierten Testfragen in Abhängigkeit zur Anzahl der generierten Trainingsfragen.}\label{plot:genprecision}
  \end{center}
\end{figure}

% Recall
\begin{figure}%[h!]
  \begin{center}
    \begin{tikzpicture}
      \begin{axis}[
          width=\linewidth, % Scale the plot to \linewidth
          grid=major, % Display a grid
          grid style={dashed,gray!30}, % Set the style
          xlabel=Anzahl der generierten Trainingsfragen, % Set the labels
          ylabel=Recall,
          legend style={at={(0.5,-0.10)},anchor=north}, % Put the legend below the plot
          x tick label style={rotate=45,anchor=east} % Display labels sideways
        ]
        \addplot table[x=Anzahl Trainingsfragen,y=Recall,col sep=comma] {../Data/Tabellen/Training-Auswertung/generated.csv};
        \addlegendentry{Training ohne Lehrbuchfragen}
        \addplot table[x=Anzahl Trainingsfragen,y=Recall,col sep=comma] {../Data/Tabellen/Training-Auswertung/generated-withtb.csv}; 
        \addlegendentry{Training mit Lehrbuchfragen}
      \end{axis}
    \end{tikzpicture}
    \caption{Durchschnittlicher Recall bei den automatisch generierten Testfragen in Abhängigkeit zur Anzahl der generierten Trainingsfragen.}\label{plot:genrecall}
  \end{center}
\end{figure}

% Confidence
\begin{figure}%[h!]
  \begin{center}
    \begin{tikzpicture}
      \begin{axis}[
          width=\linewidth, % Scale the plot to \linewidth
          grid=major, % Display a grid
          grid style={dashed,gray!30}, % Set the style
          xlabel=Anzahl der generierten Trainingsfragen, % Set the labels
          ylabel=Confidence-Wert,
          legend style={at={(0.5,-0.10)},anchor=north}, % Put the legend below the plot
          x tick label style={rotate=45,anchor=east} % Display labels sideways
        ]
        \addplot table[x=Anzahl Trainingsfragen,y=Confidence,col sep=comma] {../Data/Tabellen/Training-Auswertung/generated.csv};
        \addlegendentry{Training ohne Lehrbuchfragen}
        \addplot table[x=Anzahl Trainingsfragen,y=Confidence,col sep=comma] {../Data/Tabellen/Training-Auswertung/generated-withtb.csv}; 
        \addlegendentry{Training mit Lehrbuchfragen}
      \end{axis}
    \end{tikzpicture}
    \caption{Durchschnittlicher Confidence-Wert QAnswers bei den automatisch generierten Testfragen in Abhängigkeit zur Anzahl der generierten Trainingsfragen.}\label{plot:genconfidence}
  \end{center}
\end{figure}

\todo{Ab hier weg/updaten}

Auffällig ist, dass nur Fragen nach der Definition richtig beantwortet wurden, diese aber konsistent.
Das liegt daran, dass QAnswer so konfiguriert ist, dass es immer die Definition der gefundenen Ressource mit anzeigt.
Bei Definitionsfragen wird die Ressource meist genannt und kann so gut vom System gefunden werden.
Die einzige Frage, wo dies nicht der Fall ist, ist \texttt{10/2}, wo das Kapitel \enquote{Health Care Networks} gefunden wird.

\begin{longtable}{r c c c c c c c}
  \caption[Testdatensatz QAnswer nach Training]{Testdatensatz auf QAnswer KG nach dem Training.
  Conf.: QAnswer \emph{Confidence}-Wert}
  \label{tab:qanswernachtraining}
  \\
  \toprule
  Kapitel/ID    & Conf. & |O|   & |C|   & $|O \cap C|$  & P     & R     & F-Maß   \\
  \midrule
  \endfirsthead
  \toprule
  Kapitel/ID    & Conf. & |O|   & |C|   & $|O \cap C|$  & P     & R     & F-Maß   \\
  \midrule
  \endhead
	4/1			& 0.39 & 1 & 1 & 0 & 0.00 & 0.00 & 0.00 \\
	4/2			& 0.31 & 0 & 1 & 0 & 0.00 & 0.00 & 0.00 \\
	4/4			& 0.31 & 1 & 1 & 0 & 0.00 & 0.00 & 0.00 \\
	5/3			& 0.34 & 4 & 1 & 0 & 0.00 & 0.00 & 0.00 \\
	5/4			& 0.73 & 0 & 7 & 0 & 0.00 & 0.00 & 0.00 \\
	6/2			& 0.64 & 0 & 4 & 0 & 0.00 & 0.00 & 0.00 \\
	6.6/2		& 0.05 & 528 & 1 & 0 & 0.00 & 0.00 & 0.00 \\
	6.7/2		& 0.29 & 2 & 1 & 0 & 0.00 & 0.00 & 0.00 \\
	8/3			& 0.52 & 0 & 5 & 0 & 0.00 & 0.00 & 0.00 \\
	8/6			& 0.46 & 1 & 24 & 0 & 0.00 & 0.00 & 0.00 \\
	8.6/2		& 0.41 & 0 & 5 & 0 & 0.00 & 0.00 & 0.00 \\
	9/4			& 0.40 & 3 & 4 & 0 & 0.00 & 0.00 & 0.00 \\
	9.2/2		& 0.31 & 1 & 5 & 0 & 0.00 & 0.00 & 0.00 \\
	9.3.4.2/1	& 0.30 & 1 & 1 & 0 & 0.00 & 0.00 & 0.00 \\
	9.4/1		& 0.38 & 0 & 5 & 0 & 0.00 & 0.00 & 0.00 \\
	9.5/2		& 0.42 & 1 & 4 & 0 & 0.00 & 0.00 & 0.00 \\
	9.6/1		& 0.67 & 0 & 6 & 0 & 0.00 & 0.00 & 0.00 \\
	10/2		& 0.29 & 1 & 1 & 0 & 0.00 & 0.00 & 0.00 \\
  \midrule
  Durchschnitt  & 0.40 &  &  &   & 0.00 & 0.00 & 0.00 \\
  \bottomrule
\end{longtable}

Nach dem Training haben sich die Ergebnisse sehr stark verschlechtert.
Keine einzige Frage wurde auch nur teilweise richtig beantwortet.
Sogar der Confidence-Wert ist gesunken.
Dies deutet auf eine Überspezialisierung durch die Trainingsfragen hin, vorher funktionierte es deutlich besser.
Zur Bestätigung dieser These müsste aber erst Training und Evaluierung durch die automatisch generierten Fragen stattfinden.
Eine weitaus wahrscheinlichere Möglichkeit ist aber ein großer Fehler bei der Konfiguration der Trainingsfragen oder des Systems in QAnswer.
Dieser muss noch behoben werden.

\todo{Visualisierung der Daten (CSVs in Data/Tabellen/Training\_Auswertung)}
\todo{Erklären, was das bedeutet}
\todo{Altes angleichen und updaten}