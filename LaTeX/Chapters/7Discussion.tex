%*****************************************
\chapter{Diskussion und Ausblick}\label{ch:discussion}
%*****************************************

\section{Benchmark}

Der Benchmark wurde erstellt, indem Fragen aus \citet{bb} genommen wurden und wegen ihrer theoretischen Beantwortbarkeit klassifiziert wurden.
Die Eignung dieser Fragen für ein System, das anhand der gegebenen Frage und den in ihr enthaltenen Wörter herausfinden soll, was die Antwort auf sie ist, ist manchmal fraglich.
Oft gibt es Fragen wie \texttt{9.3.4.2/1}, \enquote{Which organizational units are involved in information management?}, wo Ressourcen nicht mit ihrem Namen genannt werden.
Hier wird das Prädikat \aurl{meta}{functionComponent} verwendet, obwohl \aurl{meta}{isInvolvedIn} deutlich naheliegender ist.
Dies lässt auf eine eventuelle Untauglichkeit der Fragen für die Ontologie schließen, oder auf eine manchmal nicht optimal modellierte Datenbasis.

Es ist aber trotzdem gelungen, die gegebenen Fragen, welche auch teilweise beantwortet werden können, einen Benchmark zu erstellen.
Dieser besteht aus Fragen, welche auf in \citet{bb} vorhandenes Wissen zurückgreifen und dieses festigen soll.
Dies ist eigentlich perfekt für die Nutzung mit \ac{snik}, nur die Formulierung ist manchmal nicht perfekt.

In Zukunft sollten definitiv noch mehr Fragen erstellt werden.
Denkbar wären vor allem zwei Methoden, die relativ gut realisierbar wären:
\begin{enumerate}
  \item Studierende der Medizininformatik nach Fragen, die sie fragen würden, fragen, oder
  \item automatisch für jedes existierende Tripel eine Frage mit festem Schema generieren.
\end{enumerate}

Für die Realisierung des ersten Punktes wäre vermutlich die Nutzung ein digitaler Fragebogen möglich, über den die Studierenden benachrichtigt würden und in den sie Fragen eintragen könnten,
eine analoge Lösung wäre auch denkbar.
Problematisch hieran wäre, wenn genügend Fragen gefunden würden, der große Arbeitsaufwand für die manuelle Erstellung der \ac{sparql}-Abfragen, als auch die Möglichkeit der Ontologie, diese zu beantworten.
Die Fragen würden von Menschen erstellt, die wirklich eine Antwort auf die Frage erhalten wollen, und würden sehr individuell und vielfältig gestaltet sein.
Dies birgt aber wieder das Problem, dass sie von Question Answering-Systemen aufgrund ihrer Art, Komplexität oder Formulierung nicht beantwortet werden können.
Es ist aber trotzdem ein Ansatz, der betrachtet werden sollte.

Beim zweiten Punkt könnten alle Fragen die Struktur \emph{Fragewort -- Prädikat -- Subjekt} haben, wobei das Fragewort durch das Prädikat bestimmt werden sollte.
Dies wäre, aufgrund der vergleichsweise geringen Zahl an Prädikaten, vermutlich leicht realisierbar.
Die Antwort wäre das Subjekt oder Objekt, das andere von beiden würde jeweils die Rolle des Subjekts in der Frage einnehmen.
Je nachdem, ob das Objekt oder Subjekt gegeben ist, ändert sich natürlich auch das Fragewort.
Denkbar wären auch Fragen der Art \enquote{How are A and B related?}, um nach dem Prädikat zu fragen.
Hierbei würde eine riesige Menge an Fragen generiert werden, die aber alle die gleiche, oder zumindest sehr ähnliche, Struktur haben.
Es müssten auch manche Fragen gefiltert oder gar nicht erst generiert werden, etwa die Fragen nach dem Label oder alternativen Labels.

\section{Systeme}

Die Erwartung war, dass das Feld mittlerweile reif genug für Systeme ist, die das einfache und präzise Nutzen von semantischem Question Answering ermöglichen.
Es war aber sehr schwer, Systeme mit ausreichender Dokumentation und bereitgestelltem Programm zu finden, die noch dazu mit eigenen Daten und vielleicht sogar über mehrere Ontologien verwendbar waren.
Funktionieren taten die meisten davon nicht.

Es muss allerdings auch gesagt werden, dass \ac{snik} nicht besonders für semantisches Question Answering geeignet ist.
Dieses geht meist, wie in \cref{sub:qasysteme} gesehen, nach der Methode vor, dass die Prädikate der \ac{sparql}-Abfrage den Prädikaten des Satzes entsprechen sollen.
Da \ac{snik} auf verhältnismäßig wenigen Prädikaten beruht, deren Label noch dazu oft wenig in einem natürlichen Satz verwendet würden, ist es meist unpraktikabel, solche zu verwenden.
An Außnahmen wie \aurl{meta}{isResponsibleForFunction} sieht man, dass diese deutlich besser funktionieren, da sie etwa in der Frage \enquote{What is the CIO responsible for?} vorhanden sind.
Prädikate wie \aurl{meta}{entityTypeComponent} mit dem Label \enquote{entity type component} haben es dort schwerer, da sie auf das Metamodell \ac{snik}s zurückgreifen
und nicht den normalen Sprachgebrauch wiederspiegeln.
Hilfreich dafür sind definitiv die Property Mappings von QAnswer, aber diese haben auch nur eine auf die eingegebenen Lexikalisierungen begrenzte Wirkungsweite.

Es ist nicht, wie anfangs gewollt, gelungen, ein semantisches Question Answering-System über alle drei Teilontologien zuverlässig verwendbar zu machen,
jedoch ist es gelungen, ein Question Answering-System über eine Teilontologie zumindest theoretisch verwendbar zu machen, die Erfolgsrate ist allerdings \todo{ernüchternd/nur mangelhaft/akzeptabel (ohne "allerdings")}
Für die Verwendung der Teilontologien zusammen fehlt nicht nur ein System, dass diese Aufgabe bewältigt, sondern vor allem auch schlicht die Trainingsfragen.
Bereits die aus dem blauen Buch waren nur teilweise geeignet, für das Training für aller Teilontologien müsste man vermutlich Nutzerfragen von Studierenden sammeln.
Noch dazu kommt das Problem der Ambiguität verschiedener Begriffe, welches mit verschiedenen Teilontologien eingeführt wird.
Natürlich können die Ressourcen, die mit \aurl{skos}{closeMatch} oder \aurl{skos}{exactMatch} verbunden sind, als ähnlich, vielleicht sogar gleich definiert werden.
Aber es gibt ja auch Ressourcen, die nur über \aurl{skos}{broadMatch} oder gar nicht verbunden sind und trotzdem fast gleich heißen.
Solche Ambiguitäten gilt es aufzulösen, das ist ein weiterer Grund, warum hier nur eine Teilontologie abgefragt wurde.
