%*****************************************
\chapter{Diskussion und Ausblick}\label{ch:discussion}
%*****************************************

\section{Benchmark}

Der Benchmark wurde erstellt, indem Fragen aus \citet{bb} genommen wurden und wegen ihrer theoretischen Beantwortbarkeit klassifiziert wurden.
Die Eignung dieser Fragen für ein System, das anhand der gegebenen Frage und den in ihr enthaltenen Wörter herausfinden soll, was die Antwort auf sie ist, ist manchmal fraglich.
Oft gibt es Fragen wie \texttt{9.3.4.2/1}, \enquote{Which organizational units are involved in information management?}, wo Ressourcen nicht mit ihrem Namen genannt werden.
Hier wird das Prädikat \aurl{meta}{functionComponent} verwendet, obwohl \aurl{meta}{isInvolvedIn} deutlich naheliegender ist.
Dies lässt auf eine eventuelle Untauglichkeit der Fragen für die Ontologie schließen, oder auf eine manchmal nicht optimal modellierte Datenbasis.

Die Antworten von paar Fragen enthalten noch falsche Ressourcen oder es fehlen wichtige Antworten, dies sollte noch behoben werden.
Es sollte allerdings kein großes Hindernis sein, da die \ac{sparql}-Abfragen größtenteils richtig sind und ein paar Fehler auch durch die Ontologie selbst stammen könnten,
also nicht die \ac{sparql}-Abfragen und somit das Training beeinflussen.

Es ist aber trotzdem gelungen, die gegebenen Fragen, welche auch teilweise beantwortet werden können, einen Benchmark zu erstellen.
Dieser besteht aus Fragen, welche auf in \citet{bb} vorhandenes Wissen zurückgreifen und dieses festigen soll.
Dies ist eigentlich perfekt für die Nutzung mit \ac{snik}, nur die Formulierung ist manchmal nicht optimal.
Im Großen und Ganzen lässt sich allerdings auch sagen, dass die Fragen für Question Answering von einfachen Fragen definitiv höchst komplex waren, was auch nicht unterschlagen werden darf.

Die simpleren Fragen waren recht schnell generierbar und haben die große Datenmenge \ac{snik}s gut genutzt,
auch wenn durch die Anzahl der Tripel in der Teilontologie \texttt{bb}, 2056, erst mehr Fragen erhofft waren.
Dies war allerdings aufgrund von fehlenden Labels oder Subjekten mit vielen Objekten über das gleiche Prädikat verbunden waren und somit die gleiche Frage erzeugten,
was über das Schlüsselwort \texttt{DISTINCT} gekürzt wurde.
Es könnte darüber nachgedacht werden, die automatisch generierten Fragen komplexer zu machen, dies ist aber nicht unbedingt erforderlich.
Es konnten viele Fragen automatisch generiert werden, was viel mehr Trainingsfragen bedeutet hat, was auch dem fine-tuning hilft.
Gerade die Fragen mit verschiedenen Intentionen sind hilfreich.

In Zukunft sollten definitiv noch mehr komplexere Fragen erstellt werden.
Denkbar wäre vor allem, Studierende der Medizininformatik nach Fragen, die sie fragen würden, zu fragen.
Für die Realisierung wäre vermutlich die Nutzung ein digitaler Fragebogen möglich, über den die Studierenden benachrichtigt würden und in den sie Fragen eintragen könnten,
eine analoge Lösung wäre auch denkbar.
Problematisch hieran wäre, wenn genügend Fragen gefunden würden, der große Arbeitsaufwand für die manuelle Erstellung der \ac{sparql}-Abfragen, als auch die Möglichkeit der Ontologie, diese zu beantworten.
Die Fragen würden von Menschen erstellt, die wirklich eine Antwort auf die Frage erhalten wollen, und würden sehr individuell und vielfältig gestaltet sein.
Dies birgt aber wieder das Problem, dass sie von Question Answering-Systemen aufgrund ihrer Art, Komplexität oder Formulierung nicht beantwortet werden können.
Es ist aber trotzdem ein Ansatz, der betrachtet werden sollte.

\section{Systeme}

Die Erwartung war, dass das Feld mittlerweile reif genug für Systeme ist, die das einfache und präzise Nutzen von semantischem Question Answering ermöglichen.
Es war aber sehr schwer, Systeme mit ausreichender Dokumentation und bereitgestelltem Programm zu finden, die noch dazu mit eigenen Daten und vielleicht sogar über mehrere Ontologien verwendbar waren.
Funktionieren taten die meisten davon nicht.

Es muss allerdings auch gesagt werden, dass \ac{snik} nicht besonders für semantisches Question Answering geeignet ist.
Dieses geht meist, wie in \cref{sub:qasysteme} gesehen, nach der Methode vor, dass die Prädikate der \ac{sparql}-Abfrage den Prädikaten des Satzes entsprechen sollen.
Da \ac{snik} auf verhältnismäßig wenigen Prädikaten beruht, deren Label noch dazu oft wenig in einem natürlichen Satz verwendet würden, ist es meist unpraktikabel, solche zu verwenden.
An Außnahmen wie \aurl{meta}{isResponsibleForFunction} sieht man, dass diese deutlich besser funktionieren, da sie etwa in der Frage \enquote{What is the CIO responsible for?} vorhanden sind.
Prädikate wie \aurl{meta}{entityTypeComponent} mit dem Label \enquote{entity type component} haben es dort schwerer, da sie auf das Metamodell \ac{snik}s zurückgreifen
und nicht den normalen Sprachgebrauch wiederspiegeln.
Hilfreich dafür sind definitiv die Property Mappings von QAnswer, aber diese haben auch nur eine auf die eingegebenen Lexikalisierungen begrenzte Wirkungsweite.

Es ist nicht, wie anfangs gewollt, gelungen, ein semantisches Question Answering-System über alle drei Teilontologien zuverlässig verwendbar zu machen,
jedoch ist es gelungen, ein Question Answering-System über eine Teilontologie zumindest theoretisch verwendbar zu machen, die Erfolgsrate ist beim untrainierten System bei den gegebenen Umständen mit einem F-Maß von 0.24 erstaunlich, jedoch war das Training enttäuschend.
Es bleibt abzuwarten, was das Training mit dem großen, automatisch generierten Benchmark verursacht
Vermutlich waren die 18 Fragen, die zum Training eingesetzt wurden, deutlich zu wenige, um irgendeine Verallgemeinerung möglich zu machen und das System auf die Ontologie einzustellen.

Für die Verwendung der Teilontologien zusammen fehlt nicht nur ein System, dass diese Aufgabe bewältigt, sondern vor allem auch schlicht die Trainingsfragen.
Bereits die aus dem blauen Buch waren nur teilweise geeignet, für das Training für aller Teilontologien müsste man vermutlich Nutzerfragen von Studierenden sammeln.
Noch dazu kommt das Problem der Ambiguität verschiedener Begriffe, welches mit verschiedenen Teilontologien eingeführt wird.
Natürlich können die Ressourcen, die mit \aurl{skos}{closeMatch} oder \aurl{skos}{exactMatch} verbunden sind, als ähnlich, vielleicht sogar gleich definiert werden.
Aber es gibt ja auch Ressourcen, die nur über \aurl{skos}{broadMatch} oder gar nicht verbunden sind und trotzdem fast gleich heißen.
Solche Ambiguitäten gilt es aufzulösen, das ist ein weiterer Grund, warum hier nur eine Teilontologie abgefragt wurde.

In Zukunft sollte das System definitiv, wie schon geplant, auch am automatisch definierten Benchmark evaluiert werden und der Effekt der Anzahl und des Typs der Fragen ausgewertet werden.
Auch denkbar wäre das Erstellen eines eigenen, auf \ac{snik} spezialisierten, Question Answering-Systems, was aber aufgrund der hohen Komplexität unpraktisch erscheint.

Für eine Nutzung des Systems durch Studenten sind die Ergebnisse momentan aber nicht ausreichend.
Infrage kommend wäre höchstens das untrainierte System, welches eine halbwegs akkurate Beantwortung von Fragen, besonders leichteren, ermöglicht.
