%*****************************************
\chapter{Lösungsansatz}\label{ch:approach}
%*****************************************

\section{Lösungsansatz zum Problem P1}

Das erste Problem befasst sich mit der Situation, dass ein Question Answering-System im Rahmen dieser Arbeit aufgrund der Aufwendigkeit schwer selbst entwickelt werden kann.
Stattdessen muss ein bereits existierendes System ausgewählt werden.
Hierzu müssen andere Systeme anhand eines Benchmarks evaluiert werden.
Für diesen müssen typische Nutzerfragen und Antworten auf diese gesammelt werden.
Dazu bieten sich die Fragen aus den Büchern an, welche schon von \citet{arneba} gesammelt und hinsichtlich der Beantwortbarkeit mit der \ac{snik}-Ontologie klassifiziert worden sind.
Für diese müssen Antworten herausgesucht und in \ac{sparql}-Abfragen umgeformt werden, um das Training zu ermöglichen.
Da dies jedoch nicht unbedingt genug Fragen sind, müssen weitere durch \ac{snik} beantwortbare Fragen formuliert werden.
Dies soll jedoch aufgrund der Zeitbeschränkung und etwas besserem Training von Modellen nur in der englischen Sprache geschehen.
Etwa die Hälfte der Fragen soll zum Training, die andere Hälfte zur Kontrolle, jeweils vor und nach dem Training, genutzt werden,
um Daten hinsichtlich der Effektivität des Trainings und letztendlich des Systems in der Fragenbeantwortung zu erhalten.

\subsection{Evaluierungsmaße}

Zur Evaluierung der Systeme werden bei den Fragen je die Indikatoren der Genauigkeit (\emph{precision}) $p$, Trefferquote (\emph{recall}) $r$ und F-Maß $f$ verwendet.
Die Genauigkeit lässt sich berechnen indem man die Anzahl richtigen Systemantworten durch die Anzahl aller Systemantworten teilt.
Wenn ein System beispielsweise insgesamt neun Antworten zurück gibt, davon jedoch nur drei richtig sind, hat es eine Genauigkeit von $\frac{1}{3}$.
Wenn das System gar keine Antworten zurück gibt, wird eine Genauigkeit von $0$ angenommen.
Die Trefferquote lässt sich berechnen, indem man die Anzahl der richtigen Systemantworten durch die Anzahl aller richtigen Antworten teilt.
Wenn es beispielsweise bei der vorherigen Frage insgesamt sechs richtige Antworten gibt, hat das System eine Trefferquote von $\frac{1}{2}$.
Das F-Maß wird durch das harmonische Mittel der Genauigkeit und Präzision berechnet, wäre in diesem Beispiel also $\frac{2}{5}$.
Die Werte lassen sich pro Frage also wie folgt berechnen, wenn $C$ die Menge aller möglichen korrekten Antworten ist und $O$ die Menge aller Systemantworten:
\begin{align*}
  p&=\frac{|O \cap C|}{|O|} \\
  r&=\frac{|O \cap C|}{|C|} \\
  f&=\frac{2pr}{p+r} \\
\end{align*}
Am Ende sollen die Werte für alle Fragen mithilfe des arithmetischen Mittels zusammengefasst werden, sodass am Ende die drei Werte jeweils gemittelt vorliegen.
Dies ist eine Standardmethode bei der Evaluation von Question Answering-Systemen und wird auch als \emph{macro-average} bezeichnet \citep{qald9}.

\section{Lösungsansatz zum Problem P2}

Das zweite Problem betrifft die mangelhafte Nützlichkeit der anderen Möglichkeiten und somit den Zwang, für solch ein System Question Answering zu verwenden.
Deshalb muss nach funktionierenden und somit portablen Systemen, die nach Möglichkeit trainiert werden können und vor allem nicht nur Ontologien wie DBpedia als Quelle verwenden,
sondern es auch ermöglichen, eigene Daten zu nutzen.
Zur Recherche sollen, wie bereits in \cref{ch:relatedWork} dargelegt, Surveys und das Question Answering Leaderboard verwendet werden.
Diese geben über die Güte der Antworten und die grundlegende Funktionsweise Aufschluss, können aber, besonders bei Forschungsprojekten,
die nur über eine bestimmte Zeit finanzielle Mittel erhalten und somit die Pflege nicht gewährleistet werden kann, schnell veralten.
Besonders bei Benchmarks wie \ac{qald}-9 ist es außerdem oft so, dass Systeme nur für Ontologien wie Wikidata und die Fragen im Benchmark erstellt wurden und somit nicht portabel sind.
Gerade das Leaderboard ist jedoch eine nützliche Ressource, da hier viele Question Answering-Systeme mit dem entsprechenden Paper, was sonst auch nicht überall vorliegt, und, falls diese existiert,
eine Demo des Systems, was der anfänglichen Evaluation hilft.
Ziel ist es, mindestens ein System zu finden, und, falls mehr, aus den gefundenen das beste System auszuwählen.
Es wird allerdings nur die Teilontologie aus \citet{bb} betrachtet, da es kein System schafft, diese zu verbinden.
