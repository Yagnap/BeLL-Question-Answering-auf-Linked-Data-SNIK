%*****************************************
\chapter{Lösungsansatz}\label{ch:approach}
%*****************************************

\section{Lösungsansatz zum Problem P1}

Das erste Problem befasst sich mit der Situation, dass ein \ac{qa}-System im Rahmen dieser Arbeit aufgrund des Aufwands schwer selbst entwickelt werden kann.
Stattdessen muss ein bereits existierendes System ausgewählt werden.
Hierzu müssen andere Systeme anhand eines Benchmarks evaluiert werden.
Für diesen müssen typische Nutzerfragen und Antworten auf diese gesammelt werden.

Dazu bieten sich die Fragen aus den Büchern an, welche schon von \citet{arneba} gesammelt und hinsichtlich der Beantwortbarkeit mit der \ac{snik}-Ontologie klassifiziert worden sind.
Für diese müssen Antworten herausgesucht und in \ac{sparql}-Abfragen umgeformt werden, um das Training zu ermöglichen.
Da dies jedoch nicht unbedingt genug Fragen sind, müssen weitere durch \ac{snik} beantwortbare Fragen formuliert werden.
Dies geschieht jedoch aufgrund der Zeitbeschränkung und etwas besserem Training von Modellen nur in der englischen Sprache.
Etwa die Hälfte der Fragen wird zum Training, die andere Hälfte zur Kontrolle jeweils vor und nach dem Training genutzt,
um Daten hinsichtlich der Effektivität des Trainings und letztendlich des Systems in der Fragenbeantwortung zu erhalten.
Da dies aber vermutlich nicht genügend Fragen sind, gibt es noch die Möglichkeit der automatischen Generierung von Fragen, in dem für jedes Tripel automatisch zwei Fragen generiert werden,
eine nach dem Objekt und eine nach dem Subjekt.
Die Fragen können etwa die Struktur \emph{Fragewort -- Prädikat -- Subjekt} haben, wobei das Fragewort durch das Prädikat bestimmt werden kann.
Dies ist, aufgrund der vergleichsweise geringen Zahl an Prädikaten, leicht realisierbar.
Die Antwort ist das Subjekt oder Objekt, das andere von beiden würde jeweils die Rolle des Subjekts in der Frage einnehmen.
Je nachdem, ob das Objekt oder Subjekt gegeben ist, ändert sich natürlich auch das Fragewort.
Hierbei wird eine riesige Menge an Fragen generiert, die aber alle die gleiche, oder zumindest sehr ähnliche, Struktur haben.
Es müssen auch manche Fragen gefiltert oder gar nicht erst generiert werden, etwa die Fragen nach dem Label oder alternativen Labels.
Es sollen also keine Beziehungen zwischen Literalen abgefragt werden, nur zwischen Ressourcen.
Dadurch kann die große Datenmenge \ac{snik}s genutzt werden.
Es werden 100 in die Test- und der Rest in die Trainingsgruppe eingeteilt.

Um den Effekt des Typs der Fragen in der Trainingsgruppe auszuwerten,
werden die beiden Frage-Antwort-Paaren bestehenden Datensätze sowohl je getrennt und einmal zusammen auf den gleichen, untrainierten Datensatz angewandt.
Zur Untersuchung des Effekts der Anzahl der Fragen werden in Schritten von zehn Fragen automatisch neue,
untrainierte Modelle erstellt und mit der entsprechenden Anzahl von Fragen trainiert.

\subsection{Evaluierungsmaße}\label{sub:evaluierungsmasse}

Zur Evaluierung der Systeme werden bei den Fragen je die Indikatoren der Genauigkeit (\emph{precision}) $p$, Trefferquote (\emph{recall}) $r$ und F-Maß $f$ verwendet.
Die Genauigkeit lässt sich berechnen indem man die Anzahl richtigen Systemantworten durch die Anzahl aller Systemantworten teilt.
Wenn ein System beispielsweise insgesamt neun Antworten zurück gibt, davon jedoch nur drei richtig sind, hat es eine Genauigkeit von $\frac{1}{3}$.
Wenn das System gar keine Antworten zurück gibt, wird eine Genauigkeit von $0$ angenommen.
Die Trefferquote lässt sich berechnen, indem man die Anzahl der richtigen Systemantworten durch die Anzahl aller richtigen Antworten teilt.
Wenn es beispielsweise bei der vorherigen Frage insgesamt sechs richtige Antworten gibt, hat das System eine Trefferquote von $\frac{1}{2}$.
Das F-Maß wird durch das harmonische Mittel der Genauigkeit und Präzision berechnet, wäre in diesem Beispiel also $\frac{2}{5}$.
Die Werte lassen sich pro Frage also wie folgt berechnen, wenn $C$ die Menge aller möglichen korrekten Antworten ist und $O$ die Menge aller Systemantworten:
\begin{align*}
  p&=\frac{|O \cap C|}{|O|} \\
  r&=\frac{|O \cap C|}{|C|} \\
  f&=\frac{2pr}{p+r}
\end{align*}
Am Ende sollen die Werte für alle Fragen mithilfe des arithmetischen Mittels zusammengefasst werden, sodass am Ende die drei Werte jeweils gemittelt vorliegen.
Dies ist eine Standardmethode bei der Evaluation von \ac{qa}-Systemen und wird auch als \emph{macro-average} bezeichnet \citep{qald9}.

\begin{align*}
  \emptyset p&=\frac{1}{n}*\sum_{i=1}^{n}p_i \\
  \emptyset r&=\frac{1}{n}*\sum_{i=1}^{n}r_i \\
  \emptyset f&=\frac{1}{n}*\sum_{i=1}^{n}f_i
\end{align*}

\section{Lösungsansatz zum Problem P2}

Das zweite Problem betrifft die mangelhafte Gebrauchstauglichkeit der anderen Möglichkeiten und somit den Zwang, für solch ein System \acl{qa} zu verwenden.
Deshalb muss nach funktionierenden und somit portablen Systemen gesucht werden.
Diese sollten nach Möglichkeit trainiert werden können und vor allem nicht nur Wissensbasen wie DBpedia als Quelle verwenden, sondern müssen es auch ermöglichen, eigene Daten zu nutzen, da die Verwendung mit \ac{snik} sonst unmöglich ist.
Zur Recherche sollen, wie bereits in \cref{ch:relatedWork} dargelegt, Surveys und das \ac{qa}-Leaderboard verwendet werden.
Diese geben über die Güte der Antworten und die grundlegende Funktionsweise Aufschluss, können aber, besonders bei Forschungsprojekten,
die nur über eine bestimmte Zeit finanzielle Mittel erhalten und somit die Pflege nicht gewährleistet werden kann, schnell veralten.
Besonders bei Benchmarks wie \ac{qald}-10 ist es außerdem oft so, dass Systeme nur für große Wissensbasen wie Wikidata und die Fragen im Benchmark erstellt wurden und somit nicht portabel sind.
Gerade das Leaderboard ist jedoch eine nützliche Ressource, da hier viele \ac{qa}-Systeme mit dem entsprechenden Paper, was sonst auch nicht überall vorliegt, und, falls diese existiert,
eine Demo des Systems, was der anfänglichen Evaluation hilft.
Ziel ist es, mindestens ein System zu finden, und, falls mehr, aus den gefundenen das beste System auszuwählen.
Es wird allerdings nur die Teilontologie aus \citet{bb} betrachtet, da es kein gefundenes System es schafft, alle Teilontologien zu verbinden.
Aufgrund der Komplexität wurde also nur die Abfrage einer Teilontologie als Ziel gesetzt.
