%*****************************************
\chapter{Lösungsansatz}\label{ch:approach}
%*****************************************

\section{Lösungsansatz zum Problem P1}

Das erste Problem befasst sich mit der Situation, dass ein Question Answering-System im Rahmen dieser Arbeit aufgrund der Aufwendigkeit schwer selbst entwickelt werden kann.
Stattdessen muss ein bereits existierendes System ausgewählt werden.
Hierzu müssen andere Systeme anhand eines Benchmarks evaluiert werden.
Für diesen müssen typische Nutzerfragen und Antworten auf diese gesammelt werden.

Dazu bieten sich die Fragen aus den Büchern an, welche schon von \citet{arneba} gesammelt und hinsichtlich der Beantwortbarkeit mit der \ac{snik}-Ontologie klassifiziert worden sind.
Für diese müssen Antworten herausgesucht und in \ac{sparql}-Abfragen umgeformt werden, um das Training zu ermöglichen.
Da dies jedoch nicht unbedingt genug Fragen sind, müssen weitere durch \ac{snik} beantwortbare Fragen formuliert werden.
Dies soll jedoch aufgrund der Zeitbeschränkung und etwas besserem Training von Modellen nur in der englischen Sprache geschehen.
Etwa die Hälfte der Fragen soll zum Training, die andere Hälfte zur Kontrolle, jeweils vor und nach dem Training, genutzt werden,
um Daten hinsichtlich der Effektivität des Trainings und letztendlich des Systems in der Fragenbeantwortung zu erhalten.

Da dies aber vermutlich nicht genügend Fragen sein werden, gibt es noch die Möglichkeit der automatischen Generierung von Fragen, in dem für jedes Tripel automatisch zwei Fragen generiert werden,
eine nach dem Objekt und eine nach dem Subjekt.
Die Fragen könnten etwa die Struktur \emph{Fragewort -- Prädikat -- Subjekt} haben, wobei das Fragewort durch das Prädikat bestimmt werden kann.
Dies ist, aufgrund der vergleichsweise geringen Zahl an Prädikaten, leicht realisierbar.
Die Antwort ist das Subjekt oder Objekt, das andere von beiden würde jeweils die Rolle des Subjekts in der Frage einnehmen.
Je nachdem, ob das Objekt oder Subjekt gegeben ist, ändert sich natürlich auch das Fragewort.
Hierbei wird eine riesige Menge an Fragen generiert werden, die aber alle die gleiche, oder zumindest sehr ähnliche, Struktur haben.
Es müssten auch manche Fragen gefiltert oder gar nicht erst generiert werden, etwa die Fragen nach dem Label oder alternativen Labels.
Es sollen also keine Beziehungen zwischen Literalen abgefragt werden, nur zwischen Ressourcen.
Dadurch kann die große Datenmenge \ac{snik}s genutzt werden.
Es sollen 100 in die Test- und der Rest in die Trainingsgruppe eingeteilt werden.

Um den Effekt des Typs der Fragen in der Trainingsgruppe auszuwerten, sollen die beiden Fragensätze sowohl je getrennt und einmal zusammen auf den gleichen, untrainierten Datensatz angewandt werden.
Zur Untersuchung des Effekts der Anzahl der Fragen sollen in Schritten von einhundert Fragen automatisch neue, untranierte Datensätze erstellt und mit der entsprechenden Anzahl von Fragen trainiert werden.

\subsection{Evaluierungsmaße}

Zur Evaluierung der Systeme werden bei den Fragen je die Indikatoren der Genauigkeit (\emph{precision}) $p$, Trefferquote (\emph{recall}) $r$ und F-Maß $f$ verwendet.
Die Genauigkeit lässt sich berechnen indem man die Anzahl richtigen Systemantworten durch die Anzahl aller Systemantworten teilt.
Wenn ein System beispielsweise insgesamt neun Antworten zurück gibt, davon jedoch nur drei richtig sind, hat es eine Genauigkeit von $\frac{1}{3}$.
Wenn das System gar keine Antworten zurück gibt, wird eine Genauigkeit von $0$ angenommen.
Die Trefferquote lässt sich berechnen, indem man die Anzahl der richtigen Systemantworten durch die Anzahl aller richtigen Antworten teilt.
Wenn es beispielsweise bei der vorherigen Frage insgesamt sechs richtige Antworten gibt, hat das System eine Trefferquote von $\frac{1}{2}$.
Das F-Maß wird durch das harmonische Mittel der Genauigkeit und Präzision berechnet, wäre in diesem Beispiel also $\frac{2}{5}$.
Die Werte lassen sich pro Frage also wie folgt berechnen, wenn $C$ die Menge aller möglichen korrekten Antworten ist und $O$ die Menge aller Systemantworten:
\begin{align*}
  p&=\frac{|O \cap C|}{|O|} \\
  r&=\frac{|O \cap C|}{|C|} \\
  f&=\frac{2pr}{p+r}
\end{align*}
Am Ende sollen die Werte für alle Fragen mithilfe des arithmetischen Mittels zusammengefasst werden, sodass am Ende die drei Werte jeweils gemittelt vorliegen.
Dies ist eine Standardmethode bei der Evaluation von Question Answering-Systemen und wird auch als \emph{macro-average} bezeichnet \citep{qald9}.

\begin{align*}
  \emptyset p&=\frac{1}{n}*\sum_{i=1}^{n}p_i \\
  \emptyset r&=\frac{1}{n}*\sum_{i=1}^{n}r_i \\
  \emptyset f&=\frac{1}{n}*\sum_{i=1}^{n}f_i
\end{align*}

\section{Lösungsansatz zum Problem P2}

Das zweite Problem betrifft die mangelhafte Nützlichkeit der anderen Möglichkeiten und somit den Zwang, für solch ein System Question Answering zu verwenden.
Deshalb muss nach funktionierenden und somit portablen Systemen, die nach Möglichkeit trainiert werden können und vor allem nicht nur Wissensbasen wie DBpedia als Quelle verwenden,
sondern es auch ermöglichen, eigene Daten zu nutzen.
Zur Recherche sollen, wie bereits in \cref{ch:relatedWork} dargelegt, Surveys und das Question Answering Leaderboard verwendet werden.
Diese geben über die Güte der Antworten und die grundlegende Funktionsweise Aufschluss, können aber, besonders bei Forschungsprojekten,
die nur über eine bestimmte Zeit finanzielle Mittel erhalten und somit die Pflege nicht gewährleistet werden kann, schnell veralten.
Besonders bei Benchmarks wie \ac{qald}-9 ist es außerdem oft so, dass Systeme nur für Ontologien wie Wikidata und die Fragen im Benchmark erstellt wurden und somit nicht portabel sind.
Gerade das Leaderboard ist jedoch eine nützliche Ressource, da hier viele Question Answering-Systeme mit dem entsprechenden Paper, was sonst auch nicht überall vorliegt, und, falls diese existiert,
eine Demo des Systems, was der anfänglichen Evaluation hilft.
Ziel ist es, mindestens ein System zu finden, und, falls mehr, aus den gefundenen das beste System auszuwählen.
Es wird allerdings nur die Teilontologie aus \citet{bb} betrachtet, da es kein gefundenes System es schafft, alle Teilontologien zu verbinden.
Aufgrund der Komplexität wurde also nur die Abfrage einer Teilontologie als Ziel gesetzt.
