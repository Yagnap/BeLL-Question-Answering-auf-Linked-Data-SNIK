% !TeX encoding = UTF-8
% !TeX spellcheck = de_DE

%% Dies gibt Warnungen aus, sollten veraltete LaTeX-Befehle verwendet werden
\RequirePackage[l2tabu, orthodox]{nag}

\documentclass[utf8,biblatex]{lni}
\bibliography{../Dokumentation/Bibliography, ../Dokumentation/snik}

%% Schöne Tabellen mittels \toprule, \midrule, \bottomrule
\usepackage{booktabs}

%% Zu Demonstrationszwecken
\usepackage[math]{blindtext}
\usepackage{mwe}

%% Akronyme
\usepackage{acronym}

% Listings

\lstdefinelanguage{SPARQl}{
  language=SQL,
  morekeywords={PREFIX, a, CONSTRUCT},
  sensitive=true,
  % DISABLE FOR PRINTING
  morecomment=[l][\color{OliveGreen}]{\#},
  morestring=[b][\color{Orange}]\",
  % ENABLE FOR PRINTING
  %morecomment=[l][\color{Black}]{\#},
  %morestring=[b][\color{Black}]\",
  %keywordstyle=\color{Black}
}

%% BibLaTeX-Sonderkonfiguration,
%% falls man schnell eine existierende Bibliographie wiederverwenden will, aber nicht die .bib-Datei händisch anpassen möchte.
%% Bitte \iffalse und \fi entfernen, dann ist diese Konfiguration aktiviert.

\AtEveryBibitem{%
  \ifentrytype{article}{%
  }{%
    \clearfield{doi}%
    \clearfield{issn}%
    \clearfield{url}%
    \clearfield{urldate}%
  }%
  \ifentrytype{inproceedings}{%
  }{%
    \clearfield{doi}%
    \clearfield{issn}%
    \clearfield{url}%
    \clearfield{urldate}%
  }%
}

\begin{document}
%%% Mehrere Autoren werden durch \and voneinander getrennt.
%%% Die Fußnote enthält die Adresse sowie eine E-Mail-Adresse.
%%% Das optionale Argument (sofern angegeben) wird für die Kopfzeile verwendet.
\title[Question Answering auf SNIK]{Question Answering auf einer Ontologie des Informationsmanagements im Krankenhaus}
%%%\subtitle{Untertitel / Subtitle} % falls benötigt
\author[Hannes R. Brunsch]% \and Konrad Höffner]
{Hannes R. Brunsch\footnote{Wilhelm-Ostwald-Schule, Gymnasium der Stadt Leipzig, Willi-Bredel-Straße 15, 04279 Leipzig, Deutschland \email{hrbrunsch@gmail.com}}}
%\and  Konrad Höffner\footnote{Universität Leipzig, Institut für Medizinische Informatik, Statistik und Epidemiologie, Härtelstraße 16--18, 04107 Leipzig, Deutsche \email{konrad.hoeffner@uni-leipzig.de}}}
\startpage{11} % Beginn der Seitenzählung für diesen Beitrag
\editor{Gesellschaft für Informatik}    % Namen der Herausgeber
\booktitle{Studierendenkonferenz Informatik} % Name des Tagungsband; optional Kurztitel
\yearofpublication{2023}
%%%\lnidoi{18.18420/provided-by-editor-02} % Falls bekannt
\maketitle

\begin{abstract}
Mit der beständig fortschreitenden Digitalisierung im Gesundheitswesen wird es immer wichtiger, auch das Wissen über das Informationsmanagement,
also die Verarbeitung von Informationen und die dazu nötigen Schritte u.v.a.m. dort digital und strukturiert erreichbar zu machen.
Diese Arbeit beschäftigt sich mit der vom IMISE entwickelten Wissensbasis SNIK.
Diese enthält Wissen aus dem Bereich des Informationsmanagements im Krankenhaus und soll künftig auch bei dem Studium der Medizininformatik helfen.
Das Wissen soll mittels geschriebener natürlicher Sprache durchsuchbar sein.
Eine Möglichkeit hierfür ist Question Answering.
Es soll möglich sein, dass ein Nutzer eine englische Frage in Satzform stellt und darauf eine Antwort bekommt.
Hierfür gibt es verschiedene Systeme, viele sind allerdings auf andere Wissensbasen spezialisiert.
Das Ziel dieser Arbeit war, nach Systemen zum Question Answering zu recherchieren und letztendlich eines auf SNIK anzuwenden.
Die Antworten wurden außerdem anhand eines vorher definierten Fragenkataloges auf ihre Genauigkeit hin überprüft und bewertet.
Als Kandidat wird QAnswer vorgeschlagen.  
\end{abstract}

\begin{keywords}
Semantic Web \and Question Answering \and Knowledge Graph Question Answering \and Closed Domain Question Answering \and SNIK \and QAnswer
\end{keywords}

%\input{Acronym.tex}

\section{Einleitung}

Das semantische Netz des Informationsmanagements im Krankenhaus (SNIK) ist eine
die Domäne des Informationsmanagements im Krankenhaus betreffende Ontologie \cite{domaene}.
Sie behandelt Wissen über Krankenhausinformationssysteme und deren Management.
Dieses wurde aus drei Lehrbüchern \cite{bb,ob,he} und einem Interview \cite{ciosurvey} manuell extrahiert und in RDF modelliert.

Momentan müssen Studierende der Medizininformatik, die nach Wissen suchen, auf eine der drei oben genannten Optionen zurückgreifen.
Jede dieser Möglichkeiten hat jedoch große Nachteile.
Der Resource Description Format (RDF)-Browser gibt nur ein sehr beschränktes Ergebnis aus, und serialisiertes RDF selbst zu lesen ist schwer und unpraktisch.
Die Graphvisualisierung kann im Zweifel unübersichtlich oder überwältigend sein, da es schwer sein kann, überhaupt die Ressource zu finden, zu der man eine Frage hat, und dann zur Antwort zu navigieren.
Im Fall von SPARQL Protocol and RDF Query Language (SPARQL) gibt es einen erheblichen Zeitaufwand für die Studierenden, da sie sich dort erst in die Syntax der Abfragesprache und das Vokabular des Fachbereichs einarbeiten müssen.

Daraus ergibt sich das Problem, dass keine der momentan existierenden Lösungen intuitiv genug funktioniert, als dass es nahezu keine Einarbeitungszeit gibt.
Die existierenden Lösungen liefern zudem nicht übersichtlich ausreichend Informationen, ihrer Expressivität sind demnach deutliche Grenzen gesetzt.

Obwohl ein Ansatz für die Lösung dieses Problems besteht, Question Answering (QA), wirft dieser direkt ein neues Problem für die entwickelnden Personen auf.
Die Implementierung eines QA-Systems mit adäquater Qualität der Antworten ist wesentlich aufwändiger~\citep[S.~3]{qanswer}, als es in einem angemessenem Zeitraum bei stark beschränkten Mitteln möglich ist.

Das Wissen zum Informationsmanagement im Krankenhaus ist komplex und oft nur schwer greifbar.
Es liegt in Form von Lehrbüchern, aber auch in SNIK vor.

Studierende haben selten Zeit, sich ganze Kapitel oder gar Bücher durchzulesen, verfügen jedoch auch nicht über die Kenntnisse SNIK effektiv zu verwenden.
Als Folge müssen sie bei Fragen oft ihren Professor oder andere Studierende hinzuziehen.
Es wäre ungemein einfacher, wenn sie das strukturierte Wissen in natürlicher Sprache abfragen könnten.
QA-Systeme sind im Idealfall 24 Stunden am Tag und 7 Tage die Woche leicht erreichbar und können sofort antworten. 
Besonders in Zeiten während und nach der Covid-19-Pandemie und immer mehr remote work, in denen direkte Kontakte mit Studierenden und Professoren oft eingeschränkt werden müssen und es digitale Veranstaltungen ohne örtliche Präsenz gibt, ist solch ein Werkzeug sehr hilfreich.

\section{Grundlagen}

\subsection{SNIK}

Die in RDF modellierten Quellen SNIKs werden alle in je einer Ontologie abgebildet.
Teilontologien werden mittels der Metaontologie modelliert und miteinander verbunden.
Deshalb ist SNIK sowohl eine Wissensbasis als auch Ontologie:
Es werden zwar keine einzelnen Krankenhäuser abgebildet, aber das Wissen aus verschiedenen Lehrbüchern, die im allgemeinen davon handeln.
Es gibt also mehrere Abstraktionsebenen.
Dadurch ist SNIK eine Ontologie mit Charakteristiken einer Wissensbasis.
Es werden zwar keine wirklichen Individuen abgebildet
- das Wissen liegt immer noch abstrakt aus Lehrbüchern vor, keine speziellen Krankenhäuser und ihre bestimmten Elemente werden betrachtet -
jedoch gibt die Metaontologie allem eine einer Wissensbasis ähnliche Struktur.

\includegraphics[width=\linewidth]{../Dokumentation/Images/snik-metamodel.pdf}\label{fig:snik-metamodel}

In \cref{fig:snik-metamodel} ist die Metaontologie dargestellt.
Von den drei Klassen \emph{Role}, \emph{EntityType} und \emph{Function} stammen alle anderen Klassen der Teilontologien ab.
Sie beschreiben Personen, Prozesse und Informationen im Krankenhaus.
Alle in der Ontologie vorhandenen Prädikate sind in der Metaontologie definiert.
So sind zwischen den in verschiedenen Teilontologien ähnliche Ressourcen in Relation gesetzt und die Beziehungen zwischen Ressourcen in den einzelnen Teilontologien dargestellt.

\subsection{Question Answering}

QA behandelt die Beantwortung von Benutzerfragen \citep{qadefinition}.
Ein QA-System muss eine Frage analysieren, eine oder mehrere Antworten bereitstellen und dem Nutzer diese präsentieren.
Die Fragen sind in natürlicher Sprache gestellt, es wird Natural Language Processing (NLP) benötigt, um sie zu verarbeiten.

Es existiert sowohl Closed Domain Question Answering (CDQA) als auch Open Domain Question Answering (ODQA).
Hier wird sich jedoch auf CDQA beschränkt, mit der durch SNIK gegebenen Domäne der Medizininformatik.
Solche QA-Programme beschränken sich auf einen Fachbereich.

Semantic Question Answering (SQA), oder auch Knowledge Base Question Answering (KBQA) bzw. KGQA, ist die Beantwortung von Fragen, die RDF-Daten in natürlicher Sprache gestellt werden.
Ein Programm für SQA erkennt verschiedene semantische Strukturen in der Frage,
zum Beispiel was für ein Typ die Antwort auf die Frage sein soll, wie etwa eine Zeit oder ein Ort \citep{sqadefinition}.
Dafür wird NLP verwendet.

Fragen werden zudem anhand ihrer Komplexität in simple und komplexe Fragen unterschieden.
Die Komplexität einer Frage leitet sich daraus ab, wie viele Schritte zum Erreichen der Antwort nötig sind.
Bei der Formulierung einer SPARQL-Abfrage zum Erhalten der Antwort lässt sich das gut erkennen.
Es ist zum Beispiel wichtig, wie viele Tripel zur Verknüpfung der Frage mit der Antwort gebraucht, oder ob Filter oder ähnliches verwendet werden.

So ist beispielsweise die Frage \enquote{Wer ist für das jährliche IT-Budget verantwortlich?} nicht komplex,
da hier nur die Beziehung zwischen \texttt{bb:AnnualITBudget} und den jeweiligen Antworten benötigt werden.
Die Frage \enquote{Welche Aufgaben hat die Person, welche auch für das jährliche IT-Budget verantwortlich ist?} könnte schon als komplex gelten,
da hier zwei oder drei Tripel für die eigentliche SPARQL-Abfrage benötigt werden:
Eines, um herauszufinden, wer alles für das jährliche IT-Budget verantwortlich ist;
eines, um ggf. zu fordern, dass es sich bei der Ressource aus dem ersten Tripel um eine Person handelt;
und eines, um letztendlich die eigentliche Intention der Frage, die Aufgaben dieser Person, auszugeben.

Eine einfache Frage ist eine Frage mit einer eindeutigen Intention, aus der eine oder mehrere Antworten erreicht werden können.
Es werden genau zwei Elemente eines Tripels, also zum Beispiel Subjekt und Prädikat, gegeben.
In ihr gibt es keine Ein- oder Beschränkungen oder Verzweigungen des Graphen.
Im Englischen wird eine solche Frage auch \emph{single-hop} genannt.

Eine komplexe Frage hat auch, wie eine einfache Frage, eine eindeutige Intention, es gibt aber mehr Eingaben als nur zwei Elemente des Tripels.
Es können komplexere Graphen entstehen, Verzweigungen und Einschränkungen können existieren.
Solche Fragen werden auch als \emph{multi-hop} bezeichnet.

Eine zusammengesetzte Frage besteht aus einer oder mehreren Teilfragen, welche miteinander verbunden sind.
So besteht beispielsweise die Frage \enquote{Wofür sind die Leiterin des Informationsmanagements und der Projektleiter jeweils verantwortlich?} aus den Teilfragen
\enquote{Wofür ist die Leiterin des Informationsmanagements verantwortlich?} und \enquote{Wofür ist der Projektleiter verantwortlich?}.
Die Frage kann also mehrere Intentionen gleichzeitig verfolgen.
Teilfragen können sowohl komplexe als auch simple Fragen sein.

Question Answering wird momentan zumeist in drei Schritten betrieben \cite{ckbqasurvey, muheqa}.
Zuerst wird die Frage analysiert, dann die Informationen herausgelesen und am Ende eine Antwort ausgewählt und ausgegeben.
Fragen werden auf ihre semantische oder syntaktische Struktur untersucht, etwa durch Part of Speech (PoS)-Tagging.
Dann werden gegebenenfalls Artikel o.ä. aus der weiteren Betrachtung ausgeschlossen, da sie semantisch keine weitere Bedeutung bringen.
Die verbleibenden Satzteile müssen nun mit den Labels oder anderen Informationen der Wissensbasis oder Ontologie abgeglichen werden, um die korrespondierenden Ressourcen zu finden.
Daraus wird meistens eine SPARQL-Abfrage mittels entweder vorgefertigter Vorlagen oder automatisch generierter Muster erzeugt.

\subsection{QA-Systeme}

Das QA-Leaderboard \citep{kgqaleaderboard} hat es sich zur Aufgabe gemacht,
dem häufig als sehr uneinheitlich und unübersichtlich \citep{diefenbachkbqa} beschriebenen Feld des KGQA eine vereinheitliche Liste mit verfügbaren QA-Systemen zu geben.
Hier werden Fragenkataloge wie QALD-9 als Benchmarks auf die verschiedenen KGQA-Systeme angewendet und die Ergebnisse aufgezeichnet.
Des Weiteren sind die 123 Systeme in einer Liste mit dem Namen des Systems, einer Beschreibung und Links zu dem originalen Paper und einer Demonstration,
REST-API oder dem GitHub-Projekt, falls diese öffentlich verfügbar existieren, ausgestattet.
Mithilfe dieses Projektes konnten mehrere Kandidaten für das Question Answering auf SNIK ausgewählt werden.

Das System gAnswer \cite{ganswer2} spaltet Fragen in Syntaxbäume auf und vergleicht später Subgraphen der möglichen Antworten, jedoch kommt es bei der komplexen Vorbereitung der Daten immer wieder zu Fehlern.
DeepPavlov \cite{deeppavlov} ist ein hochmodulares, hochflexibles System mit der Oberfläche eines Chatbots, leider aber nur für ODQA und deshalb nicht weiter nützlich für dieses Problem.
TeBaQA \cite{tebaqa} analysiert die Struktur einer Frage und versucht, sie einer Vorlage zuzuordnen, wodurch dann die SPARQL-Abfrage erstellt wird.
Auch hier gibt es nicht behebbare Probleme bei der Einrichtung des Systems für SNIK.
Betrachtet wird außerdem AskNow QA \cite{asknow}, welches mittels PoS-Tagging versucht, eine \emph{normalisierte Fragenstruktur} herzustellen und darüber eine SPARQL-Abfrage zu generieren.
Aufgrund des Entity Linkings kann allerdings momentan nur DBpedia verwendet werden.

Insbesondere wird hier QAnswer KG \cite{qanswer} verwendet.
Dieses will vor allem die Verfügbarkeit von Question Answering für eigene Datensätze erhöhen,
also das Problem der fehlenden Portabilität lösen.
Viele andere QA-Systeme fokussieren sich auf große Wissensbasen wie DBpedia und WikiData \cite{qald9plus},
welche große Mengen an Daten zur Verfügung stellen.
Bei kleineren Datenmengen gibt es aber das Problem, das häufig zu wenige Trainingsdaten zur Verfügung stehen, als dass man ein Modell von Grund auf trainieren kann.
Deshalb verwendet QAnswer Modelle, welche schon vorher viel auf normale Texte trainiert wurden und nun mittels Nutzerfragen gefinetuned wird.

QAnswer erstellt alle für die gegebene Frage möglichen N-Gramme, nachdem die sogenannten \emph{stop words}, eine vorgefertigte Liste an z.B. Artikeln, entfernt wurden.
Diese N-Gramme versucht es dann auf mögliche Repräsentationen im Datensatz mittels Labels zu matchen.
Daraus werden viele mögliche SPARQL-Abfragen kreiert, welche anhand bestimmter Kriterien einen \emph{Confidence}-Wert zwischen $0\%$ und $100\%$ erhalten.
Die SPARQL-Abfrage mit dem höchsten Wert wird ausgewählt, ist er größer als $50\%$ gilt die Antwort als richtig, sonst wird keine ausgegeben.

\section{Anpassung von QAnswer KG an SNIK}

Beim Einrichten der Umgebung wurde die Sprache auf Englisch gestellt, da in dieser auch die Fragen beantwortet werden sollen.

QAnswer KG versucht, sich auf eine Antwort zu konzentrieren und stellt nur die als am besten bewertete SPARQL-Abfrage dar.
Hier wird das Problem der Ambiguität deutlich, da bei der Frage \enquote{What is the chief information officer responsible for?} sowohl
\texttt{meta:isResponsibleForEntityType}, \texttt{meta:isResponsibleForFunction} und \texttt{meta:isResponsibleForRole} gemeint sein könnten.
Der Ansatz für die Lösung dieses Problems ist es, das System alle drei ausführen zu lassen, oder ihm das zumindest zu ermöglichen.
Dies kann zum Beispiel über Property Paths realisiert werden.
In diesem Fall erlauben Property Paths dem Prädikat, in der Abfrage verschiedene Ressourcen darstellen zu können.
Es werden letztendlich drei Abfragen ausgeführt, eine für jede mögliche Kombination der Attribute, also hier einmal pro Prädikat.
Statt allein \texttt{meta:isResponsibleForEntityType} steht durch die Nutzung von Property Paths nun \texttt{\texttt{meta:isResponsibleForRole} | \texttt{meta:isResponsibleForFunction} | \texttt{meta:isResponsibleForEntityType}} dort.
Die ganze Abfrage sieht unter Verwendung von Property Paths wie folgt aus:
\begin{lstlisting}[language=SPARQL]
  SELECT ?o
  WHERE {
    bb:ChiefInformationOfficer (meta:isResponsibleForRole | meta:isResponsibleForFunction | meta:isResponsibleForEntityType) ?o .
  }
\end{lstlisting}

Jedoch kann QAnswer keine Property Paths austellen, weshalb diese Option zur Lösung des Ambiguitätsproblems wegfällt.

Eine andere Möglichkeit, die obige Frage mittels einer SPARQL-Abfrage zu beantworten, ist mittels der \texttt{rdfs:subPropertyOf}-Beziehung:
\begin{lstlisting}[language=SPARQL]
  SELECT ?o
  WHERE {
    bb:ChiefInformationOfficer ?p ?o .
    ?p rdfs:subPropertyOf* meta:isResponsibleFor .
  }
\end{lstlisting}

Diese regelt die Hierarchie von Properties.
So sind die drei, welche hier alle in einer Abfrage zusammengefasst werden sollen, alle ein Subproperty von \texttt{meta:subPropertyOf}.

Dabei wichtig ist, dass hier \texttt{*} statt \texttt{+} oder gar nichts nach \texttt{rdfs:subPropertyOf} steht, falls \texttt{meta:isResponsibleFor} selbst genutzt wird,
und nicht nur die untergeordneten Properties.
Diese Zeichen geben wieder einen Property Path an, der von QAnswer wieder nicht gelöst werden kann.

Deshalb mussten vor dem Training die \texttt{rdfs:subPropertyOf}+-Beziehung materialisiert werden,
das heißt es wurden alle transitiven Subpropertybeziehungen zu Trainingszwecken mittels dem SPARQL-Befehl \texttt{CONSTRUCT} zu direkten Subklassenbeziehungen umgeformt.

Das ist mit dieser SPARQL-Abfrage möglich:
\begin{lstlisting}[language=SPARQL]
  CONSTRUCT {?s ?p ?o}
  FROM sniko:meta
  FROM sniko:bb
  WHERE {
    ?s ?pp ?o .
    ?pp rdfs:subPropertyOf+ ?p . 
  }
\end{lstlisting}

Es werden erst alle Tripel gefunden, die ausschließlich aus den Teilontologien \texttt{meta} und \texttt{bb} bestehen, da dies den Umfang dieser Arbeit beschreibt.
Somit werden auch nur Prädikate aus \texttt{meta} betrachtet, nicht aber \texttt{rdfs} oder anderen.
Danach werden alle Subproperties des Prädikats aus dem ersten Tripel ausgewählt.
Mittels \texttt{CONSTRUCT} werden die ausgewählten Prädikate anstelle dem anderen Property eingesetzt.
Diese Tripel werden dann manuell den RDF-Daten hinzugefügt\footnote{Verwendete n-Tripel-Datei verfügbar unter:\\\url{https://github.com/Yagnap/BeLL-Question-Answering-auf-Linked-Data-SNIK/blob/main/Data/qanswerset.nt}}

Zusätzlich werden auch die \texttt{rdfs:subClassOf}+-Beziehungen materialisiert.
Dies muss geschehen, da die Antworten für manche Textbuchfragen alle transitiven Subklassen sind.

Das kann mittels der folgenden Abfrage bewerkstelligt werden:
%\todo{das ist noch die alte Abfrage, die neue ist die mit ?s rdfs:subClassOf ?oo und ?s rdfs:subClassOf ?o.}
\begin{lstlisting}[language=SPARQL]
  CONSTRUCT {?s ?p ?oo}
  FROM sniko:meta
  FROM sniko:bb
  WHERE {
    ?s ?p ?o . 
    FILTER(?oo!=owl:Thing && ?oo!=meta:Top) .
    FILTER(STRSTARTS(STR(?oo),"http://www.snik.eu/ontology/bb")) .
    ?o rdfs:subClassOf+ ?oo .
  }
\end{lstlisting}

Beide dieser SPARQL-Abfragen wurden vom Betreuer bereitgestellt.
Durch die Materialisierung kann beispielsweise für die Frage \texttt{8/6}, \enquote{How can quality of HIS be evaluated?},
die Klasse \texttt{bb:CaseStudy} gefunden werden (siehe \cref{sub:antwortentextbuch}), obwohl diese eine direkte Subklasse von \texttt{bb:QualitativeEvaluationMethod} ist,
welche wiederum eine direkte Subklasse von \texttt{bb:EvaluationMethod} ist, welche in der in \cref{sub:sparqltextbuchfragen} sichtbaren Abfrage gesucht wird.
Für die Wissensbasis mit den unmaterialisierten transitiven Subklassenbeziehungen sieht die Abfrage so aus:
\begin{lstlisting}[language=SPARQL]
  SELECT DISTINCT ?s
  WHERE
    { ?s rdfs:subClassOf+ bb:EvaluationMethod . }
\end{lstlisting}

Nach der Materialisierung kann das \texttt{+} entfernt werden, wodurch folgende Abfrage ausreicht:
\begin{lstlisting}[language=SPARQL]
  SELECT DISTINCT ?s
  WHERE
    { ?s rdfs:subClassOf bb:EvaluationMethod . }
\end{lstlisting}

Die erste Abfrage kann nicht durch QAnswer generiert werden, die zweite jedoch schon.

Wenn keine Lösung für die Frage gefunden wurde, gab es als Ausgabe meist die Ressource selbst, also zum Beispiel \texttt{bb:ChiefInformationOfficer}.
Dies kann zwar nützlich, aber auch verwirrend sein und sollte mithilfe von Training verhindert werden.
Die Präzision der Ergebnisse ohne jegliches fine-tuning ist aber trotzdem erstaunlich.

Praktisch für die Lokalisierung der Fehler war die Funktion, sich alle generierten Anfragen anzeigen zu lassen.
Somit konnte erahnt werden, warum es das macht, was es macht.

Mit der Konfiguration an sich konnten schon viele Fehler behoben werden.
So gibt es eine Liste von \enquote{stop words}, welche nicht betrachtet werden.
Unter diesen sind häufig verwendete Präpositionen, Konjunktionen, Verben oder Füllwörter wie \enquote{and} oder \enquote{many}.
Diese sollen verhindern, dass falsche Ressourcen gefunden werden.
Hier musste diese Liste allerdings so modifiziert werden, dass das Wort \texttt{for} nicht mehr darin vorkommt, denn ohne es können Prädikate wie \enquote{responsible for} schwer erkannt werden,
besonders, da die beiden Wörter oft getrennt im Satz vorkommen.
Entfernt wurden auch \texttt{define} und \texttt{describe}.

Aus den \emph{Hidden Properties} wurde \texttt{rdfs:subClassOf} hinzugefügt.
Hidden Properties sind solche, die nicht explizit in der Frage benutzt werden, aber trotzdem gemeint werden können.
Es wurde \texttt{skos:altLabel} als weiteres Label für Ressourcen hinzugefügt, sodass QAnswer auch diese beachtet.
Die Mappings mussten auch dahingehend verändert werden, als dass \texttt{skos:definition}
und \texttt{rdfs:comment} als Beschreibung hinzugefügt wurden.
Die Definition wurde auch bei den Ergebnissen angezeigt, sodass es bei Ergebnissen von Fragen nach der Definition als richtig erachtet wurde, wenn die zu definierende Ressource richtig erkannt wurde.

Es gibt auch die Möglichkeit, direkt Wörter als Aliase für URIs zu nutzen.
Bei den \emph{Property Mapping} können URIs und dafür stehende Lexikalisierungen in Abhängigkeit gebracht werden, wie bei einem Wörterbuch.
Dies wurde hier für \enquote{phases} und \enquote{methods} bei \texttt{meta:updates} sowie \enquote{tasks} und \texttt{meta:functionComponent} gemacht.
Für \texttt{meta:entityTypeComponent} wurde \enquote{facets} hinzugefügt.
Notwendig war es aufgrund der Funktionsweise QAnswers; ohne Lexikalisierungen wüsste das Programm nicht, wofür die Ressourcen stehen.
Da die Properties in SNIK sehr allgemein gehalten sind, fehlen oft Labels, mithilfe deren QAnswer auf die Ressource als Teil der Antwort schließen kann.
Für diese stehen die Lexikalisierungen zur Verfügung.

\section{Benchmark}

Die Performance des Systems soll anhand eines Benchmarks aus Frage-Antwort-Paaren untersucht werden.
Die Frage liegt hierbei in natürlicher Sprache, die Antwort als SPARQL-Abfrage vor.
100 Fragen sollen als Testdatensatz, der immer wieder abgefragt wird, dienen,
alle anderen Fragen sollen in Schritten von 10 Fragen QAnswer trainieren und somit die Abhängigkeit von den Indikatoren \emph{Präzision}, \emph{Recall} und \emph{F-Maß} zur Anzahl der Fragen geben.

Das Lehrbuch selbst \cite{bb} enthält am Ende vn Kapiteln bereits Fragen, welche dasGelernte zusammenfassen sollen.
Ausgehend von \cite{arneba} wurden die Fragen anhand ihrer Eignung für das Training klassifiziert.
Es wurden nur Fragen in Betracht gezogen, die Anforderungsbereich 1 entsprechen, also Fakten wiedergeben sollen und nicht z.B. zusammenfassen.
Nicht alle Fragen haben Antworten in der Ontologie, diese wurden auch herausgefiltert.
Der Rest wurde in SPARQL-Abfragen umgewandelt.
Insgesamt gibt es 36 Lehrbuchfragen und zugehörige Antworten.

Da diese Fragen a) sehr kompliziert gestellt und b) viel zu wenige sind, werden aus dem Datenbestand der Teilontologie auch Fragen nach dem Schema \emph{Subjekt bzw. Objekt - Prädikat - Objekt bzw. Subjekt} gestellt.
Folgende Abfragen ergeben 621 Fragen nach dem Subjekt und 374 Fragen nach dem Objekt, also insgesamt 995 weitere Fragen, mit denen das Training beginnen kann.
\begin{lstlisting}[language=SPARQL]
# SPARQL-Abfrage für Fragen nach dem Objekt
SELECT DISTINCT CONCAT(
    "What ", REPLACE(REPLACE(REPLACE(
    STR(?pl), ".* component", CONCAT("are components of ", STR(?sl))),
    "^is ([a-z]* [a-z]*)", CONCAT("is ", STR(?sl), " $1")),
    "^([a-z]*e)s", CONCAT("is $1d by ", STR(?sl))),
    "?") as ?question,
CONCAT ("SELECT DISTINCT ?o WHERE { <", STR(?s), "> <", STR(?p), "> ?o. }") as ?sparql
FROM sniko:meta
FROM sniko:bb
{
 ?s ?p ?o.
 ?p rdfs:domain [rdfs:subClassOf meta:Top].
 ?p rdfs:range [rdfs:subClassOf meta:Top].
 ?s a [rdfs:subClassOf meta:Top].
 ?o a [rdfs:subClassOf meta:Top].
 ?s rdfs:label ?sl. FILTER(langmatches(lang(?sl),"en")).
 ?p rdfs:label ?pl. FILTER(langmatches(lang(?pl),"en")).
}
ORDER BY RAND()
\end{lstlisting}
\begin{lstlisting}[language=SPARQL]
  # SPARQL-Abfrage für Fragen nach dem Subjekt
  SELECT DISTINCT REPLACE(REPLACE(REPLACE(REPLACE(
          CONCAT("What ",?pl, " ", ?ol, "?"),
          "What is responsible", "Who is responsible"),
          "What approves", "Who approves"),
          "What is involved", "Who is involved"),
          "What .* component", "What has the component") as ?question,
  CONCAT("SELECT DISTINCT ?s WHERE { ?s <", STR(?p), "> <", STR(?o), ">. }") as ?sparql
  FROM sniko:meta
  FROM sniko:bb
  {
   ?s ?p ?o.
   ?p rdfs:domain [rdfs:subClassOf meta:Top].
   ?p rdfs:range [rdfs:subClassOf meta:Top].
   ?s a [rdfs:subClassOf meta:Top].
   ?o a [rdfs:subClassOf meta:Top].
   ?p rdfs:label ?pl. FILTER(langmatches(lang(?pl),"en")).
   ?o rdfs:label ?ol. FILTER(langmatches(lang(?ol),"en")).
  }
  ORDER BY RAND()
  \end{lstlisting}

  \begin{lstlisting}[language=SPARQL]
    # SPARQL-Abfrage für Fragen nach dem Objekt
    SELECT DISTINCT CONCAT(
        "What ", REPLACE(REPLACE(REPLACE(
        STR(?pl), ".* component", CONCAT("are components of ", STR(?sl))),
        "^is ([a-z]* [a-z]*)", CONCAT("is ", STR(?sl), " $1")),
        "^([a-z]*e)s", CONCAT("is $1d by ", STR(?sl))),
        "?") as ?question,
    CONCAT ("SELECT DISTINCT ?o WHERE { <", STR(?s), "> <", STR(?p), "> ?o. }") as ?sparql
    FROM sniko:meta
    FROM sniko:bb
    {
     ?s ?p ?o.
     ?p rdfs:domain [rdfs:subClassOf meta:Top].
     ?p rdfs:range [rdfs:subClassOf meta:Top].
     ?s a [rdfs:subClassOf meta:Top].
     ?o a [rdfs:subClassOf meta:Top].
     ?s rdfs:label ?sl. FILTER(langmatches(lang(?sl),"en")).
     ?p rdfs:label ?pl. FILTER(langmatches(lang(?pl),"en")).
    }
    ORDER BY RAND()
    \end{lstlisting}

\section{Ergebnisse}

\begin{itemize}
  \item Training beschreiben
  \item hier die Plots mit den Kurven möglichst auf einer Seite darstellen
  \begin{itemize}
    \item[$\rightarrow$] Mit groupplots \url{https://tex.stackexchange.com/questions/237213/how-to-add-one-single-legend-entry-for-several-plots}
  \end{itemize}
  \item herausfinden wie das offiziell heißt, Lernkurve?
  \begin{itemize}
    \item Die Methodologie dort an sich ist wohl sehr komplex, muss ich wohl auch einen Paragraphen oder so zu schreiben
    \item Paar Ergebnisse, die ich gefunden habe:
    %  \item \url{https://towardsdatascience.com/how-do-you-know-you-have-enough-training-data-ad9b1fd679ee}
    %  \item \url{https://towardsdatascience.com/predicting-the-effect-of-more-training-data-by-using-less-c3dde2f9ae48}
    %  \item \url{https://www.researchgate.net/publication/319272101\_Bootstrapping_the_Out-of-sample\_Predictions\_for\_Efficient\_and\_Accurate\_Cross-Validation}
    %  \item \url{https://www.researchgate.net/post/What\_should\_be\_Optimal\_size\_of\_training\_data}
    %  \item \url{https://machinelearningmastery.com/much-training-data-required-machine-learning/}
    \item Es gibt anscheinend keinen richtigen Namen für den Graphen, würde es also Performance in Abhängigkeit der Datenmenge oder so nennen
    \item Lernkurve ist anscheinend Performance in Abhängigkeit von Zeit, und darauf haben wir nur indirekt und nicht wirklich Einfluss
    \item Die Form scheint recht normal zu sein, also dass das erst recht steil ansteigt und dann einen Grenzwert hat (sieht fast schon wie eine Wurzelfunktion aus bei uns, oder?)
    \item Nur das Ende ist halt komisch, eigentlich scheinen alle zu sagen, dass mehr Daten auch gleich mehr gut sind normalerweise
    \item Liegt also entweder am Training von QAnswer oder an uns, ich würde im Rahmen vom Paper eventuell zumindest nochmal nach Ursachen zu gucken versuchen?
  \end{itemize}
\end{itemize}


\section{Diskussion und Ausblick}

Ein Modell ist immer für einen bestimmten Zweck erstellt und bildet nur einen Teil der Wirklichkeit ab.
SNIK fokussiert sich auf das Beantworten der Fragestellung "Wer (Rolle) macht was (Aufgabe) womit (Objekttyp)?".
Solche Fragen beantwortet das trainierte System auch sehr gut, die größte Herausforderung ist allerdings der "Mismatch" im verwendeten Vokabular eines menschlichen Nutzers zu den Properties der Ontologie.
Während in einer Wissensbasis die Abbildung von Verben zu Properties einfacher ist, weicht besonders bei der Beschreibung von Subklassen und Teil-Ganzes-Beziehungen das Vokabular menschlicher Nutzer oft von den Labels in der Ontologie ab.
Insgesamt erreicht das System bei dieser Art von Fragen  einen F-Score von 0.9... auf ..., siehe ...
Fragen, welche nicht diesem Schema folgen, wie die Leseverständnisfragen aus \cite{bb} (fußnote zu link im zenodo archiv), können selbst nach dem Training nur selten richtig beantwortet werden (F-Score von ... auf ..., siehe ...).
Unserer Einschätzung nach ist dies eine grundlegende Limitierung der gewählten Modellierung, wir erwarten daher auch bei zukünftigen Systemen keine überwiegend richtige Beantwortung der Verständnisfragen.
Wir planen zur Beantwortung allgemeiner Fragen, die über "Wer macht was womit" hinausgehen, das Training von Sprachmodellen direkt auf den Lehrbüchern.
Ein hybrides System aus KBQA und Sprachmodell hat das Potenzial, die Stärken beider Ansätze zu vereinen.

\begin{itemize}
  \item Diskussion von Kurven etc.
\end{itemize}

%% \bibliography{lni-paper-example-de.tex} ist hier nicht erlaubt: biblatex erwartet dies bei der Preambel
%% Starten Sie "biber paper", um eine Biliographie zu erzeugen.
\printbibliography

\end{document}
