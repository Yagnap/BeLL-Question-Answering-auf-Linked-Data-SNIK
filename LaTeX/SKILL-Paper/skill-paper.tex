% !TeX encoding = UTF-8
% !TeX spellcheck = de_DE

%% Dies gibt Warnungen aus, sollten veraltete LaTeX-Befehle verwendet werden
\RequirePackage[l2tabu, orthodox]{nag}
\documentclass[utf8,biblatex]{lni}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} % T2A for cyrillics
\bibliography{../Dokumentation/Bibliography, ../Dokumentation/snik}

%% Schöne Tabellen mittels \toprule, \midrule, \bottomrule
\usepackage{booktabs}

%% Zu Demonstrationszwecken
\usepackage[math]{blindtext}
\usepackage{mwe}

%% Akronyme
\usepackage{acronym}

% Listings
\definecolor{olivegreen}{rgb}{0.2,0.8,0.5}
\lstdefinelanguage{SPARQL}{
  language=SQL,
  morekeywords={PREFIX, a, CONSTRUCT},
  sensitive=true,
  % DISABLE FOR PRINTING
  morecomment=[l][\color{olivegreen}]{\#},
  morestring=[b][\color{orange}]\",
  % ENABLE FOR PRINTING
  %morecomment=[l][\color{black}]{\#},
  %morestring=[b][\color{black}]\",
  %keywordstyle=\color{black}
}

%% BibLaTeX-Sonderkonfiguration,
%% falls man schnell eine existierende Bibliographie wiederverwenden will, aber nicht die .bib-Datei händisch anpassen möchte.
%% Bitte \iffalse und \fi entfernen, dann ist diese Konfiguration aktiviert.

\AtEveryBibitem{%
  \ifentrytype{article}{%
  }{%
    \clearfield{doi}%
    \clearfield{issn}%
    \clearfield{url}%
    \clearfield{urldate}%
  }%
  \ifentrytype{inproceedings}{%
  }{%
    \clearfield{doi}%
    \clearfield{issn}%
    \clearfield{url}%
    \clearfield{urldate}%
  }%
}

\begin{document}
%%% Mehrere Autoren werden durch \and voneinander getrennt.
%%% Die Fußnote enthält die Adresse sowie eine E-Mail-Adresse.
%%% Das optionale Argument (sofern angegeben) wird für die Kopfzeile verwendet.
\title[Question Answering auf SNIK]{Question Answering auf einer Ontologie des Informationsmanagements im Krankenhaus}
%%%\subtitle{Untertitel / Subtitle} % falls benötigt
\author[Hannes R. Brunsch]% \and Konrad Höffner]
{Hannes R. Brunsch\footnote{Wilhelm-Ostwald-Schule, Gymnasium der Stadt Leipzig, Willi-Bredel-Straße 15, 04279 Leipzig, Deutschland \email{hrbrunsch@gmail.com}}}
%\and  Konrad Höffner\footnote{Universität Leipzig, Institut für Medizinische Informatik, Statistik und Epidemiologie, Härtelstraße 16--18, 04107 Leipzig, Deutsche \email{konrad.hoeffner@uni-leipzig.de}}}
\startpage{11} % Beginn der Seitenzählung für diesen Beitrag
\editor{Gesellschaft für Informatik}    % Namen der Herausgeber
\booktitle{Studierendenkonferenz Informatik} % Name des Tagungsband; optional Kurztitel
\yearofpublication{2023}
%%%\lnidoi{18.18420/provided-by-editor-02} % Falls bekannt
\maketitle

\begin{abstract}
Mit der beständig fortschreitenden Digitalisierung im Gesundheitswesen wird es immer wichtiger, auch das Wissen über das Informationsmanagement,
also die Verarbeitung von Informationen und die dazu nötigen Schritte u.v.a.m. dort digital und strukturiert erreichbar zu machen.
Diese Arbeit beschäftigt sich mit der vom IMISE entwickelten Wissensbasis SNIK.
Diese enthält Wissen aus dem Bereich des Informationsmanagements im Krankenhaus und soll künftig auch bei dem Studium der Medizininformatik helfen.
Das Wissen soll mittels geschriebener natürlicher Sprache durchsuchbar sein.
Eine Möglichkeit hierfür ist Question Answering.
Es soll möglich sein, dass ein Nutzer eine englische Frage in Satzform stellt und darauf eine Antwort bekommt.
Hierfür gibt es verschiedene Systeme, viele sind allerdings auf andere Wissensbasen spezialisiert.
Das Ziel dieser Arbeit war, nach Systemen zum Question Answering zu recherchieren und letztendlich eines auf SNIK anzuwenden.
Die Antworten wurden außerdem anhand eines vorher definierten Fragenkataloges auf ihre Genauigkeit hin überprüft und bewertet.
Als Kandidat wird QAnswer vorgeschlagen.  
\end{abstract}

\begin{keywords}
Semantic Web \and Question Answering \and Knowledge Graph Question Answering \and Closed Domain Question Answering \and SNIK \and QAnswer
\end{keywords}

%\input{Acronym.tex}

\section{Einleitung}

Das semantische Netz des Informationsmanagements im Krankenhaus (SNIK) ist eine
die Domäne des Informationsmanagements im Krankenhaus betreffende Ontologie \cite{domaene}.
Sie behandelt Wissen über Krankenhausinformationssysteme und deren Management.
Dieses wurde aus drei Lehrbüchern \cite{bb,ob,he} und einem Interview \cite{ciosurvey} manuell extrahiert und in RDF modelliert.

Momentan müssen Studierende der Medizininformatik, die nach Wissen suchen, auf eine der drei oben genannten Optionen zurückgreifen.
Jede dieser Möglichkeiten hat jedoch große Nachteile.
Der Resource Description Format (RDF)-Browser gibt nur ein sehr beschränktes Ergebnis aus, und serialisiertes RDF selbst zu lesen ist schwer und unpraktisch.
Die Graphvisualisierung kann im Zweifel unübersichtlich oder überwältigend sein, da es schwer sein kann, überhaupt die Ressource zu finden, zu der man eine Frage hat, und dann zur Antwort zu navigieren.
Im Fall von SPARQL Protocol and RDF Query Language (SPARQL) gibt es einen erheblichen Zeitaufwand für die Studierenden, da sie sich dort erst in die Syntax der Abfragesprache und das Vokabular des Fachbereichs einarbeiten müssen.

Daraus ergibt sich das Problem, dass keine der momentan existierenden Lösungen intuitiv genug funktioniert, als dass es nahezu keine Einarbeitungszeit gibt.
Die existierenden Lösungen liefern zudem nicht übersichtlich ausreichend Informationen, ihrer Expressivität sind demnach deutliche Grenzen gesetzt.

Obwohl ein Ansatz für die Lösung dieses Problems besteht, Question Answering (QA), wirft dieser direkt ein neues Problem für die entwickelnden Personen auf.
Die Implementierung eines QA-Systems mit adäquater Qualität der Antworten ist wesentlich aufwändiger~\citep[S.~3]{qanswer}, als es in einem angemessenem Zeitraum bei stark beschränkten Mitteln möglich ist.

Das Wissen zum Informationsmanagement im Krankenhaus ist komplex und oft nur schwer greifbar.
Es liegt in Form von Lehrbüchern, aber auch in SNIK vor.

Studierende haben selten Zeit, sich ganze Kapitel oder gar Bücher durchzulesen, verfügen jedoch auch nicht über die Kenntnisse SNIK effektiv zu verwenden.
Als Folge müssen sie bei Fragen oft ihren Professor oder andere Studierende hinzuziehen.
Es wäre ungemein einfacher, wenn sie das strukturierte Wissen in natürlicher Sprache abfragen könnten.
QA-Systeme sind im Idealfall 24 Stunden am Tag und 7 Tage die Woche leicht erreichbar und können sofort antworten. 
Besonders in Zeiten während und nach der Covid-19-Pandemie und immer mehr remote work, in denen direkte Kontakte mit Studierenden und Professoren oft eingeschränkt werden müssen und es digitale Veranstaltungen ohne örtliche Präsenz gibt, ist solch ein Werkzeug sehr hilfreich.

\section{Grundlagen}

\subsection{SNIK}

Die in RDF modellierten Quellen SNIKs werden alle in je einer Ontologie abgebildet.
Teilontologien werden mittels der Metaontologie modelliert und miteinander verbunden.
Deshalb ist SNIK sowohl eine Wissensbasis als auch Ontologie:
Es werden zwar keine einzelnen Krankenhäuser abgebildet, aber das Wissen aus verschiedenen Lehrbüchern, die im allgemeinen davon handeln.
Es gibt also mehrere Abstraktionsebenen.
Dadurch ist SNIK eine Ontologie mit Charakteristiken einer Wissensbasis.
Es werden zwar keine wirklichen Individuen abgebildet
- das Wissen liegt immer noch abstrakt aus Lehrbüchern vor, keine speziellen Krankenhäuser und ihre bestimmten Elemente werden betrachtet -
jedoch gibt die Metaontologie allem eine einer Wissensbasis ähnliche Struktur.

\includegraphics[width=\linewidth]{../Dokumentation/Images/snik-metamodel.pdf}\label{fig:snik-metamodel}

In \cref{fig:snik-metamodel} ist die Metaontologie dargestellt.
Von den drei Klassen \emph{Role}, \emph{EntityType} und \emph{Function} stammen alle anderen Klassen der Teilontologien ab.
Sie beschreiben Personen, Prozesse und Informationen im Krankenhaus.
Alle in der Ontologie vorhandenen Prädikate sind in der Metaontologie definiert.
So sind zwischen den in verschiedenen Teilontologien ähnliche Ressourcen in Relation gesetzt und die Beziehungen zwischen Ressourcen in den einzelnen Teilontologien dargestellt.

\subsection{QA-Systeme}

Das QA-Leaderboard \citep{leaderboardinproceedings} hat es sich zur Aufgabe gemacht,
dem häufig als sehr uneinheitlich und unübersichtlich \citep{diefenbachkbqa} beschriebenen Feld des KGQA eine vereinheitliche Liste mit verfügbaren QA-Systemen zu geben.
Hier werden Fragenkataloge wie QALD-9 als Benchmarks auf die verschiedenen KGQA-Systeme angewendet und die Ergebnisse aufgezeichnet.
Des Weiteren sind die 123 Systeme in einer Liste mit dem Namen des Systems, einer Beschreibung und Links zu dem originalen Paper und einer Demonstration,
REST-API oder dem GitHub-Projekt, falls diese öffentlich verfügbar existieren, ausgestattet.
Mithilfe dieses Projektes konnten mehrere Kandidaten für das Question Answering auf SNIK ausgewählt werden.

Das System gAnswer \cite{ganswerapproach} spaltet Fragen in Syntaxbäume auf und vergleicht später Subgraphen der möglichen Antworten, jedoch kommt es bei der komplexen Vorbereitung der Daten immer wieder zu Fehlern.
DeepPavlov \cite{deeppavlov} ist ein hochmodulares, hochflexibles System mit der Oberfläche eines Chatbots, leider aber nur für ODQA und deshalb nicht weiter nützlich für dieses Problem.
TeBaQA \cite{tebaqa} analysiert die Struktur einer Frage und versucht, sie einer Vorlage zuzuordnen, wodurch dann die SPARQL-Abfrage erstellt wird.
Auch hier gibt es nicht behebbare Probleme bei der Einrichtung des Systems für SNIK.
Betrachtet wird außerdem AskNow QA \cite{asknow}, welches mittels PoS-Tagging versucht, eine \emph{normalisierte Fragenstruktur} herzustellen und darüber eine SPARQL-Abfrage zu generieren.
Aufgrund des Entity Linkings kann allerdings momentan nur DBpedia verwendet werden.

Insbesondere wird hier QAnswer KG \cite{qanswer} verwendet.
Dieses will vor allem die Verfügbarkeit von Question Answering für eigene Datensätze erhöhen,
also das Problem der fehlenden Portabilität lösen.
Viele andere QA-Systeme fokussieren sich auf große Wissensbasen wie DBpedia und WikiData \cite{qald9plus},
welche große Mengen an Daten zur Verfügung stellen.
Bei kleineren Datenmengen gibt es aber das Problem, das häufig zu wenige Trainingsdaten zur Verfügung stehen, als dass man ein Modell von Grund auf trainieren kann.
Deshalb verwendet QAnswer Modelle, welche schon vorher viel auf normale Texte trainiert wurden und nun mittels Nutzerfragen gefinetuned wird.

QAnswer erstellt alle für die gegebene Frage möglichen N-Gramme, nachdem die sogenannten \emph{stop words}, eine vorgefertigte Liste an z.B. Artikeln, entfernt wurden.
Diese N-Gramme versucht es dann auf mögliche Repräsentationen im Datensatz mittels Labels zu matchen.
Daraus werden viele mögliche SPARQL-Abfragen kreiert, welche anhand bestimmter Kriterien einen \emph{Confidence}-Wert zwischen $0\%$ und $100\%$ erhalten.
Die SPARQL-Abfrage mit dem höchsten Wert wird ausgewählt, ist er größer als $50\%$ gilt die Antwort als richtig, sonst wird keine ausgegeben.

\section{Anpassung von QAnswer KG an SNIK}

Beim Einrichten der Umgebung wird die Sprache auf Englisch gestellt, da in dieser auch die Fragen gestellt werden.

QAnswer KG versucht, sich auf eine Antwort zu konzentrieren und stellt nur die als am besten bewertete SPARQL-Abfrage dar.
Hier wird das Problem der Ambiguität deutlich, da bei der Frage \enquote{What is the chief information officer responsible for?} sowohl
\texttt{meta:isResponsibleForEntityType}, \texttt{meta:isResponsibleForFunction} und \texttt{meta:isResponsibleForRole} gemeint sein können.
Der Ansatz für die Lösung dieses Problems ist es, das System alle drei ausführen zu lassen, oder ihm das zumindest zu ermöglichen.

Dies kann zum Beispiel über Property Paths realisiert werden.
In diesem Fall erlauben Property Paths dem Prädikat, in der Abfrage verschiedene Ressourcen darstellen zu können.
Es werden letztendlich drei Abfragen ausgeführt, eine für jede mögliche Kombination der Attribute, also hier einmal pro Prädikat.
Statt allein \texttt{meta:isResponsibleForEntityType} steht durch die Nutzung von Property Paths nun \texttt{meta:isResponsibleForRole | meta:isResponsibleForFunction | meta:isResponsibleForEntityType} dort.
Jedoch kann QAnswer keine Property Paths aufstellen, weshalb diese Option zur Lösung des Ambiguitätsproblems wegfällt.

Eine andere Möglichkeit, die obige Frage mittels einer SPARQL-Abfrage zu beantworten, ist mittels der \texttt{rdfs:subPropertyOf}-Beziehung.
Diese regelt die Hierarchie von Properties.
So sind die drei, welche hier alle in einer Abfrage zusammengefasst werden sollen, alle ein Subproperty von \texttt{meta:subPropertyOf}.

Deshalb müssen vor dem Training die \texttt{rdfs:subPropertyOf}+-Beziehung materialisiert werden.
Das heißt es werden alle transitiven Subpropertybeziehungen zu Trainingszwecken mittels dem SPARQL-Befehl \texttt{CONSTRUCT} zu direkten Subklassenbeziehungen umgeformt.

Es werden erst alle Tripel gefunden, die ausschließlich aus den Teilontologien \texttt{meta} und \texttt{bb} bestehen.
Somit werden auch nur Prädikate aus \texttt{meta} betrachtet, nicht aber \texttt{rdfs} oder anderen.
Danach werden alle Subproperties des Prädikats aus dem ersten Tripel ausgewählt.
Mittels \texttt{CONSTRUCT} werden die ausgewählten Prädikate anstelle dem anderen Property eingesetzt.
Diese Tripel werden dann manuell den RDF-Daten hinzugefügt\footnote{Verwendete n-Tripel-Datei verfügbar unter:\\\url{https://github.com/Yagnap/BeLL-Question-Answering-auf-Linked-Data-SNIK/blob/main/Data/qanswerset.nt}}

Zusätzlich werden auch die \texttt{rdfs:subClassOf}+-Beziehungen materialisiert.
Dies muss geschehen, da die Antworten für manche Textbuchfragen alle transitiven Subklassen sind.

Durch die Materialisierung kann beispielsweise für die Frage \texttt{8/6}, \enquote{How can quality of HIS be evaluated?},
die Klasse \texttt{bb:CaseStudy} gefunden werden, obwohl diese eine direkte Subklasse von \texttt{bb:QualitativeEvaluationMethod} ist,
welche wiederum eine direkte Subklasse von \texttt{bb:EvaluationMethod} ist, welche in der Abfrage gesucht wird.
Die Abfrage, welche die \texttt{rdfs:subClassOf+}-Beziehung kann nicht durch QAnswer generiert werden, die materialisierte \texttt{rdfs:subClassOf} jedoch schon.

Wenn keine Lösung für die Frage gefunden wird, gibt es als Ausgabe meist die Ressource selbst, also zum Beispiel \texttt{bb:ChiefInformationOfficer}.
Dies kann zwar nützlich, aber auch verwirrend sein und soll mithilfe von Training verhindert werden.
Die Präzision der Ergebnisse ohne jegliches fine-tuning ist aber trotzdem erstaunlich.

Praktisch für die Lokalisierung der Fehler ist die Funktion, sich alle generierten Anfragen anzeigen zu lassen.
Somit kann erahnt werden, warum es das macht, was es macht.

Mit der Konfiguration an sich können bereits viele Fehler behoben werden.
Unter den \emph{stop words} sind häufig verwendete Präpositionen, Konjunktionen, Verben oder Füllwörter wie \enquote{and} oder \enquote{many}.
Diese sollen verhindern, dass falsche Ressourcen gefunden werden.
Hier muss diese Liste allerdings so modifiziert werden, dass das Wort \texttt{for} nicht mehr darin vorkommt, denn ohne es können Prädikate wie \enquote{responsible for} schwer erkannt werden,
besonders, da die beiden Wörter oft getrennt im Satz vorkommen.
Entfernt wurden auch \texttt{define} und \texttt{describe}, da auch diese in Labels vorkommen.

Zu den \emph{Hidden Properties} wurde \texttt{rdfs:subClassOf} hinzugefügt.
Hidden Properties sind solche, die nicht explizit in der Frage benutzt werden, aber trotzdem impliziert werden können.

Es wurde \texttt{skos:altLabel} als weiteres Label für Ressourcen hinzugefügt, sodass QAnswer auch diese beachtet.
Die Mappings müssen auch dahingehend verändert werden, als dass \texttt{skos:definition}
und \texttt{rdfs:comment} als Beschreibung hinzugefügt werden.
Die Definition wird auch bei den Ergebnissen angezeigt, sodass es bei Ergebnissen von Fragen nach der Definition als richtig erachtet wird, wenn die zu definierende Ressource richtig erkannt wurde.

Es gibt auch die Möglichkeit, direkt Wörter als Aliase für URIs zu nutzen.
Bei den \emph{Property Mapping} können URIs und dafür stehende Lexikalisierungen in Abhängigkeit gebracht werden, wie bei einem Wörterbuch.
Dies wird hier für \enquote{phases} und \enquote{methods} bei \texttt{meta:updates} sowie \enquote{tasks} und \texttt{meta:functionComponent} gemacht.
Für \texttt{meta:entityTypeComponent} wird \enquote{facets} hinzugefügt.
Notwendig ist es aufgrund der Funktionsweise QAnswers; ohne Lexikalisierungen weiß das Programm nicht, wofür die Ressourcen stehen.
Da die Properties in SNIK sehr allgemein gehalten sind, fehlen oft Labels, mithilfe derer QAnswer auf die Ressource als Teil der Antwort schließen kann.
Für diese stehen die Lexikalisierungen zur Verfügung.

\section{Benchmark}

Die Performance des Systems soll anhand eines Benchmarks aus Frage-Antwort-Paaren untersucht werden.
Die Frage liegt hierbei in natürlicher Sprache, die Antwort als SPARQL-Abfrage vor.
100 Fragen sollen als Testdatensatz, der immer wieder abgefragt wird, dienen,
alle anderen Fragen sollen in Schritten von 10 Fragen QAnswer trainieren und somit die Abhängigkeit von den Indikatoren \emph{Präzision}, \emph{Recall} und \emph{F-Maß} zur Anzahl der Fragen geben.

Das Lehrbuch selbst \cite{bb} enthält am Ende vn Kapiteln bereits Fragen, welche dasGelernte zusammenfassen sollen.
Ausgehend von \cite{arneba} wurden die Fragen anhand ihrer Eignung für das Training klassifiziert.
Es wurden nur Fragen in Betracht gezogen, die Anforderungsbereich 1 entsprechen, also Fakten wiedergeben sollen und nicht z.B. zusammenfassen.
Nicht alle Fragen haben Antworten in der Ontologie, diese wurden auch herausgefiltert.
Der Rest wurde in SPARQL-Abfragen umgewandelt.
Insgesamt gibt es 36 Lehrbuchfragen und zugehörige Antworten.

Da diese Fragen a) sehr kompliziert gestellt und b) viel zu wenige sind, werden aus dem Datenbestand der Teilontologie auch Fragen nach dem Schema \emph{Subjekt bzw. Objekt - Prädikat - Objekt bzw. Subjekt} gestellt.
Folgende Abfragen ergeben 621 Fragen nach dem Subjekt und 374 Fragen nach dem Objekt, also insgesamt 995 weitere Fragen, mit denen das Training beginnen kann.
\begin{lstlisting}[language=SPARQL]
# SPARQL-Abfrage Fragen nach dem Objekt
SELECT DISTINCT CONCAT(
    "What ", REPLACE(REPLACE(REPLACE(
    STR(?pl), ".* component", CONCAT("are components of ", STR(?sl))),
    "^is ([a-z]* [a-z]*)", CONCAT("is ", STR(?sl), " $1")),
    "^([a-z]*e)s", CONCAT("is $1d by ", STR(?sl))),
    "?") as ?question,
CONCAT ("SELECT DISTINCT ?o WHERE { <", STR(?s), "> <", STR(?p), "> ?o. }") as ?sparql
FROM sniko:meta
FROM sniko:bb
{
 ?s ?p ?o.
 ?p rdfs:domain [rdfs:subClassOf meta:Top].
 ?p rdfs:range [rdfs:subClassOf meta:Top].
 ?s a [rdfs:subClassOf meta:Top].
 ?o a [rdfs:subClassOf meta:Top].
 ?s rdfs:label ?sl. FILTER(langmatches(lang(?sl),"en")).
 ?p rdfs:label ?pl. FILTER(langmatches(lang(?pl),"en")).
}
ORDER BY RAND()
\end{lstlisting}
\begin{lstlisting}[language=SPARQL]
  # SPARQL-Abfrage Fragen nach dem Subjekt
  SELECT DISTINCT REPLACE(REPLACE(REPLACE(REPLACE(
          CONCAT("What ",?pl, " ", ?ol, "?"),
          "What is responsible", "Who is responsible"),
          "What approves", "Who approves"),
          "What is involved", "Who is involved"),
          "What .* component", "What has the component") as ?question,
  CONCAT("SELECT DISTINCT ?s WHERE { ?s <", STR(?p), "> <", STR(?o), ">. }") as ?sparql
  FROM sniko:meta
  FROM sniko:bb
  {
   ?s ?p ?o.
   ?p rdfs:domain [rdfs:subClassOf meta:Top].
   ?p rdfs:range [rdfs:subClassOf meta:Top].
   ?s a [rdfs:subClassOf meta:Top].
   ?o a [rdfs:subClassOf meta:Top].
   ?p rdfs:label ?pl. FILTER(langmatches(lang(?pl),"en")).
   ?o rdfs:label ?ol. FILTER(langmatches(lang(?ol),"en")).
  }
  ORDER BY RAND()
  \end{lstlisting}

  \begin{lstlisting}[language=SPARQL]
    # SPARQL-Abfrage Fragen nach dem Objekt
    SELECT DISTINCT CONCAT(
        "What ", REPLACE(REPLACE(REPLACE(
        STR(?pl), ".* component", CONCAT("are components of ", STR(?sl))),
        "^is ([a-z]* [a-z]*)", CONCAT("is ", STR(?sl), " $1")),
        "^([a-z]*e)s", CONCAT("is $1d by ", STR(?sl))),
        "?") as ?question,
    CONCAT ("SELECT DISTINCT ?o WHERE { <", STR(?s), "> <", STR(?p), "> ?o. }") as ?sparql
    FROM sniko:meta
    FROM sniko:bb
    {
     ?s ?p ?o.
     ?p rdfs:domain [rdfs:subClassOf meta:Top].
     ?p rdfs:range [rdfs:subClassOf meta:Top].
     ?s a [rdfs:subClassOf meta:Top].
     ?o a [rdfs:subClassOf meta:Top].
     ?s rdfs:label ?sl. FILTER(langmatches(lang(?sl),"en")).
     ?p rdfs:label ?pl. FILTER(langmatches(lang(?pl),"en")).
    }
    ORDER BY RAND()
    \end{lstlisting}

\section{Ergebnisse}

\begin{itemize}
  \item Training beschreiben
  \item hier die Plots mit den Kurven möglichst auf einer Seite darstellen
  \begin{itemize}
    \item[$\rightarrow$] Mit groupplots \url{https://tex.stackexchange.com/questions/237213/how-to-add-one-single-legend-entry-for-several-plots}
  \end{itemize}
  \item herausfinden wie das offiziell heißt, Lernkurve?
  \begin{itemize}
    \item Die Methodologie dort an sich ist wohl sehr komplex, muss ich wohl auch einen Paragraphen oder so zu schreiben
    \item Paar Ergebnisse, die ich gefunden habe:
    %  \item \url{https://towardsdatascience.com/how-do-you-know-you-have-enough-training-data-ad9b1fd679ee}
    %  \item \url{https://towardsdatascience.com/predicting-the-effect-of-more-training-data-by-using-less-c3dde2f9ae48}
    %  \item \url{https://www.researchgate.net/publication/319272101\_Bootstrapping_the_Out-of-sample\_Predictions\_for\_Efficient\_and\_Accurate\_Cross-Validation}
    %  \item \url{https://www.researchgate.net/post/What\_should\_be\_Optimal\_size\_of\_training\_data}
    %  \item \url{https://machinelearningmastery.com/much-training-data-required-machine-learning/}
    \item Es gibt anscheinend keinen richtigen Namen für den Graphen, würde es also Performance in Abhängigkeit der Datenmenge oder so nennen
    \item Lernkurve ist anscheinend Performance in Abhängigkeit von Zeit, und darauf haben wir nur indirekt und nicht wirklich Einfluss
    \item Die Form scheint recht normal zu sein, also dass das erst recht steil ansteigt und dann einen Grenzwert hat (sieht fast schon wie eine Wurzelfunktion aus bei uns, oder?)
    \item Nur das Ende ist halt komisch, eigentlich scheinen alle zu sagen, dass mehr Daten auch gleich mehr gut sind normalerweise
    \item Liegt also entweder am Training von QAnswer oder an uns, ich würde im Rahmen vom Paper eventuell zumindest nochmal nach Ursachen zu gucken versuchen?
  \end{itemize}
\end{itemize}


\section{Diskussion und Ausblick}

Ein Modell ist immer für einen bestimmten Zweck erstellt und bildet nur einen Teil der Wirklichkeit ab.
SNIK fokussiert sich auf das Beantworten der Fragestellung "Wer (Rolle) macht was (Aufgabe) womit (Objekttyp)?".
Solche Fragen beantwortet das trainierte System auch sehr gut, die größte Herausforderung ist allerdings der "Mismatch" im verwendeten Vokabular eines menschlichen Nutzers zu den Properties der Ontologie.
Während in einer Wissensbasis die Abbildung von Verben zu Properties einfacher ist, weicht besonders bei der Beschreibung von Subklassen und Teil-Ganzes-Beziehungen das Vokabular menschlicher Nutzer oft von den Labels in der Ontologie ab.
Insgesamt erreicht das System bei dieser Art von Fragen  einen F-Score von 0.9... auf ..., siehe ...
Fragen, welche nicht diesem Schema folgen, wie die Leseverständnisfragen aus \cite{bb} (fußnote zu link im zenodo archiv), können selbst nach dem Training nur selten richtig beantwortet werden (F-Score von ... auf ..., siehe ...).
Unserer Einschätzung nach ist dies eine grundlegende Limitierung der gewählten Modellierung, wir erwarten daher auch bei zukünftigen Systemen keine überwiegend richtige Beantwortung der Verständnisfragen.
Wir planen zur Beantwortung allgemeiner Fragen, die über "Wer macht was womit" hinausgehen, das Training von Sprachmodellen direkt auf den Lehrbüchern.
Ein hybrides System aus KBQA und Sprachmodell hat das Potenzial, die Stärken beider Ansätze zu vereinen.

\begin{itemize}
  \item Diskussion von Kurven etc.
\end{itemize}

%% \bibliography{lni-paper-example-de.tex} ist hier nicht erlaubt: biblatex erwartet dies bei der Preambel
%% Starten Sie "biber paper", um eine Biliographie zu erzeugen.
\printbibliography

\end{document}
