% !TeX encoding = UTF-8
% !TeX spellcheck = de_DE

%% Dies gibt Warnungen aus, sollten veraltete LaTeX-Befehle verwendet werden
\RequirePackage[l2tabu, orthodox]{nag}
\documentclass[utf8,biblatex]{lni}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} % T2A for cyrillics
\bibliography{../Dokumentation/Bibliography, ../Dokumentation/snik}

%% Schöne Tabellen mittels \toprule, \midrule, \bottomrule
\usepackage{booktabs}

%% Zu Demonstrationszwecken
\usepackage[math]{blindtext}
\usepackage{mwe}

%% Akronyme
\usepackage{acronym}

%% Plots
\usepackage{tikz}
\usepackage{pgfplots}
\usepgfplotslibrary{groupplots}

% Listings
\definecolor{olivegreen}{rgb}{0.2,0.8,0.5}
\lstdefinelanguage{SPARQL}{
  language=SQL,
  morekeywords={PREFIX, a, CONSTRUCT},
  sensitive=true,
  % DISABLE FOR PRINTING
  morecomment=[l][\color{olivegreen}]{\#},
  morestring=[b][\color{orange}]\",
  % ENABLE FOR PRINTING
  %morecomment=[l][\color{black}]{\#},
  %morestring=[b][\color{black}]\",
  %keywordstyle=\color{black}
}

%% BibLaTeX-Sonderkonfiguration,
%% falls man schnell eine existierende Bibliographie wiederverwenden will, aber nicht die .bib-Datei händisch anpassen möchte.
%% Bitte \iffalse und \fi entfernen, dann ist diese Konfiguration aktiviert.

\AtEveryBibitem{%
  \ifentrytype{article}{%
  }{%
    \clearfield{doi}%
    \clearfield{issn}%
    \clearfield{url}%
    \clearfield{urldate}%
  }%
  \ifentrytype{inproceedings}{%
  }{%
    \clearfield{doi}%
    \clearfield{issn}%
    \clearfield{url}%
    \clearfield{urldate}%
  }%
}

\begin{document}
%%% Mehrere Autoren werden durch \and voneinander getrennt.
%%% Die Fußnote enthält die Adresse sowie eine E-Mail-Adresse.
%%% Das optionale Argument (sofern angegeben) wird für die Kopfzeile verwendet.
\title[Question Answering auf SNIK]{Question Answering auf einer Ontologie des Informationsmanagements im Krankenhaus}
%%%\subtitle{Untertitel / Subtitle} % falls benötigt
\author[Hannes R. Brunsch]% \and Konrad Höffner]
{Hannes R. Brunsch\footnote{Wilhelm-Ostwald-Schule, Gymnasium der Stadt Leipzig, Willi-Bredel-Straße 15, 04279 Leipzig, Deutschland \email{hrbrunsch@gmail.com}}}
%\and  Konrad Höffner\footnote{Universität Leipzig, Institut für Medizinische Informatik, Statistik und Epidemiologie, Härtelstraße 16--18, 04107 Leipzig, Deutsche \email{konrad.hoeffner@uni-leipzig.de}}}
\startpage{11} % Beginn der Seitenzählung für diesen Beitrag
\editor{Gesellschaft für Informatik}    % Namen der Herausgeber
\booktitle{Studierendenkonferenz Informatik} % Name des Tagungsband; optional Kurztitel
\yearofpublication{2023}
%%%\lnidoi{18.18420/provided-by-editor-02} % Falls bekannt
\maketitle

\begin{abstract}
Mit der beständig fortschreitenden Digitalisierung im Gesundheitswesen wird es immer wichtiger, auch das Wissen über das Informationsmanagement,
also die Verarbeitung von Informationen und die dazu nötigen Schritte u.v.a.m. dort digital und strukturiert erreichbar zu machen.
Diese Arbeit beschäftigt sich mit der vom IMISE entwickelten Wissensbasis SNIK.
Diese enthält Wissen aus dem Bereich des Informationsmanagements im Krankenhaus und soll künftig auch bei dem Studium der Medizininformatik helfen.
Das Wissen soll mittels geschriebener natürlicher Sprache durchsuchbar sein.
Eine Möglichkeit hierfür ist Question Answering.
Es soll möglich sein, dass ein Nutzer eine englische Frage in Satzform stellt und darauf eine Antwort bekommt.
Hierfür gibt es verschiedene Systeme, viele sind allerdings auf andere Wissensbasen spezialisiert.
Das Ziel dieser Arbeit war, nach Systemen zum Question Answering zu recherchieren und letztendlich eines auf SNIK anzuwenden.
Die Antworten wurden außerdem anhand eines vorher definierten Fragenkataloges auf ihre Genauigkeit hin überprüft und bewertet.
Als Kandidat wird QAnswer vorgeschlagen.  
\end{abstract}

\begin{keywords}
Semantic Web \and Question Answering \and Knowledge Graph Question Answering \and Closed Domain Question Answering \and SNIK \and QAnswer
\end{keywords}

%\input{Acronym.tex}

\section{Einleitung}

Das semantische Netz des Informationsmanagements im Krankenhaus (SNIK) ist eine
die Domäne des Informationsmanagements im Krankenhaus betreffende Ontologie \cite{domaene}.
Sie behandelt Wissen über Krankenhausinformationssysteme und deren Management.
Dieses wurde aus drei Lehrbüchern \cite{bb,ob,he} und einem Interview \cite{ciosurvey} manuell extrahiert und in RDF modelliert.

Momentan müssen Studierende der Medizininformatik, die nach Wissen suchen, auf eine der drei oben genannten Optionen zurückgreifen.
Jede dieser Möglichkeiten hat jedoch große Nachteile.
Der Resource Description Format (RDF)-Browser gibt nur ein sehr beschränktes Ergebnis aus, und serialisiertes RDF selbst zu lesen ist schwer und unpraktisch.
Die Graphvisualisierung kann im Zweifel unübersichtlich oder überwältigend sein, da es schwer sein kann, überhaupt die Ressource zu finden, zu der man eine Frage hat, und dann zur Antwort zu navigieren.
Im Fall von SPARQL Protocol and RDF Query Language (SPARQL) gibt es einen erheblichen Zeitaufwand für die Studierenden, da sie sich dort erst in die Syntax der Abfragesprache und das Vokabular des Fachbereichs einarbeiten müssen.

Daraus ergibt sich das Problem, dass keine der momentan existierenden Lösungen intuitiv genug funktioniert, als dass es nahezu keine Einarbeitungszeit gibt.
Die existierenden Lösungen liefern zudem nicht übersichtlich ausreichend Informationen, ihrer Expressivität sind demnach deutliche Grenzen gesetzt.

Obwohl ein Ansatz für die Lösung dieses Problems besteht, Question Answering (QA), wirft dieser direkt ein neues Problem für die entwickelnden Personen auf.
Die Implementierung eines QA-Systems mit adäquater Qualität der Antworten ist wesentlich aufwändiger~\citep[S.~3]{qanswer}, als es in einem angemessenem Zeitraum bei stark beschränkten Mitteln möglich ist.

Das Wissen zum Informationsmanagement im Krankenhaus ist komplex und oft nur schwer greifbar.
Es liegt in Form von Lehrbüchern, aber auch in SNIK vor.

Studierende haben selten Zeit, sich ganze Kapitel oder gar Bücher durchzulesen, verfügen jedoch auch nicht über die Kenntnisse SNIK effektiv zu verwenden.
Als Folge müssen sie bei Fragen oft ihren Professor oder andere Studierende hinzuziehen.
Es wäre ungemein einfacher, wenn sie das strukturierte Wissen in natürlicher Sprache abfragen könnten.
QA-Systeme sind im Idealfall 24 Stunden am Tag und 7 Tage die Woche leicht erreichbar und können sofort antworten. 
Besonders in Zeiten während und nach der Covid-19-Pandemie und immer mehr remote work, in denen direkte Kontakte mit Studierenden und Professoren oft eingeschränkt werden müssen und es digitale Veranstaltungen ohne örtliche Präsenz gibt, ist solch ein Werkzeug sehr hilfreich.

\section{Grundlagen}

\subsection{SNIK}

Die in RDF modellierten Quellen SNIKs werden alle in je einer Ontologie abgebildet.
Teilontologien werden mittels der Metaontologie modelliert und miteinander verbunden.
Deshalb ist SNIK sowohl eine Wissensbasis als auch Ontologie:
Es werden zwar keine einzelnen Krankenhäuser abgebildet, aber das Wissen aus verschiedenen Lehrbüchern, die im allgemeinen davon handeln.
Es gibt also mehrere Abstraktionsebenen.
Dadurch ist SNIK eine Ontologie mit Charakteristiken einer Wissensbasis.
Es werden zwar keine wirklichen Individuen abgebildet
- das Wissen liegt immer noch abstrakt aus Lehrbüchern vor, keine speziellen Krankenhäuser und ihre bestimmten Elemente werden betrachtet -
jedoch gibt die Metaontologie allem eine einer Wissensbasis ähnliche Struktur.

\begin{figure}[h]
  \includegraphics[width=\linewidth]{../Dokumentation/Images/snik-metamodel.pdf}\label{fig:snik-metamodel}
  \caption[SNIK Metamodell Version 8]{Das SNIK Metamodell Version 8. Quelle: \url{https://www.snik.eu/public/SNIK_Metamodell_V8.svg}}
\end{figure}
In \cref{fig:snik-metamodel} ist die Metaontologie dargestellt.
Von den drei Klassen \emph{Role}, \emph{EntityType} und \emph{Function} stammen alle anderen Klassen der Teilontologien ab.
Sie beschreiben Personen, Prozesse und Informationen im Krankenhaus.
Alle in der Ontologie vorhandenen Prädikate sind in der Metaontologie definiert.
So sind zwischen den in verschiedenen Teilontologien ähnliche Ressourcen in Relation gesetzt und die Beziehungen zwischen Ressourcen in den einzelnen Teilontologien dargestellt.

\subsection{QA-Systeme}

Das QA-Leaderboard \citep{leaderboardinproceedings} hat es sich zur Aufgabe gemacht,
dem häufig als sehr uneinheitlich und unübersichtlich \citep{diefenbachkbqa} beschriebenen Feld des KGQA eine vereinheitliche Liste mit verfügbaren QA-Systemen zu geben.
Hier werden Fragenkataloge wie QALD-9 als Benchmarks auf die verschiedenen KGQA-Systeme angewendet und die Ergebnisse aufgezeichnet.
Des Weiteren sind die 123 Systeme in einer Liste mit dem Namen des Systems, einer Beschreibung und Links zu dem originalen Paper und einer Demonstration,
REST-API oder dem GitHub-Projekt, falls diese öffentlich verfügbar existieren, ausgestattet.
Mithilfe dieses Projektes konnten mehrere Kandidaten für das Question Answering auf SNIK ausgewählt werden.

Das System gAnswer \cite{ganswerapproach} spaltet Fragen in Syntaxbäume auf und vergleicht später Subgraphen der möglichen Antworten, jedoch kommt es bei der komplexen Vorbereitung der Daten immer wieder zu Fehlern.
DeepPavlov \cite{deeppavlov} ist ein hochmodulares, hochflexibles System mit der Oberfläche eines Chatbots, leider aber nur für ODQA und deshalb nicht weiter nützlich für dieses Problem.
TeBaQA \cite{tebaqa} analysiert die Struktur einer Frage und versucht, sie einer Vorlage zuzuordnen, wodurch dann die SPARQL-Abfrage erstellt wird.
Auch hier gibt es nicht behebbare Probleme bei der Einrichtung des Systems für SNIK.
Betrachtet wird außerdem AskNow QA \cite{asknow}, welches mittels PoS-Tagging versucht, eine \emph{normalisierte Fragenstruktur} herzustellen und darüber eine SPARQL-Abfrage zu generieren.
Aufgrund des Entity Linkings kann allerdings momentan nur DBpedia verwendet werden.

Insbesondere wird hier QAnswer KG \cite{qanswer} verwendet.
Dieses will vor allem die Verfügbarkeit von Question Answering für eigene Datensätze erhöhen,
also das Problem der fehlenden Portabilität lösen.
Viele andere QA-Systeme fokussieren sich auf große Wissensbasen wie DBpedia und WikiData \cite{qald9plus},
welche große Mengen an Daten zur Verfügung stellen.
Bei kleineren Datenmengen gibt es aber das Problem, das häufig zu wenige Trainingsdaten zur Verfügung stehen, als dass man ein Modell von Grund auf trainieren kann.
Deshalb verwendet QAnswer Modelle, welche schon vorher viel auf normale Texte trainiert wurden und nun mittels Nutzerfragen gefinetuned wird.

QAnswer erstellt alle für die gegebene Frage möglichen N-Gramme, nachdem die sogenannten \emph{stop words}, eine vorgefertigte Liste an z.B. Artikeln, entfernt wurden.
Diese N-Gramme versucht es dann auf mögliche Repräsentationen im Datensatz mittels Labels zu matchen.
Daraus werden viele mögliche SPARQL-Abfragen kreiert, welche anhand bestimmter Kriterien einen \emph{Confidence}-Wert zwischen $0\%$ und $100\%$ erhalten.
Die SPARQL-Abfrage mit dem höchsten Wert wird ausgewählt, ist er größer als $50\%$ gilt die Antwort als richtig, sonst wird keine ausgegeben.

\section{Anpassung von QAnswer KG an SNIK}

Beim Einrichten der Umgebung wird die Sprache auf Englisch gestellt, da in dieser auch die Fragen gestellt werden.

QAnswer KG versucht, sich auf eine Antwort zu konzentrieren und stellt nur die als am besten bewertete SPARQL-Abfrage dar.
Hier wird das Problem der Ambiguität deutlich, da bei der Frage \enquote{What is the chief information officer responsible for?} sowohl
\texttt{meta:isResponsibleForEntityType}, \texttt{meta:isResponsibleForFunction} und \texttt{meta:isResponsibleForRole} gemeint sein können.
Der Ansatz für die Lösung dieses Problems ist es, das System alle drei ausführen zu lassen, oder ihm das zumindest zu ermöglichen.

Dies kann zum Beispiel über Property Paths realisiert werden.
In diesem Fall erlauben Property Paths dem Prädikat, in der Abfrage verschiedene Ressourcen darstellen zu können.
Es werden letztendlich drei Abfragen ausgeführt, eine für jede mögliche Kombination der Attribute, also hier einmal pro Prädikat.
Statt allein \texttt{meta:isResponsibleForEntityType} steht durch die Nutzung von Property Paths nun \texttt{meta:isResponsibleForRole | meta:isResponsibleForFunction | meta:isResponsibleForEntityType} dort.
Jedoch kann QAnswer keine Property Paths aufstellen, weshalb diese Option zur Lösung des Ambiguitätsproblems wegfällt.

Eine andere Möglichkeit, die obige Frage mittels einer SPARQL-Abfrage zu beantworten, ist mittels der \texttt{rdfs:subPropertyOf}-Beziehung.
Diese regelt die Hierarchie von Properties.
So sind die drei, welche hier alle in einer Abfrage zusammengefasst werden sollen, alle ein Subproperty von \texttt{meta:subPropertyOf}.

Deshalb müssen vor dem Training die \texttt{rdfs:subPropertyOf}+-Beziehung materialisiert werden.
Das heißt es werden alle transitiven Subpropertybeziehungen zu Trainingszwecken mittels dem SPARQL-Befehl \texttt{CONSTRUCT} zu direkten Subklassenbeziehungen umgeformt.

Es werden erst alle Tripel gefunden, die ausschließlich aus den Teilontologien \texttt{meta} und \texttt{bb} bestehen.
Somit werden auch nur Prädikate aus \texttt{meta} betrachtet, nicht aber \texttt{rdfs} oder anderen.
Danach werden alle Subproperties des Prädikats aus dem ersten Tripel ausgewählt.
Mittels \texttt{CONSTRUCT} werden die ausgewählten Prädikate anstelle dem anderen Property eingesetzt.
Diese Tripel werden dann manuell den RDF-Daten hinzugefügt\footnote{Verwendete n-Tripel-Datei verfügbar unter:\\\url{https://github.com/Yagnap/BeLL-Question-Answering-auf-Linked-Data-SNIK/blob/main/Data/qanswerset.nt}}

Zusätzlich werden auch die \texttt{rdfs:subClassOf}+-Beziehungen materialisiert.
Dies muss geschehen, da die Antworten für manche Textbuchfragen alle transitiven Subklassen sind.

Durch die Materialisierung kann beispielsweise für die Frage \texttt{8/6}, \enquote{How can quality of HIS be evaluated?},
die Klasse \texttt{bb:CaseStudy} gefunden werden, obwohl diese eine direkte Subklasse von \texttt{bb:QualitativeEvaluationMethod} ist,
welche wiederum eine direkte Subklasse von \texttt{bb:EvaluationMethod} ist, welche in der Abfrage gesucht wird.
Die Abfrage, welche die \texttt{rdfs:subClassOf+}-Beziehung kann nicht durch QAnswer generiert werden, die materialisierte \texttt{rdfs:subClassOf} jedoch schon.

Wenn keine Lösung für die Frage gefunden wird, gibt es als Ausgabe meist die Ressource selbst, also zum Beispiel \texttt{bb:ChiefInformationOfficer}.
Dies kann zwar nützlich, aber auch verwirrend sein und soll mithilfe von Training verhindert werden.
Die Präzision der Ergebnisse ohne jegliches fine-tuning ist aber trotzdem erstaunlich.

Praktisch für die Lokalisierung der Fehler ist die Funktion, sich alle generierten Anfragen anzeigen zu lassen.
Somit kann erahnt werden, warum es das macht, was es macht.

Mit der Konfiguration an sich können bereits viele Fehler behoben werden.
Unter den \emph{stop words} sind häufig verwendete Präpositionen, Konjunktionen, Verben oder Füllwörter wie \enquote{and} oder \enquote{many}.
Diese sollen verhindern, dass falsche Ressourcen gefunden werden.
Hier muss diese Liste allerdings so modifiziert werden, dass das Wort \texttt{for} nicht mehr darin vorkommt, denn ohne es können Prädikate wie \enquote{responsible for} schwer erkannt werden,
besonders, da die beiden Wörter oft getrennt im Satz vorkommen.
Entfernt wurden auch \texttt{define} und \texttt{describe}, da auch diese in Labels vorkommen.

Zu den \emph{Hidden Properties} wurde \texttt{rdfs:subClassOf} hinzugefügt.
Hidden Properties sind solche, die nicht explizit in der Frage benutzt werden, aber trotzdem impliziert werden können.

Es wurde \texttt{skos:altLabel} als weiteres Label für Ressourcen hinzugefügt, sodass QAnswer auch diese beachtet.
Die Mappings müssen auch dahingehend verändert werden, als dass \texttt{skos:definition}
und \texttt{rdfs:comment} als Beschreibung hinzugefügt werden.
Die Definition wird auch bei den Ergebnissen angezeigt, sodass es bei Ergebnissen von Fragen nach der Definition als richtig erachtet wird, wenn die zu definierende Ressource richtig erkannt wurde.

Es gibt auch die Möglichkeit, direkt Wörter als Aliase für URIs zu nutzen.
Bei den \emph{Property Mapping} können URIs und dafür stehende Lexikalisierungen in Abhängigkeit gebracht werden, wie bei einem Wörterbuch.
Dies wird hier für \enquote{phases} und \enquote{methods} bei \texttt{meta:updates} sowie \enquote{tasks} und \texttt{meta:functionComponent} gemacht.
Für \texttt{meta:entityTypeComponent} wird \enquote{facets} hinzugefügt.
Notwendig ist es aufgrund der Funktionsweise QAnswers; ohne Lexikalisierungen weiß das Programm nicht, wofür die Ressourcen stehen.
Da die Properties in SNIK sehr allgemein gehalten sind, fehlen oft Labels, mithilfe derer QAnswer auf die Ressource als Teil der Antwort schließen kann.
Für diese stehen die Lexikalisierungen zur Verfügung.

\section{Benchmark}

Die Performance des Systems soll anhand eines Benchmarks aus Frage-Antwort-Paaren untersucht werden.
Die Frage liegt hierbei in natürlicher Sprache, die Antwort als SPARQL-Abfrage vor.
100 Fragen sollen als Testdatensatz, der immer wieder abgefragt wird, dienen,
alle anderen Fragen sollen in Schritten von 10 Fragen QAnswer trainieren und somit die Abhängigkeit von den Indikatoren \emph{Präzision}, \emph{Recall} und \emph{F-Maß} zur Anzahl der Fragen geben.

QAnswer kann mittels Nutzerfragen trainiert werden.
Herausgefunden werden soll der Einfluss der Anzahl der Nutzerfragen auf die Qualität der Antworten und somit das mögliche Finden eines Optimums.
Weiterhin wird dies einmal mit und einmal ohne Lehrbuchfragen geschehen, da auch der Einfluss von deren erhöhter Schwierigkeit auf die Ergebnisse bestimmt wird.

Das Lehrbuch selbst \cite{bb} enthält am Ende vn Kapiteln bereits Fragen, welche dasGelernte zusammenfassen sollen.
Ausgehend von \cite{arneba} wurden die Fragen anhand ihrer Eignung für das Training klassifiziert.
Es wurden nur Fragen in Betracht gezogen, die Anforderungsbereich 1 entsprechen, also Fakten wiedergeben sollen und nicht z.B. zusammenfassen.
Nicht alle Fragen haben Antworten in der Ontologie, diese wurden auch herausgefiltert.
Der Rest wurde in SPARQL-Abfragen umgewandelt.
Insgesamt gibt es 36 Lehrbuchfragen und zugehörige Antworten.

Da diese Fragen sowohl sehr kompliziert gestellt als auch viel zu wenige sind, werden aus dem Datenbestand der Teilontologie auch Fragen nach dem Schema \emph{Subjekt bzw. Objekt - Prädikat - Objekt bzw. Subjekt} gestellt.
Diese und die zugehörigen Antworten lassen sich mittels SPARQL-Abfragen automatisch erstellen.
Daraus entstehen 621 Fragen nach dem Subjekt und 374 Fragen nach dem Objekt, also insgesamt 995 weitere Frage-Antwort-Paare, mit denen das Training beginnen kann.
\section{Ergebnisse}

Das Frage-Antwort-Set aus dem Lehrbuch wurde in zwei Hälften geteilt, eine zum Training und eine zum Testen.
Die Fragen wurden randomisiert in die Gruppen eingeteilt, sodass 18 im Trainingsdatensatz und 18 im Testdatensatz sind.

Von den automatisch generierten Paaren wurden 100 für das Testen und 895 für das Training genutzt.

Das Training verläuft so ab, dass dem System automatisiert pro Schritt zehn weitere automatisch generierte Paare aus dem Trainingsdatensatz gegeben werden,
wodurch jede Runde mit zehn zusätzlichen Fragen trainiert wird.
Der Trainingssatz der Lehrbuchfragen wird in der ersten Runde nicht mit verwendet,
in der zweiten Runde werden von Anfang an alle Textbuchfragen verwendet.

Dann wird QAnswer mithilfe der API-Methode trainiert.
Auf das trainierte System wird der Testdatensatz zur Evaluierung angewandt.
Über eine API-Abfrage wird die Antwort mit dem höchsten Confidence-Wert, eine SPARQL-Abfrage, geholt und auf dem SPARQL-Endpunkt von SNIK ausgeführt.
Die Antworten der richtigen Lösung wurden schon vorher gespeichert und werden nun mit denen der Abfrage von QAnswer verglichen.

Dann wird das trainierte Modell zurückgesetzt, die gegebenen Frage-Antwort-Paare bleiben jedoch erhalten.

% PLOTS
\begin{figure}%[h!]
  \centering
  \begin{tikzpicture}
    \begin{groupplot}[
        group style = {
          xlabels at = edge bottom,
          ylabels at = edge left,
          horizontal sep = 2cm,
          vertical sep = 3cm,
          group size = 2 by 3,
        },
        width = 0.5\linewidth
        ]

        % F-Score
        \nextgroupplot[
          title=F-Maß autogenerierte Fragen,
          grid=major, % Display a grid
          grid style={dashed,gray!30}, % Set the style
          xlabel=Anzahl der generierten Trainingsfragen, % Set the labels
          ylabel=F-Maß,
          label style={font=\scriptsize},
					xtick distance=200,
        ]
        \addplot table[x=Anzahl Trainingsfragen,y=F-Score,col sep=comma] {../../Data/Tabellen/Training-Auswertung/generated.csv};
        \addplot table[x=Anzahl Trainingsfragen,y=F-Score,col sep=comma] {../../Data/Tabellen/Training-Auswertung/generated-withtb.csv}; 

        % F-Score TB
        \nextgroupplot[
          title=F-Maß Lehrbuchfragen,
          grid=major, % Display a grid
          grid style={dashed,gray!30}, % Set the style
          xlabel=Anzahl der generierten Trainingsfragen, % Set the labels
          ylabel=F-Maß,
          label style={font=\scriptsize},
					xtick distance=200,
        ]
        \addplot table[x=Anzahl Trainingsfragen,y=F-Score,col sep=comma] {../../Data/Tabellen/Training-Auswertung/textbook-av.csv};
        \addplot table[x=Anzahl Trainingsfragen,y=F-Score,col sep=comma] {../../Data/Tabellen/Training-Auswertung/textbook-av-withtb.csv}; 
        
        % Precision
        \nextgroupplot[
          title=Präzision autogenerierte Fragen,
          grid=major, % Display a grid
          grid style={dashed,gray!30}, % Set the style
          xlabel=Anzahl der generierten Trainingsfragen, % Set the labels
          ylabel=Präzision,
          label style={font=\scriptsize},
					xtick distance=200,
        ]
        \addplot table[x=Anzahl Trainingsfragen,y=Precision,col sep=comma] {../../Data/Tabellen/Training-Auswertung/generated.csv};
        \addplot table[x=Anzahl Trainingsfragen,y=Precision,col sep=comma] {../../Data/Tabellen/Training-Auswertung/generated-withtb.csv}; 

        % Precision TB
        \nextgroupplot[
          title = Präzision Lehrbuchfragen,
          grid=major, % Display a grid
          grid style={dashed,gray!30}, % Set the style
          xlabel=Anzahl der generierten Trainingsfragen, % Set the labels
          ylabel=Präzision,
          label style={font=\scriptsize},
					xtick distance=200,
        ]
        \addplot table[x=Anzahl Trainingsfragen,y=Precision,col sep=comma] {../../Data/Tabellen/Training-Auswertung/textbook-av.csv};
        \addplot table[x=Anzahl Trainingsfragen,y=Precision,col sep=comma] {../../Data/Tabellen/Training-Auswertung/textbook-av-withtb.csv}; 
        
        % Recall
        \nextgroupplot[
          title=Recall autogenerierte Fragen,
          grid=major, % Display a grid
          grid style={dashed,gray!30}, % Set the style
          xlabel=Anzahl der generierten Trainingsfragen, % Set the labels
          ylabel=Recall,
          label style={font=\scriptsize},
					xtick distance=200,
        ]
        \addplot table[x=Anzahl Trainingsfragen,y=Recall,col sep=comma] {../../Data/Tabellen/Training-Auswertung/generated.csv};
        \addplot table[x=Anzahl Trainingsfragen,y=Recall,col sep=comma] {../../Data/Tabellen/Training-Auswertung/generated-withtb.csv}; 

        % Recall TB
        \nextgroupplot[
          title=Recall Textbuchfragen,
          grid=major, % Display a grid
          grid style={dashed,gray!30}, % Set the style
          xlabel=Anzahl der generierten Trainingsfragen, % Set the labels
          ylabel=Recall,
          label style={font=\scriptsize},
					xtick distance=200,
          legend style={at={(-0.2,-0.3)},anchor=north}, % Put the legend below the plot
        ]
        \addplot table[x=Anzahl Trainingsfragen,y=Recall,col sep=comma] {../../Data/Tabellen/Training-Auswertung/textbook-av.csv};
        \addlegendentry{Training ohne Lehrbuchfragen}
        \addplot table[x=Anzahl Trainingsfragen,y=Recall,col sep=comma] {../../Data/Tabellen/Training-Auswertung/textbook-av-withtb.csv}; 
        \addlegendentry{Training mit Lehrbuchfragen}

        \legend{Training ohne Lehrbuchfragen,Training mit Lehrbuchfragen}
    \end{groupplot}
  \end{tikzpicture}
  \caption{F-Maß, Präzision, und Recall von Lehrbuchtestfragen und automatisch generierten Testfragen in Abhängigkeit zur Anzahl und Zusammensetzung der Trainingsfragen.}
\end{figure}

\begin{figure}
  \begin{tikzpicture}
    \begin{groupplot}[
      group style = {
        xlabels at = edge bottom,
        ylabels at = edge left,
        horizontal sep = 2cm,
        vertical sep = 3cm,
        group size = 2 by 1,
      },
      width = 0.5\linewidth
      ]

      % Confidence
      \nextgroupplot[
        title=Confidence autogenerierte Fragen,
        grid=major, % Display a grid
        grid style={dashed,gray!30}, % Set the style
        xlabel=Anzahl der generierten Trainingsfragen, % Set the labels
        ylabel=Confidence-Wert,
        label style={font=\scriptsize},
        xtick distance=200,
      ]
      \addplot table[x=Anzahl Trainingsfragen,y=Confidence,col sep=comma] {../../Data/Tabellen/Training-Auswertung/generated.csv};
      \addplot table[x=Anzahl Trainingsfragen,y=Confidence,col sep=comma] {../../Data/Tabellen/Training-Auswertung/generated-withtb.csv}; 

      % Confidence TB
      \nextgroupplot[
        title=Confidence Textbuchfragen,
        grid=major, % Display a grid
        grid style={dashed,gray!30}, % Set the style
        xlabel=Anzahl der generierten Trainingsfragen, % Set the labels
        ylabel=Confidence-Wert,
        label style={font=\scriptsize},
        xtick distance=200,
        legend style={at={(-0.2,-0.3)},anchor=north}, % Put the legend below the plot
      ]
      \addplot table[x=Anzahl Trainingsfragen,y=Confidence,col sep=comma] {../../Data/Tabellen/Training-Auswertung/textbook-av.csv};
      \addlegendentry{Training ohne Lehrbuchfragen}
      \addplot table[x=Anzahl Trainingsfragen,y=Confidence,col sep=comma] {../../Data/Tabellen/Training-Auswertung/textbook-av-withtb.csv}; 
      \addlegendentry{Training mit Lehrbuchfragen}

    \end{groupplot}
  \end{tikzpicture}
  \caption{Confidence-Wert von Lehrbuchtestfragen und automatisch generierten Testfragen in Abhängigkeit zur Anzahl und Zusammensetzung der Trainingsfragen.}
\end{figure}

An den Daten der automatisch generierten Fragen sieht man, wie groß das F-Maß abhängig von der Fragenanzahl ist.
Es sind zwei Datensätze dargestellt.
Der eine wurde andere ausschließlich über die SPARQL-generierten Frage-Antwort-Paare trainiert, beim anderen wurden die Lehrbuchfragen beim Training inkludiert.
Am Anfang, als noch gar keine oder nur sehr wenige Fragen zum Training verwendet wurden, ist das F-Maß eher gering.
Besonders beim Training mit den Lehrbuchfragen liegt es bei dem Modell ohne Training deutlich unter 30\%.
Ab 30 Fragen nähert es sich den Werten des Trainings ohne die Lehrbuchfragen an und übersteigt diese bei 40 Fragen erstmals.
Bei 70 Fragen erreicht das F-Maß des Trainings mit den Lehrbuchfragen bei 90\% ein Maximum und bleibt sehr lange nahezu konstant.

Bei 520 bzw. 570 Fragen sinken die Werte für beide Graphen aber sehr abrupt sehr stark ab, Varianz wird viel größer,
mit Werten zwischen fast 10\% und mehr als 90\%.

Auch die Graphen von Präzision und Recall bieten ein ähnliches Bild.
Sie haben auch am Anfang kurz geringere Werte, eine starke, konstante Mitte und danach eine extreme Varianz.

Der Confidence-Wert sieht im Allgemeinen deutlich konstanter aus, an ihm spiegeln sich aber auch einige der beim F-Maß betrachteten Phänomene.
Anfangs ist der Wert durchschnittlich etwas geringer als später, aber immer noch fast überall über 50\%.
Ab etwa 100 Fragen erreicht es ein Niveau zwischen 80\% und 90\%, auf dem es bleibt.
Ab 510 bzw. 560 Fragen kommt es jedoch zu zu größeren Ausreißern, die teilweise bis auf 11\% heruntergehen.

Die Graphen der Lehrbuchfragen sehen denen der automatisch generierten auf den ersten Blick nicht sehr ähnlich aus,
es lassen sich aber doch einige Gemeinsamkeiten erkennen.
Sie sind über die Mitte auch recht konstant und haben mit mehr Trainingsfragen auch eine sehr viel größere Varianz.
Anfangs sind die Ergebnisse auch hier schlechter.

Die durchschnittliche Varianz ist hier aber größer als bei den generierten Fragen, und außerdem sind die Werte deutlich niedriger.
Bei den automatisch generierten Fragen waren die Werte konstant bei und über 90\%, hier nur knapp über 25\%.
Außerdem gibt die Verwendung des Lehrbuchfragen-Trainingssets deutlich schlechtere Ergebnisse, bei den generierten Fragen war dieser Unterschied auch deutlich geringer.

\section{Diskussion und Ausblick}

Ein Modell ist immer für einen bestimmten Zweck erstellt und bildet nur einen Teil der Wirklichkeit ab.
SNIK fokussiert sich auf das Beantworten der Fragestellung "Wer (Rolle) macht was (Aufgabe) womit (Objekttyp)?".
Solche Fragen beantwortet das trainierte System auch sehr gut, die größte Herausforderung ist allerdings der "Mismatch" im verwendeten Vokabular eines menschlichen Nutzers zu den Properties der Ontologie.
Während in einer Wissensbasis die Abbildung von Verben zu Properties einfacher ist, weicht besonders bei der Beschreibung von Subklassen und Teil-Ganzes-Beziehungen das Vokabular menschlicher Nutzer oft von den Labels in der Ontologie ab.
Insgesamt erreicht das System bei dieser Art von Fragen  einen F-Score von 0.9... auf ..., siehe ...
Fragen, welche nicht diesem Schema folgen, wie die Leseverständnisfragen aus \cite{bb} (fußnote zu link im zenodo archiv), können selbst nach dem Training nur selten richtig beantwortet werden (F-Score von ... auf ..., siehe ...).
Unserer Einschätzung nach ist dies eine grundlegende Limitierung der gewählten Modellierung, wir erwarten daher auch bei zukünftigen Systemen keine überwiegend richtige Beantwortung der Verständnisfragen.
Wir planen zur Beantwortung allgemeiner Fragen, die über "Wer macht was womit" hinausgehen, das Training von Sprachmodellen direkt auf den Lehrbüchern.
Ein hybrides System aus KGQA und Sprachmodell hat das Potenzial, die Stärken beider Ansätze zu vereinen.

\begin{itemize}
  \item Diskussion von Kurven etc.
\end{itemize}

%% \bibliography{lni-paper-example-de.tex} ist hier nicht erlaubt: biblatex erwartet dies bei der Preambel
%% Starten Sie "biber paper", um eine Bibliographie zu erzeugen.
\printbibliography

\end{document}
